{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Hi there \ud83d\udd96! \u00b6 My name is \ud83d\udc68Alejandro Piad Morffis. Here are some things about me: I live in Havana, \ud83c\udde8\ud83c\uddfaCuba. I'm currently finishing a \ud83c\udf93PhD in Computer Science, part-time in Alicante, \ud83c\uddea\ud83c\uddf8Spain. In my free time, I also enjoy \ud83d\udcbb coding (mostly in \ud83d\udc0dPython), \ud83c\udfaeplaying video games (sadly not much lately), and \u270f\ufe0f writing . My two passions are \ud83d\udcda teaching and \u2697\ufe0f researching . I teach Programming, Compilers, AI, and a bunch of other stuff at the University of Havana. I also do research there, mostly on how to use artificial intelligence to better understand human languages, and on the democratization of machine learning tools. You can find me online on \ud83d\udde8\ufe0f Twitter , \ud83d\udcbc LinkedIn , \ud83d\udd25 Reddit , \ud83d\udcf1 Telegram and \ud83c\udfa5 Youtube . \ud83d\udc8c The best way to contact me is to mention @AlejandroPiad on Twitter. I follow very few people (only those with whom I interact frequently) but I try to reply whenever someone asks me to. And these are the values I stand for: \u2764\ufe0f I believe that people are generally good, and if given the chance, they will show the better parts of themselves. \u2764\ufe0f I believe people should have the chance to speak their minds, without fear to be silenced or hated for it, even if they are wrong. And they should have the option to honestly recognize their mistakes, learn from them, and be forgiven. \u2764\ufe0f I do not tolerate racism or discrimination of any kind, towards me or others around me, and I work very hard to apply those same standards to myself. \u2764\ufe0f I'm dedicated to education because I think that access to high-quality, unbiased, and free education is one of the best gifts anyone can receive, and one of the easiest ways to make people more reasonable and tolerant.","title":"\ud83d\udd96 Hello World!"},{"location":"#hi-there","text":"My name is \ud83d\udc68Alejandro Piad Morffis. Here are some things about me: I live in Havana, \ud83c\udde8\ud83c\uddfaCuba. I'm currently finishing a \ud83c\udf93PhD in Computer Science, part-time in Alicante, \ud83c\uddea\ud83c\uddf8Spain. In my free time, I also enjoy \ud83d\udcbb coding (mostly in \ud83d\udc0dPython), \ud83c\udfaeplaying video games (sadly not much lately), and \u270f\ufe0f writing . My two passions are \ud83d\udcda teaching and \u2697\ufe0f researching . I teach Programming, Compilers, AI, and a bunch of other stuff at the University of Havana. I also do research there, mostly on how to use artificial intelligence to better understand human languages, and on the democratization of machine learning tools. You can find me online on \ud83d\udde8\ufe0f Twitter , \ud83d\udcbc LinkedIn , \ud83d\udd25 Reddit , \ud83d\udcf1 Telegram and \ud83c\udfa5 Youtube . \ud83d\udc8c The best way to contact me is to mention @AlejandroPiad on Twitter. I follow very few people (only those with whom I interact frequently) but I try to reply whenever someone asks me to. And these are the values I stand for: \u2764\ufe0f I believe that people are generally good, and if given the chance, they will show the better parts of themselves. \u2764\ufe0f I believe people should have the chance to speak their minds, without fear to be silenced or hated for it, even if they are wrong. And they should have the option to honestly recognize their mistakes, learn from them, and be forgiven. \u2764\ufe0f I do not tolerate racism or discrimination of any kind, towards me or others around me, and I work very hard to apply those same standards to myself. \u2764\ufe0f I'm dedicated to education because I think that access to high-quality, unbiased, and free education is one of the best gifts anyone can receive, and one of the easiest ways to make people more reasonable and tolerant.","title":"Hi there \ud83d\udd96!"},{"location":"about/projects/","text":"Here are some of the most interesting projects I'm working on: Using the amazing github-readme-stats AutoGOAL \u00b6 \ud83d\udde8\ufe0f Ask me on Twitter. AutoGOAL is a Python framework for Automated Machine Learning that I and a few other colleagues are building. It's the main research result of my wife's Ph.D. , and it is a team project in which I'm a proud contributor and evangelist. Auditorium \u00b6 \ud83d\udde8\ufe0f Ask me on Twitter. Auditorium is my attempt to bring together Python with HTML for interactive slideshows. It is based on reveal.js and allows creating a slideshow with pure Python code, including some interactive stuff like rendering graphs on the fly and running animations. Illiterate \u00b6 \ud83d\udde8\ufe0f Ask me on Twitter. Illiterate is my take at the dilemma of code comments vs documentation. The idea stems from my love for the Literate Programming paradigm with a pragmatic twist that doesn't require any external tooling but rather relies on discipline and conventions.","title":"\ud83d\udcbb Projects"},{"location":"about/projects/#autogoal","text":"\ud83d\udde8\ufe0f Ask me on Twitter. AutoGOAL is a Python framework for Automated Machine Learning that I and a few other colleagues are building. It's the main research result of my wife's Ph.D. , and it is a team project in which I'm a proud contributor and evangelist.","title":"AutoGOAL"},{"location":"about/projects/#auditorium","text":"\ud83d\udde8\ufe0f Ask me on Twitter. Auditorium is my attempt to bring together Python with HTML for interactive slideshows. It is based on reveal.js and allows creating a slideshow with pure Python code, including some interactive stuff like rendering graphs on the fly and running animations.","title":"Auditorium"},{"location":"about/projects/#illiterate","text":"\ud83d\udde8\ufe0f Ask me on Twitter. Illiterate is my take at the dilemma of code comments vs documentation. The idea stems from my love for the Literate Programming paradigm with a pragmatic twist that doesn't require any external tooling but rather relies on discipline and conventions.","title":"Illiterate"},{"location":"about/research/","text":"","title":"\u2697\ufe0f Research"},{"location":"about/teaching/","text":"","title":"\ud83d\udcda Teaching"},{"location":"essays/","text":"This is a collection of some things I've written over the years. Instead of blog posts, I consider them kind of short essays. These are varied in format and content, some are more technical, some are more philosophical, others a bit more pragmatic. I honestly don't like to write tutorial-like articles. I've tried a bunch of times and I've failed (though I might try again in the future). For now, I think there are enough amazing people out there doing it much better than I. Hence, these short essays are not about specific technologies or how-to guides. I try to select topics I'm passionate about and that I think have a chance to remain relevant for a longer time. If there is a particular topic you think I might have something interesting to say about, feel free to contact me on Twitter . I've also recently started to post short, actionable stuff in the form of tweetstorm . These are threads of 10-20 tweets that I'm also collecting here for future reference.","title":"\u2753 About essays"},{"location":"essays/academia-oss/","text":"What Academia can learn from Open Source \u00b6 I'm an academic. I love doing research and writing papers. What I don't love is playing the publishing game and waste my time micro-managing all these bureaucratic aspects of academia. I also love open-source software, and while the FOSS community is far from perfect, there are some ideas I think Academia could borrow that would make it more inclusive for everyone and more useful for society. \u26a0\ufe0f This is a rant about some things I think are wrong in Academia and some ideas about how to improve this situation. I mostly focus on Artificial Intelligence because that's my field, but I think most of these ideas apply everywhere. I'm not trying to discredit or criticize any individual or organization, but rather raise some questions that I think all of us scientists, as a community, should attend. I declare myself as guilty of all the sins I describe. \ud83d\uddde\ufe0f The setup \u00b6 If you've ever tried Academia you have surely been in this situation. You come up with a good idea, do some experiments, write a paper about it and... that's when the real work starts. Whether you send that paper to a conference or a journal, you'll get 2 to 5 reviewers to critic your paper, ask you for improvements and decide if your work is good enough for publication. If it's a conference, you'll usually either get accepted or rejected, but if it's a journal, you might get a second chance to improve and resubmit. This process is called peer review , and it's one of the fundamental pillars of Science. Don't get me wrong, peer review is extremely important. You see, Science is a social process. Yes, you can follow the scientific method and come up with a Frankenstein monster all by yourself on a private island, and you would be doing science (without capital \"s\"). It is only when those results are scrutinized, retested, and confirmed by additional researchers, that they become part of the continuous and incremental body of accumulated knowledge that we call Science. Peer review is a fundamental part of this process because it ensures that you are not deluding yourself into believing what you want to believe. It also guarantees we all follow the same high standards of openness, honesty, and goodwill. However, problems arise when the means become an end in itself. Since peer review is such an important concept in Science, we have built all our social scientific processes around it. We set deadlines, ratings, whole systems to formalize and organize what peer review means. We have double-blind and single-blind peer review to guarantee that authors and reviewers don't take revenge on each other. We have evaluation forms and protocols, and we have workshops and workshops about peer review. And yet, time and time again, experiments have shown that reviews are significantly inconsistent. If you randomly redistribute the papers on a top AI conference, a large part of the accepted papers get rejected, and vice-versa. However, I do not take this as evidence that scientists are lousy reviewers. Not even close. Scientists are pretty good at being objectively critical of other's and our work, we do that every single day! I think the problem lies in the system and the incentives built around it, mostly for the benefit of the big players in the Academic world, the publishers. \ud83e\udd15 The symptoms \u00b6 Every time you take a metric and turn it into an objective, it ceases to be a useful metric. This has happened in Science with the concept of publishing a paper . Publishing a paper is the main mechanism for socializing research. A research paper usually describes some scientific hypotheses in as clear terms as possible, a protocol to test (i.e., falsify) those hypotheses, and an honest and critical discussion of results and their implications. By reading a paper, fellow scientists can come up with additional hypotheses or ideas, and build on top of previous work. And every time you use someone else's ideas as part of your own, you are supposed to include a citation. This is what Newton was referring to when he said he had \"stand on the shoulder of giants\". In time, the most significant scientific discoveries should get a large number of citations, because everyone building on top of your ideas would cite you. Hence, a large number of citations is seen as a sign of scientific achievement, and that is often taken as the One Metric of Academic Success . See the problem here? Once citations become a distinction mark, everyone tries to maximize them. A lot of strategies begin to arise, like publishing lots of low-effort papers instead of fewer and better ones, and working only on the most fashionable topics. Since to get cited you have to get published first, publishers become the gatekeepers. A feedback loop starts to build in which publishers try to be as exclusive as possible to attract better papers, since more citations imply more readers which implies more subscriptions; and authors try to aim for the most exclusive publishers since, otherwise, they won't get enough citations. In this dynamic, two very harmful things start to happen. \ud83c\udd70\ufe0f First, scientists spend a lot of effort and money, very often public money, on research that never gets published because of the massive competition. Ironically, once that research made with public money is published, is often put behind a subscription paywall, which most universities and institutions subscribe to. So taxpayers end up paying for research twice, once when done by Alice and again when Bob wants to read Alice's paper. Isn't that crazy enough? \ud83c\udd71\ufe0f The second issue is more subtle but far more harmful. In this process of out-competing each other for citations, we forgot what's important about Science. It's a social process designed to improve human life by solving humanity's most pressing problems. But this competition, far from what free-market ideologists could believe, only serves to undermine the very purpose of Science: \ud83d\udc4e The most fashionable topics get the most attention, and those are often not correlated with the need of the many. \ud83d\udc4e Also, scientists are not born, they are educated. If competition is so fierce that junior researchers don't get a break, we end up losing the best minds before they get a chance to shine. \ud83d\udc4e And finally, this constant competition for citations discourages any kind of self-critic research, any analysis of negative results, and any replication study, because no one will cite you for saying \" yeah, I retested this, and it does seem to work as they originally said... \". This discussion started with peer review, and how the whole academic publishing is built around this concept. Now is the time to criticize it. Since scientists are forced to compete for attention, we have turned peer review from the supportive and self-healing process it should be into the most unpleasant part of doing research. To be fair, not all reviewers are nasty, and when we do, I'm arguing is more often than not because we are forced by the system. \u2b50 The new paradigm \u00b6 I believe the root of the problem in this picture should be clear by now. \u26a0\ufe0f The incentives for scientists are not aligned with the purpose of Science. So, how do we realign the incentives of scientists with the original purpose of Science, and make it better for everyone? Honestly, I don't know. But I think we can take some ideas from the FOSS community to at least foster some good practices which I believe might put us on the right track. The idea starts with embracing Openness in the whole process of scientific discovery and innovation. This is not my original idea, of course, there are some commonly shared principles of \"open science\" in the academic community. This is one possible way to express them: Open Methodology : Document the application of methods and the entire process behind them as far as practicable and relevant. Open Source : Use open source technology (software and hardware) and open your own technologies. Open Data : Make the data freely and easily available. Open Access : Publish openly and make publications usable and accessible to everyone. Open Peer Review : Provide peer review in an open and public forum. Open Educational Resources : Use free and open materials for education and in university teaching. In this form, these principles are quite abstract, and there are many ways in which they could be implemented. There are plenty of degrees of \"open science\" like publishing in open access journals managed by non-profit organizations, publishing pre-prints before submitting to \"traditional\" journals, and all the good practices around making data and protocols publicly available. I want to focus on some key ideas I think could be fruitful to try, without implying that this is the absolute solution to this problem, but rather a small part of a much larger paradigm shift that Science has to undertake. \u2699\ufe0f The practices \u00b6 These are my proposals. Most of them relate specifically to the peer-review process because, as I said before, this process is a pillar of the scientific process, but also because I think this is the one place where we as a community can innovate the most, without requiring government grants or changing the way bureaucratic institutions work. The peer-review process is at the base of the entire scientific process and any major change in its functioning could have a massive impact up the chain. 1\ufe0f\u20e3 Public reviews \u00b6 Let's start by acknowledging that single- and double-blind reviews are more harmful than helpful. These measures are supposed to shield reviewers and authors from future retaliation and disallow any form of favouritism, which should make the review process more just and honest. In practice, they shield reviewers from criticism and make the whole review process less transparent. I propose to turn this concept around and make all reviews completely public. We have to trust we are all reasonable individuals and professional scientists, who should be able to provide objective judgment without favouritism. But if we don't, then our reviews themselves are public, and subject to review and criticism. \ud83d\udc49 This is very easy to implement with any workflow that allows posting comments on a public forum. Note that I don't necessarily mean that anyone can review (this is discussed further down) but even if only specific \"official\" reviewers are assigned to a paper, their comments and their identity and credentials should be public. 2\ufe0f\u20e3 Continuous peer-review \u00b6 This idea ties in with the previous point. Currently, almost all peer-review (that I'm aware of) happens in the context of some specific conference or journal. What I'm proposing here is to detach the peer-review process from any journal or conference and make it instead integral to the paper. Every paper would carry around the Internet with all its reviews, and if rejected at some previous point, a future conference or journal editor would have access to the full history of reviews and changes to reconsider the paper for \"mainstream\" publication (we'll talk more about what this means later). \ud83d\udc49 I can see this happening similar to how issues are handled in Github. You publish a paper, and potential reviewers would open \"issues\" against it, one for each important thing to address. Issues would be discussed and worked on in public and there would a history of every change introduced into the paper with links to which issues are being fixed. Since no paper is perfect, conferences and journal editors should not aim for publishing issue-free papers, but rather papers that show a healthy list of open and closed issues and demonstrable usefulness in their current state. A healthy list of open and closed issues would be an indication of a solid paper, the same way as for software. 3\ufe0f\u20e3 Encouraging reviews \u00b6 The next problem I want to tackle consists of how to kick-start the reviewing process. Once we detach reviews from specific conferences or journals, how can we guarantee everyone has access to good reviews? For sure, rockstar scientists will get thousands of reviews but what about junior researchers who are just starting? One idea is to see reviewing as an integral part of the scientific career. Researchers should be evaluated also in terms of how much value they put back to the community, and one way senior scientists can contribute is to review junior scientists. We should be proud to put in our CVs how many reviews we have given. And good reviews, which are in turn evaluated positively by the author and other reviewers, should count towards one's scientific output. \ud83d\udc49 Scientists would get a \"badge\" with the number reviews they have given, and display it on their homepage, their LinkedIn or ResearchGate profile, etc. This badge would link to some online list that links back to all reviews. This could be maybe hosted our ORCID profiles or any similar non-profit initiative. Also, senior researchers are part of a community, and are often connected with like-minded individuals in other institutions and countries. It should be part of their work to look up for each other's students and junior researchers. And yes, someone will say \" but then you can give a good review to my students if I give a good review to yours \". Again, this is why all review is, first and foremost, public in nature. 4\ufe0f\u20e3 Qualitative evaluations \u00b6 Now let's move on to specific review formats. Too often I see very long lists of checkboxes and 1-5 ratings, etc. I believe there is value in having a structured evaluation template, to make sure we more or less agree on what are the core issues we should care about. But going to the extreme of having 10 different ratings for a paper is insane! What is the difference between 6 and 7? When putting reviewers under the pressure of giving numerical scores, we are asking them to unconsciously introduce all the biases they have about that particular problem or field or approach or author. There is simply no objective way to numerically compare two different papers. A good research paper needs to have a solid methodology (correctly apply the principles of science as it is common practice in that field), provide relevant results and conclusions (either positive or negative), be feasible to reproduce by independent researchers, and have a clear presentation. Either the paper is good enough to be considered publishable, if all these aspects are covered, or it isn't. That's it. \ud83d\udc49 I prefer a simple evaluation form that asks: \" Is this aspect of the paper up to the scientific standard? , and a piece of free-form text for you to explain what is lacking in each aspect. Methodology [x] \ud83d\udc4d [ ] \ud83d\udc4e Results [x] \ud83d\udc4d [ ] \ud83d\udc4e Reproducibility [ ] \ud83d\udc4d [x] \ud83d\udc4e Presentation [ ] \ud83d\udc4d [x] \ud83d\udc4e A specific conference or journal might want to evaluate the potential impact or significance of a paper before accepting it for publishing. But impact or significance is not what Science is about. There are however legitimate cases where impact or significance is important. If you have to allocate a restricted pool of resources (e.g., grant money) of course you want to evaluate impact. Yet, I argue this is not part of the peer-review process, but a posterior analysis that each institution or publisher should do based on their specific criteria. Peer-review should be a process by which the scientific community as a whole evaluates that some research is sound science, irrespective of idiosyncrasies. 5\ufe0f\u20e3 Self-publishing \u00b6 Now that the peer-review process is completely detached from the \"mainstream\" publishing industry, who decides when is a paper ready to be published? Well, of course, the authors! It is up to the authors to determine that, given all the feedback received, they consider their work is production-ready. \ud83d\udc49 All papers would be published first in draft-mode, perhaps even before being completely written. During the draft phase, you collect all the feedback you can from peer reviews and work on the issues you consider more relevant. When you feel it is good enough (possibly because most of the recent reviews are favourable) you hit that Publish button and create a release. If some errors appear, later on, you fix them and publish another release. \ud83d\udc49 What becomes citable then? Easy, each release of each paper gets a unique DOI that will forever point to that exact version, together with all its metadata and reviews. If I cite something yours and criticize it, and you later fix it, that's OK. My critic is still valid because it points to a previous version that is indexable, and the fact that you fixed it only speaks higher of you! But wait, won't authors publish a lot of low-effort papers to engross their CVs? Well, maybe someone will but, who cares? We started by saying that, intuitively, citations should be a good measure of scientific quality. This is still true in this format. If I release a bunch of crappy papers, no one will cite them. And also, who thinks CVs are useful? Anyone trying to evaluate me as a researcher would not look into some list of titles and numbers I pasted in a Word document. They would go to my researcher profile and see my most significant work, the reviews it has received, and how my whole research process works! This doesn't mean that I get to decide my work is relevant, however. This just means I get to decide my work is ready to be consumed by the scientific community. The community will still judge my work's relevance by citing it, criticizing it, and in two more ways I left for the end. 6\ufe0f\u20e3 Conferences for networking \u00b6 Now that all papers are being published by their authors, what's the purpose of scientific conferences? We can now recover their original purpose. Conferences were created as a medium to get like-minded scientists together to share their experiences and to discuss the most relevant problems of their field. But as conferences became more and more a mainstream path for getting published, their organization has become more and more about managing the peer-review process. Now that peer-review is detached from conferences, their organizers are free to focus on scooping what are the most interesting topics and the most significant results in those topics and invite those speakers they believe will the bring the biggest value. Which papers get to be presented? I think we could deal with that in two ways: \ud83d\udc49 As an organizer of a thematic conference, I would spend half of the year looking around for interesting papers to invite their authors. They would still pay for their accommodations (or their institutions would) and they would come to enjoy what's best in every conference, the networking. \ud83d\udc49 I could also open a call for papers, as usual, but authors would submit papers that are already reviewed and released. My role would be to decide, based primarily on thematic fit, what I think is more relevant for my community. \ud83d\udc49 There is even no need to attach participation to a published paper. Authors could simply submit \"talks\", possibly backed by one or more papers that support their submission, as it is already common in some conferences. This would completely reshape what conferences are for (at least in my field). There is no reason why we should wait to the top conferences of the year to be able to read papers. We would go to conferences for the chance to talk with the researchers we admire about their work. And someone asks, but how would conferences compete if they have no publication rights? Well, I argue this would be very good. Conference organizers would have to compete on the grounds of providing a better environment for networking, interesting events, nice amenities, but no one would have a monopoly on the knowledge itself. There is even no reason why the same talk cannot be presented in more than one conference if enough people are willing to listen. 7\ufe0f\u20e3 Journals for socializing \u00b6 And finally we come round to the original culprit, the infamous research journal. Now that papers are published openly, what are journals good for? Well, what they were originally designed for, socializing research! Journals were created as a means for academic societies to collect the most relevant research in a given community and publish it for a larger audience. Then commercial publishers arrived and turned science into a business, and journals became paywalled gatekeepers of knowledge, that require original research often paid with public money that they resell again for public money. The largest academic publishers often state they have costs to cover, but there is plenty of evidence that they make a significant profit. And that's OK, but if I as a journal editor want to make a profit, I'm gonna have to innovate. \ud83d\udc49 Like conferences, I could scoop around and feature the most interesting papers in some thematic issues, maybe ask the authors to give some new comments on them, prepare explainer videos, add links, and put some effort into turning those \"raw\" papers into beautifully typeset pages. \ud83d\udc49 SOTA reviews would be a nice fit for journals as well. These are not original research papers, but they often provide a lot of value by analyzing a bunch of papers and giving advice on common trends or highlighting interesting lines for future research. \ud83d\udc49 I would also have editorial articles specifically written for an issue that could summarize in layman terms about a particular subject, to introduce it to a larger audience. I would even pay scientists that are good communicators for this work. Journals would have to compete on the grounds of being good at selecting topics and papers to socialize, and provide some additional editorial value. In any case, original research papers would be owned only by their authors and would be published always with some public license (e.g., Creative Commons). This would ensure that Science belongs to the ones who ultimately pay for it, that is, society. \ud83d\udcbb The system \u00b6 Putting all these ideas together in a functioning system will require a lot of work. From the infrastructure point of view, I envision something aking Github, a repository of open access papers with builtin comments, reviewing, and social features. Ideally, it would also have a web UI for editing, similar to Overleaf and, of course, fully integrated with Git. I understand this might not be the best solution for academic communities that are not very closely related to software, i.e., social sciences, mostly because it could pose a significant learning curve for their members and become more of a hinder than a help. From the social point of view, kick-starting such a system would require a massive community effort. And not because of the infrastructure cost, that's the minor issue. I think the largest obstacle for this kind of paradigm shift is that a large part of the community would have to move away simultaneously from journals and conferences as the main publication channels. Otherwise, the few that start the effort will be completely disconnected from the rest of the community. I can see this happening as an effort from, say, the AI community, or any other technically-savvy collective. Tomorrow morning, all the senior scientists that publish in ICLR, ICML, NeurIPs, and ACL, could suddenly decide they want to go fully open. It would require the conference organizers to support the initiative as well. Instead of opening a call for papers, the conferences could decide that they would open a call for submissions, which should peer-reviewed and published in this new format. Some non-profit organization could be formed from within the community to provide the infrastructure. Since there will be some operative costs, this platform would require some payment, but it would be very small compared to publishing fees in most major open access journals. Also, some of the big players in the industry could support this initiative by providing hosting and infrastructure for free. This would be a big PR boost to these companies. \ud83d\udcca The metrics \u00b6 We started this discussion by saying that an intuitively good metric to estimate scientific impact, i.e., citations, had become an objective and thus lost their entire meaning. But citations are not an inherently bad metric, it's just when we use citations as the one quantitative metric to compare individual researchers that we miss the entire point. Likewise, the fact that we self-publish all our papers doesn't mean that being featured in a major conference or journal is worthless. On the contrary, when everything we publish is open, being featured in a mainstream publication becomes an even better measure of impact, because it is no longer tied to my financial capacity or any other unfair advantage I might enjoy in the community. This would be very good for Third World researchers, who produce valuable science, but are often cutoff from mainstream publication for reasons completely unrelated to the quality of the research. If we can restructure the incentives and processes of Science such that they are aligned with the purpose of Science as primarily a means to improve human life, everything else would fall into place. Once researchers are free from predatory publishing practices, meaningless numerical statistics and unhealthy competition, I believe we will all focus what we love most, doing sound research for the good of mankind. Then, all those metrics that are used today will regain their meaning. Being invited to a top-tier conference would mean that what your community wants to hear from you. Being featured in a top-tier journal would mean that some editors consider your work is high-quality. And being cited often would mean that your research is producing real impact, that you are becoming a giant on whose shoulders others can stand. \ud83d\udde8\ufe0f This is a topic of which I'm very passionate about, and I want to hear your feedback. If you want to discuss this topic with me, give me a shout in Twitter . \ud83d\udeab If you find some typo, error or you have any suggestion to help me improve this page, you can suggest and edit or open an issue in Github .","title":"Academia & OSS"},{"location":"essays/academia-oss/#what-academia-can-learn-from-open-source","text":"I'm an academic. I love doing research and writing papers. What I don't love is playing the publishing game and waste my time micro-managing all these bureaucratic aspects of academia. I also love open-source software, and while the FOSS community is far from perfect, there are some ideas I think Academia could borrow that would make it more inclusive for everyone and more useful for society. \u26a0\ufe0f This is a rant about some things I think are wrong in Academia and some ideas about how to improve this situation. I mostly focus on Artificial Intelligence because that's my field, but I think most of these ideas apply everywhere. I'm not trying to discredit or criticize any individual or organization, but rather raise some questions that I think all of us scientists, as a community, should attend. I declare myself as guilty of all the sins I describe.","title":"What Academia can learn from Open Source"},{"location":"essays/academia-oss/#the-setup","text":"If you've ever tried Academia you have surely been in this situation. You come up with a good idea, do some experiments, write a paper about it and... that's when the real work starts. Whether you send that paper to a conference or a journal, you'll get 2 to 5 reviewers to critic your paper, ask you for improvements and decide if your work is good enough for publication. If it's a conference, you'll usually either get accepted or rejected, but if it's a journal, you might get a second chance to improve and resubmit. This process is called peer review , and it's one of the fundamental pillars of Science. Don't get me wrong, peer review is extremely important. You see, Science is a social process. Yes, you can follow the scientific method and come up with a Frankenstein monster all by yourself on a private island, and you would be doing science (without capital \"s\"). It is only when those results are scrutinized, retested, and confirmed by additional researchers, that they become part of the continuous and incremental body of accumulated knowledge that we call Science. Peer review is a fundamental part of this process because it ensures that you are not deluding yourself into believing what you want to believe. It also guarantees we all follow the same high standards of openness, honesty, and goodwill. However, problems arise when the means become an end in itself. Since peer review is such an important concept in Science, we have built all our social scientific processes around it. We set deadlines, ratings, whole systems to formalize and organize what peer review means. We have double-blind and single-blind peer review to guarantee that authors and reviewers don't take revenge on each other. We have evaluation forms and protocols, and we have workshops and workshops about peer review. And yet, time and time again, experiments have shown that reviews are significantly inconsistent. If you randomly redistribute the papers on a top AI conference, a large part of the accepted papers get rejected, and vice-versa. However, I do not take this as evidence that scientists are lousy reviewers. Not even close. Scientists are pretty good at being objectively critical of other's and our work, we do that every single day! I think the problem lies in the system and the incentives built around it, mostly for the benefit of the big players in the Academic world, the publishers.","title":"\ud83d\uddde\ufe0f The setup"},{"location":"essays/academia-oss/#the-symptoms","text":"Every time you take a metric and turn it into an objective, it ceases to be a useful metric. This has happened in Science with the concept of publishing a paper . Publishing a paper is the main mechanism for socializing research. A research paper usually describes some scientific hypotheses in as clear terms as possible, a protocol to test (i.e., falsify) those hypotheses, and an honest and critical discussion of results and their implications. By reading a paper, fellow scientists can come up with additional hypotheses or ideas, and build on top of previous work. And every time you use someone else's ideas as part of your own, you are supposed to include a citation. This is what Newton was referring to when he said he had \"stand on the shoulder of giants\". In time, the most significant scientific discoveries should get a large number of citations, because everyone building on top of your ideas would cite you. Hence, a large number of citations is seen as a sign of scientific achievement, and that is often taken as the One Metric of Academic Success . See the problem here? Once citations become a distinction mark, everyone tries to maximize them. A lot of strategies begin to arise, like publishing lots of low-effort papers instead of fewer and better ones, and working only on the most fashionable topics. Since to get cited you have to get published first, publishers become the gatekeepers. A feedback loop starts to build in which publishers try to be as exclusive as possible to attract better papers, since more citations imply more readers which implies more subscriptions; and authors try to aim for the most exclusive publishers since, otherwise, they won't get enough citations. In this dynamic, two very harmful things start to happen. \ud83c\udd70\ufe0f First, scientists spend a lot of effort and money, very often public money, on research that never gets published because of the massive competition. Ironically, once that research made with public money is published, is often put behind a subscription paywall, which most universities and institutions subscribe to. So taxpayers end up paying for research twice, once when done by Alice and again when Bob wants to read Alice's paper. Isn't that crazy enough? \ud83c\udd71\ufe0f The second issue is more subtle but far more harmful. In this process of out-competing each other for citations, we forgot what's important about Science. It's a social process designed to improve human life by solving humanity's most pressing problems. But this competition, far from what free-market ideologists could believe, only serves to undermine the very purpose of Science: \ud83d\udc4e The most fashionable topics get the most attention, and those are often not correlated with the need of the many. \ud83d\udc4e Also, scientists are not born, they are educated. If competition is so fierce that junior researchers don't get a break, we end up losing the best minds before they get a chance to shine. \ud83d\udc4e And finally, this constant competition for citations discourages any kind of self-critic research, any analysis of negative results, and any replication study, because no one will cite you for saying \" yeah, I retested this, and it does seem to work as they originally said... \". This discussion started with peer review, and how the whole academic publishing is built around this concept. Now is the time to criticize it. Since scientists are forced to compete for attention, we have turned peer review from the supportive and self-healing process it should be into the most unpleasant part of doing research. To be fair, not all reviewers are nasty, and when we do, I'm arguing is more often than not because we are forced by the system.","title":"\ud83e\udd15 The symptoms"},{"location":"essays/academia-oss/#the-new-paradigm","text":"I believe the root of the problem in this picture should be clear by now. \u26a0\ufe0f The incentives for scientists are not aligned with the purpose of Science. So, how do we realign the incentives of scientists with the original purpose of Science, and make it better for everyone? Honestly, I don't know. But I think we can take some ideas from the FOSS community to at least foster some good practices which I believe might put us on the right track. The idea starts with embracing Openness in the whole process of scientific discovery and innovation. This is not my original idea, of course, there are some commonly shared principles of \"open science\" in the academic community. This is one possible way to express them: Open Methodology : Document the application of methods and the entire process behind them as far as practicable and relevant. Open Source : Use open source technology (software and hardware) and open your own technologies. Open Data : Make the data freely and easily available. Open Access : Publish openly and make publications usable and accessible to everyone. Open Peer Review : Provide peer review in an open and public forum. Open Educational Resources : Use free and open materials for education and in university teaching. In this form, these principles are quite abstract, and there are many ways in which they could be implemented. There are plenty of degrees of \"open science\" like publishing in open access journals managed by non-profit organizations, publishing pre-prints before submitting to \"traditional\" journals, and all the good practices around making data and protocols publicly available. I want to focus on some key ideas I think could be fruitful to try, without implying that this is the absolute solution to this problem, but rather a small part of a much larger paradigm shift that Science has to undertake.","title":"\u2b50 The new paradigm"},{"location":"essays/academia-oss/#the-practices","text":"These are my proposals. Most of them relate specifically to the peer-review process because, as I said before, this process is a pillar of the scientific process, but also because I think this is the one place where we as a community can innovate the most, without requiring government grants or changing the way bureaucratic institutions work. The peer-review process is at the base of the entire scientific process and any major change in its functioning could have a massive impact up the chain.","title":"\u2699\ufe0f The practices"},{"location":"essays/academia-oss/#1-public-reviews","text":"Let's start by acknowledging that single- and double-blind reviews are more harmful than helpful. These measures are supposed to shield reviewers and authors from future retaliation and disallow any form of favouritism, which should make the review process more just and honest. In practice, they shield reviewers from criticism and make the whole review process less transparent. I propose to turn this concept around and make all reviews completely public. We have to trust we are all reasonable individuals and professional scientists, who should be able to provide objective judgment without favouritism. But if we don't, then our reviews themselves are public, and subject to review and criticism. \ud83d\udc49 This is very easy to implement with any workflow that allows posting comments on a public forum. Note that I don't necessarily mean that anyone can review (this is discussed further down) but even if only specific \"official\" reviewers are assigned to a paper, their comments and their identity and credentials should be public.","title":"1\ufe0f\u20e3 Public reviews"},{"location":"essays/academia-oss/#2-continuous-peer-review","text":"This idea ties in with the previous point. Currently, almost all peer-review (that I'm aware of) happens in the context of some specific conference or journal. What I'm proposing here is to detach the peer-review process from any journal or conference and make it instead integral to the paper. Every paper would carry around the Internet with all its reviews, and if rejected at some previous point, a future conference or journal editor would have access to the full history of reviews and changes to reconsider the paper for \"mainstream\" publication (we'll talk more about what this means later). \ud83d\udc49 I can see this happening similar to how issues are handled in Github. You publish a paper, and potential reviewers would open \"issues\" against it, one for each important thing to address. Issues would be discussed and worked on in public and there would a history of every change introduced into the paper with links to which issues are being fixed. Since no paper is perfect, conferences and journal editors should not aim for publishing issue-free papers, but rather papers that show a healthy list of open and closed issues and demonstrable usefulness in their current state. A healthy list of open and closed issues would be an indication of a solid paper, the same way as for software.","title":"2\ufe0f\u20e3 Continuous peer-review"},{"location":"essays/academia-oss/#3-encouraging-reviews","text":"The next problem I want to tackle consists of how to kick-start the reviewing process. Once we detach reviews from specific conferences or journals, how can we guarantee everyone has access to good reviews? For sure, rockstar scientists will get thousands of reviews but what about junior researchers who are just starting? One idea is to see reviewing as an integral part of the scientific career. Researchers should be evaluated also in terms of how much value they put back to the community, and one way senior scientists can contribute is to review junior scientists. We should be proud to put in our CVs how many reviews we have given. And good reviews, which are in turn evaluated positively by the author and other reviewers, should count towards one's scientific output. \ud83d\udc49 Scientists would get a \"badge\" with the number reviews they have given, and display it on their homepage, their LinkedIn or ResearchGate profile, etc. This badge would link to some online list that links back to all reviews. This could be maybe hosted our ORCID profiles or any similar non-profit initiative. Also, senior researchers are part of a community, and are often connected with like-minded individuals in other institutions and countries. It should be part of their work to look up for each other's students and junior researchers. And yes, someone will say \" but then you can give a good review to my students if I give a good review to yours \". Again, this is why all review is, first and foremost, public in nature.","title":"3\ufe0f\u20e3 Encouraging reviews"},{"location":"essays/academia-oss/#4-qualitative-evaluations","text":"Now let's move on to specific review formats. Too often I see very long lists of checkboxes and 1-5 ratings, etc. I believe there is value in having a structured evaluation template, to make sure we more or less agree on what are the core issues we should care about. But going to the extreme of having 10 different ratings for a paper is insane! What is the difference between 6 and 7? When putting reviewers under the pressure of giving numerical scores, we are asking them to unconsciously introduce all the biases they have about that particular problem or field or approach or author. There is simply no objective way to numerically compare two different papers. A good research paper needs to have a solid methodology (correctly apply the principles of science as it is common practice in that field), provide relevant results and conclusions (either positive or negative), be feasible to reproduce by independent researchers, and have a clear presentation. Either the paper is good enough to be considered publishable, if all these aspects are covered, or it isn't. That's it. \ud83d\udc49 I prefer a simple evaluation form that asks: \" Is this aspect of the paper up to the scientific standard? , and a piece of free-form text for you to explain what is lacking in each aspect. Methodology [x] \ud83d\udc4d [ ] \ud83d\udc4e Results [x] \ud83d\udc4d [ ] \ud83d\udc4e Reproducibility [ ] \ud83d\udc4d [x] \ud83d\udc4e Presentation [ ] \ud83d\udc4d [x] \ud83d\udc4e A specific conference or journal might want to evaluate the potential impact or significance of a paper before accepting it for publishing. But impact or significance is not what Science is about. There are however legitimate cases where impact or significance is important. If you have to allocate a restricted pool of resources (e.g., grant money) of course you want to evaluate impact. Yet, I argue this is not part of the peer-review process, but a posterior analysis that each institution or publisher should do based on their specific criteria. Peer-review should be a process by which the scientific community as a whole evaluates that some research is sound science, irrespective of idiosyncrasies.","title":"4\ufe0f\u20e3 Qualitative evaluations"},{"location":"essays/academia-oss/#5-self-publishing","text":"Now that the peer-review process is completely detached from the \"mainstream\" publishing industry, who decides when is a paper ready to be published? Well, of course, the authors! It is up to the authors to determine that, given all the feedback received, they consider their work is production-ready. \ud83d\udc49 All papers would be published first in draft-mode, perhaps even before being completely written. During the draft phase, you collect all the feedback you can from peer reviews and work on the issues you consider more relevant. When you feel it is good enough (possibly because most of the recent reviews are favourable) you hit that Publish button and create a release. If some errors appear, later on, you fix them and publish another release. \ud83d\udc49 What becomes citable then? Easy, each release of each paper gets a unique DOI that will forever point to that exact version, together with all its metadata and reviews. If I cite something yours and criticize it, and you later fix it, that's OK. My critic is still valid because it points to a previous version that is indexable, and the fact that you fixed it only speaks higher of you! But wait, won't authors publish a lot of low-effort papers to engross their CVs? Well, maybe someone will but, who cares? We started by saying that, intuitively, citations should be a good measure of scientific quality. This is still true in this format. If I release a bunch of crappy papers, no one will cite them. And also, who thinks CVs are useful? Anyone trying to evaluate me as a researcher would not look into some list of titles and numbers I pasted in a Word document. They would go to my researcher profile and see my most significant work, the reviews it has received, and how my whole research process works! This doesn't mean that I get to decide my work is relevant, however. This just means I get to decide my work is ready to be consumed by the scientific community. The community will still judge my work's relevance by citing it, criticizing it, and in two more ways I left for the end.","title":"5\ufe0f\u20e3 Self-publishing"},{"location":"essays/academia-oss/#6-conferences-for-networking","text":"Now that all papers are being published by their authors, what's the purpose of scientific conferences? We can now recover their original purpose. Conferences were created as a medium to get like-minded scientists together to share their experiences and to discuss the most relevant problems of their field. But as conferences became more and more a mainstream path for getting published, their organization has become more and more about managing the peer-review process. Now that peer-review is detached from conferences, their organizers are free to focus on scooping what are the most interesting topics and the most significant results in those topics and invite those speakers they believe will the bring the biggest value. Which papers get to be presented? I think we could deal with that in two ways: \ud83d\udc49 As an organizer of a thematic conference, I would spend half of the year looking around for interesting papers to invite their authors. They would still pay for their accommodations (or their institutions would) and they would come to enjoy what's best in every conference, the networking. \ud83d\udc49 I could also open a call for papers, as usual, but authors would submit papers that are already reviewed and released. My role would be to decide, based primarily on thematic fit, what I think is more relevant for my community. \ud83d\udc49 There is even no need to attach participation to a published paper. Authors could simply submit \"talks\", possibly backed by one or more papers that support their submission, as it is already common in some conferences. This would completely reshape what conferences are for (at least in my field). There is no reason why we should wait to the top conferences of the year to be able to read papers. We would go to conferences for the chance to talk with the researchers we admire about their work. And someone asks, but how would conferences compete if they have no publication rights? Well, I argue this would be very good. Conference organizers would have to compete on the grounds of providing a better environment for networking, interesting events, nice amenities, but no one would have a monopoly on the knowledge itself. There is even no reason why the same talk cannot be presented in more than one conference if enough people are willing to listen.","title":"6\ufe0f\u20e3 Conferences for networking"},{"location":"essays/academia-oss/#7-journals-for-socializing","text":"And finally we come round to the original culprit, the infamous research journal. Now that papers are published openly, what are journals good for? Well, what they were originally designed for, socializing research! Journals were created as a means for academic societies to collect the most relevant research in a given community and publish it for a larger audience. Then commercial publishers arrived and turned science into a business, and journals became paywalled gatekeepers of knowledge, that require original research often paid with public money that they resell again for public money. The largest academic publishers often state they have costs to cover, but there is plenty of evidence that they make a significant profit. And that's OK, but if I as a journal editor want to make a profit, I'm gonna have to innovate. \ud83d\udc49 Like conferences, I could scoop around and feature the most interesting papers in some thematic issues, maybe ask the authors to give some new comments on them, prepare explainer videos, add links, and put some effort into turning those \"raw\" papers into beautifully typeset pages. \ud83d\udc49 SOTA reviews would be a nice fit for journals as well. These are not original research papers, but they often provide a lot of value by analyzing a bunch of papers and giving advice on common trends or highlighting interesting lines for future research. \ud83d\udc49 I would also have editorial articles specifically written for an issue that could summarize in layman terms about a particular subject, to introduce it to a larger audience. I would even pay scientists that are good communicators for this work. Journals would have to compete on the grounds of being good at selecting topics and papers to socialize, and provide some additional editorial value. In any case, original research papers would be owned only by their authors and would be published always with some public license (e.g., Creative Commons). This would ensure that Science belongs to the ones who ultimately pay for it, that is, society.","title":"7\ufe0f\u20e3 Journals for socializing"},{"location":"essays/academia-oss/#the-system","text":"Putting all these ideas together in a functioning system will require a lot of work. From the infrastructure point of view, I envision something aking Github, a repository of open access papers with builtin comments, reviewing, and social features. Ideally, it would also have a web UI for editing, similar to Overleaf and, of course, fully integrated with Git. I understand this might not be the best solution for academic communities that are not very closely related to software, i.e., social sciences, mostly because it could pose a significant learning curve for their members and become more of a hinder than a help. From the social point of view, kick-starting such a system would require a massive community effort. And not because of the infrastructure cost, that's the minor issue. I think the largest obstacle for this kind of paradigm shift is that a large part of the community would have to move away simultaneously from journals and conferences as the main publication channels. Otherwise, the few that start the effort will be completely disconnected from the rest of the community. I can see this happening as an effort from, say, the AI community, or any other technically-savvy collective. Tomorrow morning, all the senior scientists that publish in ICLR, ICML, NeurIPs, and ACL, could suddenly decide they want to go fully open. It would require the conference organizers to support the initiative as well. Instead of opening a call for papers, the conferences could decide that they would open a call for submissions, which should peer-reviewed and published in this new format. Some non-profit organization could be formed from within the community to provide the infrastructure. Since there will be some operative costs, this platform would require some payment, but it would be very small compared to publishing fees in most major open access journals. Also, some of the big players in the industry could support this initiative by providing hosting and infrastructure for free. This would be a big PR boost to these companies.","title":"\ud83d\udcbb The system"},{"location":"essays/academia-oss/#the-metrics","text":"We started this discussion by saying that an intuitively good metric to estimate scientific impact, i.e., citations, had become an objective and thus lost their entire meaning. But citations are not an inherently bad metric, it's just when we use citations as the one quantitative metric to compare individual researchers that we miss the entire point. Likewise, the fact that we self-publish all our papers doesn't mean that being featured in a major conference or journal is worthless. On the contrary, when everything we publish is open, being featured in a mainstream publication becomes an even better measure of impact, because it is no longer tied to my financial capacity or any other unfair advantage I might enjoy in the community. This would be very good for Third World researchers, who produce valuable science, but are often cutoff from mainstream publication for reasons completely unrelated to the quality of the research. If we can restructure the incentives and processes of Science such that they are aligned with the purpose of Science as primarily a means to improve human life, everything else would fall into place. Once researchers are free from predatory publishing practices, meaningless numerical statistics and unhealthy competition, I believe we will all focus what we love most, doing sound research for the good of mankind. Then, all those metrics that are used today will regain their meaning. Being invited to a top-tier conference would mean that what your community wants to hear from you. Being featured in a top-tier journal would mean that some editors consider your work is high-quality. And being cited often would mean that your research is producing real impact, that you are becoming a giant on whose shoulders others can stand. \ud83d\udde8\ufe0f This is a topic of which I'm very passionate about, and I want to hear your feedback. If you want to discuss this topic with me, give me a shout in Twitter . \ud83d\udeab If you find some typo, error or you have any suggestion to help me improve this page, you can suggest and edit or open an issue in Github .","title":"\ud83d\udcca The metrics"},{"location":"essays/opinions/","text":"How opinions should work \u00b6 Can an opinion be wrong? What would it mean to say \"yes\", or \"no\", for that matter? And what is an opinion, that makes it different from a fact? I'm gonna try to answer these questions from a very opinionated point-of-view (see what I did there?) by crafting some definitions that I think are, at least, somewhat helpful to drive this discussion. But keep in mind, this is only my opinion, in a very strict sense of opinion, that I'm gonna explain to you. To define what is an opinion , I think we have to contrast it with the complementary concept of fact . I'm gonna say there are two types of statements you can make about any subject or object: opinions and facts , and these are disjoint sets. What is a fact? \u00b6 I'm gonna define a fact as an is-statement , for example, the snow is white , or coffee is bad for health . Alternatively, I'm gonna say facts are objective statements. Facts can be right or wrong, in a very strict sense: whether they correspond to objective reality or not. However, just trying to define what objective reality is, is tremendously complex. So I will assume that most of us share a common idea of what objective reality is, even if we don't agree that some statements are part of, or can even refer to, that reality. More generally, facts can have a degree of accuracy, so they are neither right nor wrong, but somewhere in between. For example, when you say Earth is round , that is not exactly right, but is a far better description of Earth than, say, Earth is flat . Hence, among two contradictory facts (two facts that cannot be true at the same time), I'll generally say that the most accurate is \"right\", and the other, \"wrong\", but this is nuanced and contextual. The reason we care about facts, is that they are useful. Facts allow us to communicate with each other about reality. Most of us, I think, we'll agree on a large common set of facts, such that something seems to be pulling you to the ground, and that you cannot breath under water. However, many of us will not agree on some facts, even if they are a very accurate description of objective reality. This disagreement can be due to several reasons. The most obvious one, is that we have different tools to observe objective reality, such that we can be seeing different portions of the same phenomenon and thus disagreeing by a simple mismatch of data. A more complex reason is that, even when starting with the same data, we can have different reasoning processes, some better than others, and thus we can reach different conclusions. Finally, a less obvious reason, is that we don't agree on what a fact is, and one of us is taking a fact as an opinion, or an opinion as a fact. The good thing about facts is that there is, in principle, an objective way to solve any disagreement. We just have to show each other our data and describe our thought processes. This doesn't mean we can always know if a fact is right or wrong, sometimes we won't even have the technology to be able to collect or analyze that data. But at least, we should be able to agree if a fact seems right from the collective data we have gathered, or if we cannot decide yet. This is, in part, how Science works. With all this in mind, we can reach an alternative definition for facts. I would say: \ud83d\udcdd Facts are the type of statements about objective reality that can be, at least in theory, agreed upon by anyone who has access to the same set of observations and uses a principled reasoning procedure that doesn't produce contradictions. What is an opinion? \u00b6 If facts are is-statements , then I'll say opinions are ought-statements . For example, education ought to be free , or people ought to be able to say whatever they like . Alternatively, I'll say opinions are subjective statements. Do we need opinions in the first place? Can't we just get along with facts? Well, sadly, there are fundamental questions we can only answer if we allow subjective statements. The most obvious case is possibly organizing a community. For example, if you want to build a society that allows people to live together in harmony, you'll probably want a rule like people ought to respect each other's freedom . Now there's a funny thing that happens with statements. It's called the \"Hume's guillotine\", and it basically says you cannot produce an ought-statement from a chain of is-statements by pure logical reasoning. For example, you might want to say weather is cold today , and cold is bad for health , so you ought to wear a jacket . Seems reasonable, right? But the thing is, you're making an assumption, which is that I want to protect my health . And you cannot derive that as a fact from objective reality. At most, you could say you ought to protect your health , and so, you need to introduce an opinion that we need to agree on. So, bottom line is, opinions, as I have defined them here, cannot be proven logically from facts. There is no science that can validate if an opinion is \"right\", starting only from pure objective truths. You have to start with an opinion somewhere, that is taken at face value and accepted as a universal truth. A subjective truth. Keep in mind, though, that some things which may look as opinions are actually facts, and vice-versa. For example, if you say, I like coffee , that is not an opinion, that is a fact about yourself. It is a fact because we can precisely define what like means, and we can agree within reasonable accuracy, that the observations we all perceive of your coffee consumption behavior are consistent with what we refer to as liking . And if you say, democracy is a successful sociopolitical system , that is also not an opinion, it is a fact, provided you can describe what \"successful\" means in this context. It can either be a true or a false fact, depending on which metrics you pick, or somewhere in between. However, whatever the case, that fact doesn't directly imply that people ought to prefer living in a democracy , by Hume's guillotine. That is an opinion, and thus cannot be scientifically proven from objective reality. Hence, to conclude this section, I think we can come up with an alternative definition for opinion. I would say: \ud83d\udcdd Opinions are the type of statements about subjective reality, for which there are equally-valid alternative statements, that cannot be differentiated by using just true facts and any principled reasoning procedure. \u26a0\ufe0f Before we move on, I want to stress that the distinction between facts and opinions is not a linguistic issue. Is not the case that something is a fact because it is expressed as an is-statement . Or vice-versa, that I can turn a fact into an opinion by changing the way it is expressed. These syntactical \"rules\" are just a tool for us to communicate the semantic meaning of the word \"fact\" and \"opinion\", but they are not the definition. The definitions, as hard as they are to get right, rest on the key distinction that facts are statements that can be proven right or wrong, independent of your beliefs; and opinions are exactly the opposite, statements that cannot be evaluated outside of a belief system. Agree to disagree? \u00b6 Can an opinion be wrong? We could say an opinion is \"trivially wrong\" when it contradicts objective reality, or simply put, if it contradicts some true fact(s). For example, if you say electrons ought to be positively charged ; but I'll argue that those are, at the very least, useless opinions. More importantly, then, can an opinion be wrong when it doesn't contradict objective reality? For example, if you say people ought to be able to decide if they want to vaccinate their children ? Yes, vaccines are proven to help fight diseases, that is a fact. But we have to agree, as before, that people ought to do what's best for public health , and that, again, is an opinion (not necessarily held by a majority). If we say useful opinions cannot be measured against objective reality, then we can only compare opinions with other opinions. Can we agree on a set of basic opinions that determine which other opinions are valid or not? I'm gonna propose a few. I'll start by proposing that opinions ought to be only about subjective reality . This means that, in my view, you cannot have an opinion on something that can be proven scientifically. The Earth is either round or flat, and you're not entitled to believe it is any different. You can disagree with the science used to determine it. You can be a skeptic and attempt to do the experiments yourself. But you cannot believe that people are entitled to decide what charge of the electron, or what shape of planet, they want to believe in. It is not easy, at all, to determine what can be scientifically proven, so there will be statements which, at some points in time, some people will rightfully argue there is still room for opinions. But I think that in most of the statements that matter for practical purposes, even if we don't know their truth value, we can pretty much agree if they are about objective or subjective reality. Next, I'll propose that opinions ought to acknowledge their status . Hence, you cannot have an opinion, and then believe that opinion to be anything else than an opinion, i.e., you are not allowed to believe your opinion is a fact. But it's OK as long as you understand all you have is an opinion, and thus, it cannot be compared to a fact. Finally, I'm gonna argue that people ought to be allowed to have opinions that do not conform to the dominant moral dogma . I'm totally OK with, and I will fight for, people's right to hold an unpopular opinion, even if I don't agree with that specific opinion. All in all, if an opinion contradicts one of the previous three statements, I'm gonna call it a \"wrong opinion\". For example, from my point of view, you cannot say it's my opinion that vaccines don't work , but you can say people should be able to decide if they want to vaccinate . Maybe I won't agree with you, but I'll defend your right to have an opinion I disagree with, and I'll do my best to show my point of view. And this is my opinion on how opinions ought to work. \ud83d\udde8\ufe0f This is a topic of which I'm very passionate about, and I want to hear your feedback. If you want to discuss this topic with me, give me a shout in Twitter . \ud83d\udeab If you find some typo, error or you have any suggestion to help me improve this page, you can suggest and edit or open an issue in Github .","title":"Opinions"},{"location":"essays/opinions/#how-opinions-should-work","text":"Can an opinion be wrong? What would it mean to say \"yes\", or \"no\", for that matter? And what is an opinion, that makes it different from a fact? I'm gonna try to answer these questions from a very opinionated point-of-view (see what I did there?) by crafting some definitions that I think are, at least, somewhat helpful to drive this discussion. But keep in mind, this is only my opinion, in a very strict sense of opinion, that I'm gonna explain to you. To define what is an opinion , I think we have to contrast it with the complementary concept of fact . I'm gonna say there are two types of statements you can make about any subject or object: opinions and facts , and these are disjoint sets.","title":"How opinions should work"},{"location":"essays/opinions/#what-is-a-fact","text":"I'm gonna define a fact as an is-statement , for example, the snow is white , or coffee is bad for health . Alternatively, I'm gonna say facts are objective statements. Facts can be right or wrong, in a very strict sense: whether they correspond to objective reality or not. However, just trying to define what objective reality is, is tremendously complex. So I will assume that most of us share a common idea of what objective reality is, even if we don't agree that some statements are part of, or can even refer to, that reality. More generally, facts can have a degree of accuracy, so they are neither right nor wrong, but somewhere in between. For example, when you say Earth is round , that is not exactly right, but is a far better description of Earth than, say, Earth is flat . Hence, among two contradictory facts (two facts that cannot be true at the same time), I'll generally say that the most accurate is \"right\", and the other, \"wrong\", but this is nuanced and contextual. The reason we care about facts, is that they are useful. Facts allow us to communicate with each other about reality. Most of us, I think, we'll agree on a large common set of facts, such that something seems to be pulling you to the ground, and that you cannot breath under water. However, many of us will not agree on some facts, even if they are a very accurate description of objective reality. This disagreement can be due to several reasons. The most obvious one, is that we have different tools to observe objective reality, such that we can be seeing different portions of the same phenomenon and thus disagreeing by a simple mismatch of data. A more complex reason is that, even when starting with the same data, we can have different reasoning processes, some better than others, and thus we can reach different conclusions. Finally, a less obvious reason, is that we don't agree on what a fact is, and one of us is taking a fact as an opinion, or an opinion as a fact. The good thing about facts is that there is, in principle, an objective way to solve any disagreement. We just have to show each other our data and describe our thought processes. This doesn't mean we can always know if a fact is right or wrong, sometimes we won't even have the technology to be able to collect or analyze that data. But at least, we should be able to agree if a fact seems right from the collective data we have gathered, or if we cannot decide yet. This is, in part, how Science works. With all this in mind, we can reach an alternative definition for facts. I would say: \ud83d\udcdd Facts are the type of statements about objective reality that can be, at least in theory, agreed upon by anyone who has access to the same set of observations and uses a principled reasoning procedure that doesn't produce contradictions.","title":"What is a fact?"},{"location":"essays/opinions/#what-is-an-opinion","text":"If facts are is-statements , then I'll say opinions are ought-statements . For example, education ought to be free , or people ought to be able to say whatever they like . Alternatively, I'll say opinions are subjective statements. Do we need opinions in the first place? Can't we just get along with facts? Well, sadly, there are fundamental questions we can only answer if we allow subjective statements. The most obvious case is possibly organizing a community. For example, if you want to build a society that allows people to live together in harmony, you'll probably want a rule like people ought to respect each other's freedom . Now there's a funny thing that happens with statements. It's called the \"Hume's guillotine\", and it basically says you cannot produce an ought-statement from a chain of is-statements by pure logical reasoning. For example, you might want to say weather is cold today , and cold is bad for health , so you ought to wear a jacket . Seems reasonable, right? But the thing is, you're making an assumption, which is that I want to protect my health . And you cannot derive that as a fact from objective reality. At most, you could say you ought to protect your health , and so, you need to introduce an opinion that we need to agree on. So, bottom line is, opinions, as I have defined them here, cannot be proven logically from facts. There is no science that can validate if an opinion is \"right\", starting only from pure objective truths. You have to start with an opinion somewhere, that is taken at face value and accepted as a universal truth. A subjective truth. Keep in mind, though, that some things which may look as opinions are actually facts, and vice-versa. For example, if you say, I like coffee , that is not an opinion, that is a fact about yourself. It is a fact because we can precisely define what like means, and we can agree within reasonable accuracy, that the observations we all perceive of your coffee consumption behavior are consistent with what we refer to as liking . And if you say, democracy is a successful sociopolitical system , that is also not an opinion, it is a fact, provided you can describe what \"successful\" means in this context. It can either be a true or a false fact, depending on which metrics you pick, or somewhere in between. However, whatever the case, that fact doesn't directly imply that people ought to prefer living in a democracy , by Hume's guillotine. That is an opinion, and thus cannot be scientifically proven from objective reality. Hence, to conclude this section, I think we can come up with an alternative definition for opinion. I would say: \ud83d\udcdd Opinions are the type of statements about subjective reality, for which there are equally-valid alternative statements, that cannot be differentiated by using just true facts and any principled reasoning procedure. \u26a0\ufe0f Before we move on, I want to stress that the distinction between facts and opinions is not a linguistic issue. Is not the case that something is a fact because it is expressed as an is-statement . Or vice-versa, that I can turn a fact into an opinion by changing the way it is expressed. These syntactical \"rules\" are just a tool for us to communicate the semantic meaning of the word \"fact\" and \"opinion\", but they are not the definition. The definitions, as hard as they are to get right, rest on the key distinction that facts are statements that can be proven right or wrong, independent of your beliefs; and opinions are exactly the opposite, statements that cannot be evaluated outside of a belief system.","title":"What is an opinion?"},{"location":"essays/opinions/#agree-to-disagree","text":"Can an opinion be wrong? We could say an opinion is \"trivially wrong\" when it contradicts objective reality, or simply put, if it contradicts some true fact(s). For example, if you say electrons ought to be positively charged ; but I'll argue that those are, at the very least, useless opinions. More importantly, then, can an opinion be wrong when it doesn't contradict objective reality? For example, if you say people ought to be able to decide if they want to vaccinate their children ? Yes, vaccines are proven to help fight diseases, that is a fact. But we have to agree, as before, that people ought to do what's best for public health , and that, again, is an opinion (not necessarily held by a majority). If we say useful opinions cannot be measured against objective reality, then we can only compare opinions with other opinions. Can we agree on a set of basic opinions that determine which other opinions are valid or not? I'm gonna propose a few. I'll start by proposing that opinions ought to be only about subjective reality . This means that, in my view, you cannot have an opinion on something that can be proven scientifically. The Earth is either round or flat, and you're not entitled to believe it is any different. You can disagree with the science used to determine it. You can be a skeptic and attempt to do the experiments yourself. But you cannot believe that people are entitled to decide what charge of the electron, or what shape of planet, they want to believe in. It is not easy, at all, to determine what can be scientifically proven, so there will be statements which, at some points in time, some people will rightfully argue there is still room for opinions. But I think that in most of the statements that matter for practical purposes, even if we don't know their truth value, we can pretty much agree if they are about objective or subjective reality. Next, I'll propose that opinions ought to acknowledge their status . Hence, you cannot have an opinion, and then believe that opinion to be anything else than an opinion, i.e., you are not allowed to believe your opinion is a fact. But it's OK as long as you understand all you have is an opinion, and thus, it cannot be compared to a fact. Finally, I'm gonna argue that people ought to be allowed to have opinions that do not conform to the dominant moral dogma . I'm totally OK with, and I will fight for, people's right to hold an unpopular opinion, even if I don't agree with that specific opinion. All in all, if an opinion contradicts one of the previous three statements, I'm gonna call it a \"wrong opinion\". For example, from my point of view, you cannot say it's my opinion that vaccines don't work , but you can say people should be able to decide if they want to vaccinate . Maybe I won't agree with you, but I'll defend your right to have an opinion I disagree with, and I'll do my best to show my point of view. And this is my opinion on how opinions ought to work. \ud83d\udde8\ufe0f This is a topic of which I'm very passionate about, and I want to hear your feedback. If you want to discuss this topic with me, give me a shout in Twitter . \ud83d\udeab If you find some typo, error or you have any suggestion to help me improve this page, you can suggest and edit or open an issue in Github .","title":"Agree to disagree?"},{"location":"essays/teaching/","text":"Why I am a teacher \u00b6 When people ask me who I am, the short answer is \"a teacher\", even though I do a bunch of other stuff. This is why. I didn't know I wanted to be a college teacher until about 3rd year of my major. Even then, the thought came slowly, something like \"yeah teaching could nice, but I may enjoy something else as well\". By the end of my 5th year I was certain there was nothing else that could fill me up. I've been in front of a classroom ever since, at least twice a week one semester a year. And I've loved every minute of it. Students are the same everywhere. They have hopes, dreams, and a lot of misconceptions. They come thinking they want to be something (an engineer, a computer scientist, a journalist, a lawyer, ...), because they think that choice will lead them to do something (solve problems, work at a large company, travel the world, ...). So they focus on that search: what is the something they want to do, hence, the something they want to be? My main task is to try and convince them otherwise. The search is not about a something . It's about a someone . You need to come to college to discover who you want to be instead of what you want to do. Engineer, computer scientist, lawyer, all of those are just labels that somehow try to average over the set of things that people who label themselves that way like to do. Don't get me wrong, labels are important. They help us organize and understand the world. But if there is one occasion when you don't want a simple label to smooth away all the tiny details, is when choosing who you want to be for life. \u2b50 So when people ask me \"are you an engineer, a scientist, or a philosopher?\" I answer yes. I'm mostly a scientist, because I do more research than the average engineer; but I'm also an engineer, because I solve more problems than the average philosopher; and, I'm also a philosopher, because I like to think more about the implications of my decisions than the average scientist. I'm also a lot of other things, if you ask. It's not that I'm somehow \"better\" than any of these individual labels, it's simply that I choose to be my own brew of these \"things\", taking from each what I like and dumping what I don't. To a simple question ( what are you?) I can only give a simple answer (yes). If you want the details, you'll have to ask the deeper question: who are you? My first day in class every year, I like to throw a simple question at my students: who do you want to be? Most of them answer with a combination of whats. I want to be this or I want to do that. Over the year, some start to discover they want to be someone , not just something. They start dumping the labels and start answering to this question not with things (I want to be a programmer) but with choices (I want to solve this problem, I want to cure this disease, I want to build this thing). Those choices become the who they strive for. Eventually, even if unconsciously, most reach this state. A tiny fraction of them will consciously acknowledge it. And an even smaller fraction, maybe one or two a year, sometimes none, will come one day and say to me something like \"thank you for helping me find my who\", even if not with those exact words. And that's it. That small moment when I realize someone found a better version of themselves, and I had a tiny bit of influence. That's my reward. That's who I am.","title":"Why I love teaching"},{"location":"essays/teaching/#why-i-am-a-teacher","text":"When people ask me who I am, the short answer is \"a teacher\", even though I do a bunch of other stuff. This is why. I didn't know I wanted to be a college teacher until about 3rd year of my major. Even then, the thought came slowly, something like \"yeah teaching could nice, but I may enjoy something else as well\". By the end of my 5th year I was certain there was nothing else that could fill me up. I've been in front of a classroom ever since, at least twice a week one semester a year. And I've loved every minute of it. Students are the same everywhere. They have hopes, dreams, and a lot of misconceptions. They come thinking they want to be something (an engineer, a computer scientist, a journalist, a lawyer, ...), because they think that choice will lead them to do something (solve problems, work at a large company, travel the world, ...). So they focus on that search: what is the something they want to do, hence, the something they want to be? My main task is to try and convince them otherwise. The search is not about a something . It's about a someone . You need to come to college to discover who you want to be instead of what you want to do. Engineer, computer scientist, lawyer, all of those are just labels that somehow try to average over the set of things that people who label themselves that way like to do. Don't get me wrong, labels are important. They help us organize and understand the world. But if there is one occasion when you don't want a simple label to smooth away all the tiny details, is when choosing who you want to be for life. \u2b50 So when people ask me \"are you an engineer, a scientist, or a philosopher?\" I answer yes. I'm mostly a scientist, because I do more research than the average engineer; but I'm also an engineer, because I solve more problems than the average philosopher; and, I'm also a philosopher, because I like to think more about the implications of my decisions than the average scientist. I'm also a lot of other things, if you ask. It's not that I'm somehow \"better\" than any of these individual labels, it's simply that I choose to be my own brew of these \"things\", taking from each what I like and dumping what I don't. To a simple question ( what are you?) I can only give a simple answer (yes). If you want the details, you'll have to ask the deeper question: who are you? My first day in class every year, I like to throw a simple question at my students: who do you want to be? Most of them answer with a combination of whats. I want to be this or I want to do that. Over the year, some start to discover they want to be someone , not just something. They start dumping the labels and start answering to this question not with things (I want to be a programmer) but with choices (I want to solve this problem, I want to cure this disease, I want to build this thing). Those choices become the who they strive for. Eventually, even if unconsciously, most reach this state. A tiny fraction of them will consciously acknowledge it. And an even smaller fraction, maybe one or two a year, sometimes none, will come one day and say to me something like \"thank you for helping me find my who\", even if not with those exact words. And that's it. That small moment when I realize someone found a better version of themselves, and I had a tiny bit of influence. That's my reward. That's who I am.","title":"Why I am a teacher"},{"location":"essays/team-playing/","text":"It's all about team-playing \u00b6 Modern higher education is all about competences and skills. In the process, we are losing some very bright people who just don't fit this narrow-minded model of professionalism. Modern education started with the Industrial Revolution and the need to graduate tons of skilled workers to carry on the same tasks over and over. Previously, education was only for the brightest and/or luckiest, and usually consisted of a very custom path through which a tutor would guide you. Nowadays in universities all around the world, we have reduced students to numbers, grades, percentages, as if we were producing computer chips or combustion engines. Efficiency is all that matters. To achieve the highest possible efficiency, all around the world we educators have become engineers of sorts. We designed what we call a \"model of the professional\", which is a set of skills and competencies that an abstract ideal professional should have. Then we designed an evaluation metric he micro-average of a ton of super-narrow scores that measure super-specific skills such as taking a derivative or coding a recursive function. Finally, we designed a pipeline that takes students on one end and produces \"professionals\" on the other end. Those \"smart\" enough to learn to beat the system get the highest grades and are stamped with an abstract generic title of Computer Scientist, Medical Doctor, Lawyer, very much like a certificate of quality in a generic bottle of wine. This system is deeply flawed, and educators all over the world know it and have been discussing it for a long time. It's hard to change for many reasons, the least of which is the lack of teachers willing to dump the generic instruction set and craft custom learning paths for their students. I think this system is based on two basic assumptions, intuitive but flawed. Changing those assumptions could shed light on ways to improve the system. 1\ufe0f\u20e3 The first assumption is that students are a blank slate that when fed through this generic pipeline we call higher education will be magically morphed into this generic professional we designed. This is wrong for so many reasons that is hard to acknowledge it as a basic assumption of our system. \ud83d\udc49 Ask any university professor and they will all tell you the same: All students are different. They all have different skills, interests and biases. They all require a different approach to get the most out of them. And almost all of them, when given the chance and the right environment, will become the best versions of themselves. Yet time and time again we treat them as generic droids on which we can dump a generic course and expect a generic performance in return. 2\ufe0f\u20e3 The second assumption, I think, is harder to spot, because of the way the university is disconnected from real life all around the world. We educators think that society wants this \"model of the professional\" because we think that a hospital needs 100 equally generic doctors, and a software company needs 100 equally generic programmers. However, this is also wrong at many levels. \ud83d\udc49 Everywhere we ask in the industry we keep hearing the same: we need unique people with unique skills that bring something new to the team. It's like trying to build an ensemble out of 100 equal models. You get much better results with a variety of approaches to the same problem, than with an array of 100 exactly equal programs. Yet we keep translating what society asks into skills and competencies. They tell us they need unique people, and we add \"uniqueness\" to the set of generic skills we want to teach with our generic college programs! So let's dump those two assumptions and acknowledge that we have a bunch of different kids with different interests and capabilities, and we need to turn them into a bunch of different professionals with different mindsets and skills. Now the question is how on earth can we do that? As engineers, we need to design a streamlined pipeline and a proper evaluation metric. And we need to do that, unfortunately, because there are so many more students than teachers that we cannot hope to be the Aristotle to each Alexander, and that's not about to change in the near future. I think one possible strategy is to focus on a single evaluation metric, and a single skill: \ud83d\udca1 Strive to transform every student into an effective team-player. Let's take it piece by piece. Every student is different, so everyone will have a different set of potential capabilities that could make them effective team-players. If we encourage those specific capabilities on each student, we are giving each one a different learning path. This one will focus on improving her analytical skills, that one will focus on improving his management skills, the other one her social skills, and so on. Each one is focusing on their own most interesting, most desirable version of themselves. On the other hand, everyone is optimizing the same metric, being a good team-player, whatever the team. Give them back to society and they will fit in the right spot. The one spot that needs that specific mindset. Almost all low-hanging fruits that a single bright person could take are already taken, the problems that are left to solve as a society are the hard problems, and they all require teamwork. The easier problems are being automated away as I type. So, I argue, the most important skill today is being an effective team-player. If we strive to turn our students into exactly that, we are giving them the best education possible, and we are giving society the best possible return on that investment. The final question is how exactly do we do that? How do we discover what makes every student unique and valuable in a team? Isn't that the same Aristotle & Alexander dilemma? I think a possible solution is simply to let each of them discover it by themselves. As educators, instead of trying to tell everyone what to do, let's focus on designing learning environments that are comfortable for every student to explore their own skills and capabilities and decide the best way to serve the team. And let's evaluate them on co-op instead of solo so that when trying to beat the system, they will effectively optimize what we, the rest of the world, need them to be good at. \ud83d\udde8\ufe0f This is a topic of which I'm very passionate about, and I want to hear your feedback. If you want to discuss this topic with me, give me a shout in Twitter . \ud83d\udeab If you find some typo, error or you have any suggestion to help me improve this page, you can suggest and edit or open an issue in Github .","title":"Team-playing"},{"location":"essays/team-playing/#its-all-about-team-playing","text":"Modern higher education is all about competences and skills. In the process, we are losing some very bright people who just don't fit this narrow-minded model of professionalism. Modern education started with the Industrial Revolution and the need to graduate tons of skilled workers to carry on the same tasks over and over. Previously, education was only for the brightest and/or luckiest, and usually consisted of a very custom path through which a tutor would guide you. Nowadays in universities all around the world, we have reduced students to numbers, grades, percentages, as if we were producing computer chips or combustion engines. Efficiency is all that matters. To achieve the highest possible efficiency, all around the world we educators have become engineers of sorts. We designed what we call a \"model of the professional\", which is a set of skills and competencies that an abstract ideal professional should have. Then we designed an evaluation metric he micro-average of a ton of super-narrow scores that measure super-specific skills such as taking a derivative or coding a recursive function. Finally, we designed a pipeline that takes students on one end and produces \"professionals\" on the other end. Those \"smart\" enough to learn to beat the system get the highest grades and are stamped with an abstract generic title of Computer Scientist, Medical Doctor, Lawyer, very much like a certificate of quality in a generic bottle of wine. This system is deeply flawed, and educators all over the world know it and have been discussing it for a long time. It's hard to change for many reasons, the least of which is the lack of teachers willing to dump the generic instruction set and craft custom learning paths for their students. I think this system is based on two basic assumptions, intuitive but flawed. Changing those assumptions could shed light on ways to improve the system. 1\ufe0f\u20e3 The first assumption is that students are a blank slate that when fed through this generic pipeline we call higher education will be magically morphed into this generic professional we designed. This is wrong for so many reasons that is hard to acknowledge it as a basic assumption of our system. \ud83d\udc49 Ask any university professor and they will all tell you the same: All students are different. They all have different skills, interests and biases. They all require a different approach to get the most out of them. And almost all of them, when given the chance and the right environment, will become the best versions of themselves. Yet time and time again we treat them as generic droids on which we can dump a generic course and expect a generic performance in return. 2\ufe0f\u20e3 The second assumption, I think, is harder to spot, because of the way the university is disconnected from real life all around the world. We educators think that society wants this \"model of the professional\" because we think that a hospital needs 100 equally generic doctors, and a software company needs 100 equally generic programmers. However, this is also wrong at many levels. \ud83d\udc49 Everywhere we ask in the industry we keep hearing the same: we need unique people with unique skills that bring something new to the team. It's like trying to build an ensemble out of 100 equal models. You get much better results with a variety of approaches to the same problem, than with an array of 100 exactly equal programs. Yet we keep translating what society asks into skills and competencies. They tell us they need unique people, and we add \"uniqueness\" to the set of generic skills we want to teach with our generic college programs! So let's dump those two assumptions and acknowledge that we have a bunch of different kids with different interests and capabilities, and we need to turn them into a bunch of different professionals with different mindsets and skills. Now the question is how on earth can we do that? As engineers, we need to design a streamlined pipeline and a proper evaluation metric. And we need to do that, unfortunately, because there are so many more students than teachers that we cannot hope to be the Aristotle to each Alexander, and that's not about to change in the near future. I think one possible strategy is to focus on a single evaluation metric, and a single skill: \ud83d\udca1 Strive to transform every student into an effective team-player. Let's take it piece by piece. Every student is different, so everyone will have a different set of potential capabilities that could make them effective team-players. If we encourage those specific capabilities on each student, we are giving each one a different learning path. This one will focus on improving her analytical skills, that one will focus on improving his management skills, the other one her social skills, and so on. Each one is focusing on their own most interesting, most desirable version of themselves. On the other hand, everyone is optimizing the same metric, being a good team-player, whatever the team. Give them back to society and they will fit in the right spot. The one spot that needs that specific mindset. Almost all low-hanging fruits that a single bright person could take are already taken, the problems that are left to solve as a society are the hard problems, and they all require teamwork. The easier problems are being automated away as I type. So, I argue, the most important skill today is being an effective team-player. If we strive to turn our students into exactly that, we are giving them the best education possible, and we are giving society the best possible return on that investment. The final question is how exactly do we do that? How do we discover what makes every student unique and valuable in a team? Isn't that the same Aristotle & Alexander dilemma? I think a possible solution is simply to let each of them discover it by themselves. As educators, instead of trying to tell everyone what to do, let's focus on designing learning environments that are comfortable for every student to explore their own skills and capabilities and decide the best way to serve the team. And let's evaluate them on co-op instead of solo so that when trying to beat the system, they will effectively optimize what we, the rest of the world, need them to be good at. \ud83d\udde8\ufe0f This is a topic of which I'm very passionate about, and I want to hear your feedback. If you want to discuss this topic with me, give me a shout in Twitter . \ud83d\udeab If you find some typo, error or you have any suggestion to help me improve this page, you can suggest and edit or open an issue in Github .","title":"It's all about team-playing"},{"location":"guides/technical-writing/","text":"Technical Writing \u00b6 Some think writing requires inspiration, a muse, or special talent. While these may (or not) be necessary for writing a fiction masterpiece, they certainly aren't needed to write a decent technical piece. Technical writing is a skill that can be learned and perfected with practice, and there are methods to help you move forward. The main blockers I've struggled with during technical writing are: Lack of original ideas. Missing a clear and unified structure. Running out of ideas or motivation halfway through. Dull and boring writing, lacking impact. A key ingredient to overcoming these blockers is to iterate a lot. It is impossible for most of us to come up with original, coherent, and impactful ideas, all at once. Instead, you start with a bunch of unconnected ideas, and you relentlessly edit, rewrite, and improve. The other ingredient is to know when to stop. No technical article is ever ready, you just decide to publish it when it's good enough. There are two reasons why this relentless editing is difficult: It is very hard to edit your own words once you get too attached to them. The longer you spend thinking about a specific sentence, the harder it will be for you to let it go. Besides, language is contextual, every sentence is connected with its surroundings. The longer a sentence lives, there harder it is to refactor it without compromising the story in which it is embedded. Hence, we need to delay the moment when ideas grow roots to as late as possible. We want them to float freely and bounce around as long as possible. And we want them to grow roots and entrench themselves into a coherent narrative at just the right time. Here's a losely-defined process that works for me. It starts with brain-dumping the key ideas before threading an underlying structure . Then, we'll make three passes, one to fill with content , another to prune for clarity and a third one to polish for impact . And that's it, three drafts and we're ready to publish. Brain-dump key ideas \u00b6 The first step is to come up with as many ideas as possible, regardless of their quality. You've been thinking about a topic for a while, and you already have a bunch of unconnected thoughts you want to explore. Sit down and write them, one by one, as they come up to your mind. Forget about structure, order, story, coherence; nothing matters beyond individual ideas. Sometimes an idea requires one sentence, sometimes a couple, and sometimes barely a phrase. Let each idea float freely, unconnected with the rest. Describe it with just the few words you need to understand it when you come back later. It doesn't need to make sense to anyone but you. The key insight is to not spend too much time thinking about any particular idea. Do not refine them, do not rewrite them to make them clearer beyond their initial conception. Open your mind and let ideas pour out as unchallenged as you can possibly do. Thread a structure \u00b6 Now you have a heap of ideas, some potentially good, some definitely bad, some ugly. They are floating freely, unconnected, unperturbed by their companions. Most of these ideas won't see the light of day, but you still don't know which ones will make the cut. The next step is to thread a meaningful structure around the ones that really matter. Your task is to reorder ideas so that similar topics start to cluster and a structure begins to emerge. Since ideas are slim and free, you can move them around, group them into sections and subsections as you wish. Play around with different structures here and there; maybe save copies to revisit later. Resist explaining or refining the ideas. Keep them slim. Remember that fat ideas grow roots. They become heavy and hard to reorder into a different structure. They attach to their context. As you play with different structures, you will converge into a definite form, a story where some ideas fit neatly, and some don't. Cut these loose without a second thought. Ideas are cheap anyway. That's why we want them to be slim in the first place, and why we didn't let ourselves get attached to them. At the end of this step you'll have a blueprint, a kind of skeleton that threads across all the remaining ideas, connecting them into a larger narrative. You'll have removed any idea that didn't fit, and possibly sprinkled here and there a few supporting ones where the structure felt less solid. They are ready to grow roots. Fill with content \u00b6 Now that you have a solid structure, it's time to let ideas flourish. Expand every idea into a paragraph, or two, or three, as long as you need. You're only interested in quantity now, not quality. Try and respect the structure you already have in place. Why?Because if you deviate too much from it, you'll be back at square one, lacking a clear structure, but with bloated paragraphs instead of slim ideas. And we already know this is bad: ideas are easy to move around, but paragraphs grow roots, and they are almost impossible to refactor once they start taking shape. Spend as little time as possible thinking about any given sentence. Just write them down as they come, without concern for clarity, conciseness, or impact. Why? Because the longer you linger on a sentence, the more you'll grow attached to it, and the harder it will be to cut it down later. Keep adding content as long as you have the energy to do it. At some point, ideas just stop growing, and the struggle to extract meaning from them becomes so big that you spend more time thinking than writing. Stop there. Resist any urge to double-think about what to write. If it's not coming easily, it's time to rest. You might need a couple more rounds to cover all the ideas in your outline, so take your time to recharge. Prune for clarity \u00b6 At this point you are saying everything you want to say, but cluttered and densely. Most ideas deas are overexposed, overgrown, touching their surroundings too much. You'll have repeated the same stuff over and over. It's time to prune. Aim to cut every paragraph down to half its size. First, just remove it to see if you needed it. If you don't miss it, it's gone for good. Otherwise, put it back and do the same with every sentence. You'll find that half your content can be cut clean. Now go over what's left and rewrite to make every idea as clear-cut as possible. Remove everything nonesential. Simplify tenses. Put the action at the forefront of each sentence and paragraph. Stop when you feel there's nothing unnecesary left. Every word must have a job: if you remove it, something breaks. You can come back a couple times and remove a word here and there. You'll know when it's time to move on. Polish for impact \u00b6 You have a lean story now, nothing unnecessary, but it's probably kind of gray, dull, boring; using words like \"probably\" and \"kind of\". It's time to make it blossom. In each paragraph, think how do you want the reader to feel: informed, impressed, intrigued, excited? Look for the words that conflict with that desired tone. Identify the key ideas in your narrative that should feel like a punch in the face. Build momemtum towards them, lowering the tone of the previous paragraphs to make them smoother, and then deliver that punch with maximum impact. Rewrite those key sentences to make them more impactful, ressonant, bolder. Drop boring qualifiers: instead of \"very important\" say \"crucial\". Remove any brakes and cushions. Finally, find the most relevant idea you want to deliver. Go back and add hints here and there to build towards it: drop an open question early on, or a subtle mention in a related passage. Repeat it a few times to make it ressonate. And that's it. When you're done with this third draft, your text is ready to publish. Put it out there and move on to the next piece. I purposefully left out anything concerning getting feedback, and not because it's irrelevant. On the contrary, getting feedback early on is critical. But this is a sufficiently complex topic on its own, so I'll leave it for another ocasion.","title":"Technical Writing"},{"location":"guides/technical-writing/#technical-writing","text":"Some think writing requires inspiration, a muse, or special talent. While these may (or not) be necessary for writing a fiction masterpiece, they certainly aren't needed to write a decent technical piece. Technical writing is a skill that can be learned and perfected with practice, and there are methods to help you move forward. The main blockers I've struggled with during technical writing are: Lack of original ideas. Missing a clear and unified structure. Running out of ideas or motivation halfway through. Dull and boring writing, lacking impact. A key ingredient to overcoming these blockers is to iterate a lot. It is impossible for most of us to come up with original, coherent, and impactful ideas, all at once. Instead, you start with a bunch of unconnected ideas, and you relentlessly edit, rewrite, and improve. The other ingredient is to know when to stop. No technical article is ever ready, you just decide to publish it when it's good enough. There are two reasons why this relentless editing is difficult: It is very hard to edit your own words once you get too attached to them. The longer you spend thinking about a specific sentence, the harder it will be for you to let it go. Besides, language is contextual, every sentence is connected with its surroundings. The longer a sentence lives, there harder it is to refactor it without compromising the story in which it is embedded. Hence, we need to delay the moment when ideas grow roots to as late as possible. We want them to float freely and bounce around as long as possible. And we want them to grow roots and entrench themselves into a coherent narrative at just the right time. Here's a losely-defined process that works for me. It starts with brain-dumping the key ideas before threading an underlying structure . Then, we'll make three passes, one to fill with content , another to prune for clarity and a third one to polish for impact . And that's it, three drafts and we're ready to publish.","title":"Technical Writing"},{"location":"guides/technical-writing/#brain-dump-key-ideas","text":"The first step is to come up with as many ideas as possible, regardless of their quality. You've been thinking about a topic for a while, and you already have a bunch of unconnected thoughts you want to explore. Sit down and write them, one by one, as they come up to your mind. Forget about structure, order, story, coherence; nothing matters beyond individual ideas. Sometimes an idea requires one sentence, sometimes a couple, and sometimes barely a phrase. Let each idea float freely, unconnected with the rest. Describe it with just the few words you need to understand it when you come back later. It doesn't need to make sense to anyone but you. The key insight is to not spend too much time thinking about any particular idea. Do not refine them, do not rewrite them to make them clearer beyond their initial conception. Open your mind and let ideas pour out as unchallenged as you can possibly do.","title":"Brain-dump key ideas"},{"location":"guides/technical-writing/#thread-a-structure","text":"Now you have a heap of ideas, some potentially good, some definitely bad, some ugly. They are floating freely, unconnected, unperturbed by their companions. Most of these ideas won't see the light of day, but you still don't know which ones will make the cut. The next step is to thread a meaningful structure around the ones that really matter. Your task is to reorder ideas so that similar topics start to cluster and a structure begins to emerge. Since ideas are slim and free, you can move them around, group them into sections and subsections as you wish. Play around with different structures here and there; maybe save copies to revisit later. Resist explaining or refining the ideas. Keep them slim. Remember that fat ideas grow roots. They become heavy and hard to reorder into a different structure. They attach to their context. As you play with different structures, you will converge into a definite form, a story where some ideas fit neatly, and some don't. Cut these loose without a second thought. Ideas are cheap anyway. That's why we want them to be slim in the first place, and why we didn't let ourselves get attached to them. At the end of this step you'll have a blueprint, a kind of skeleton that threads across all the remaining ideas, connecting them into a larger narrative. You'll have removed any idea that didn't fit, and possibly sprinkled here and there a few supporting ones where the structure felt less solid. They are ready to grow roots.","title":"Thread a structure"},{"location":"guides/technical-writing/#fill-with-content","text":"Now that you have a solid structure, it's time to let ideas flourish. Expand every idea into a paragraph, or two, or three, as long as you need. You're only interested in quantity now, not quality. Try and respect the structure you already have in place. Why?Because if you deviate too much from it, you'll be back at square one, lacking a clear structure, but with bloated paragraphs instead of slim ideas. And we already know this is bad: ideas are easy to move around, but paragraphs grow roots, and they are almost impossible to refactor once they start taking shape. Spend as little time as possible thinking about any given sentence. Just write them down as they come, without concern for clarity, conciseness, or impact. Why? Because the longer you linger on a sentence, the more you'll grow attached to it, and the harder it will be to cut it down later. Keep adding content as long as you have the energy to do it. At some point, ideas just stop growing, and the struggle to extract meaning from them becomes so big that you spend more time thinking than writing. Stop there. Resist any urge to double-think about what to write. If it's not coming easily, it's time to rest. You might need a couple more rounds to cover all the ideas in your outline, so take your time to recharge.","title":"Fill with content"},{"location":"guides/technical-writing/#prune-for-clarity","text":"At this point you are saying everything you want to say, but cluttered and densely. Most ideas deas are overexposed, overgrown, touching their surroundings too much. You'll have repeated the same stuff over and over. It's time to prune. Aim to cut every paragraph down to half its size. First, just remove it to see if you needed it. If you don't miss it, it's gone for good. Otherwise, put it back and do the same with every sentence. You'll find that half your content can be cut clean. Now go over what's left and rewrite to make every idea as clear-cut as possible. Remove everything nonesential. Simplify tenses. Put the action at the forefront of each sentence and paragraph. Stop when you feel there's nothing unnecesary left. Every word must have a job: if you remove it, something breaks. You can come back a couple times and remove a word here and there. You'll know when it's time to move on.","title":"Prune for clarity"},{"location":"guides/technical-writing/#polish-for-impact","text":"You have a lean story now, nothing unnecessary, but it's probably kind of gray, dull, boring; using words like \"probably\" and \"kind of\". It's time to make it blossom. In each paragraph, think how do you want the reader to feel: informed, impressed, intrigued, excited? Look for the words that conflict with that desired tone. Identify the key ideas in your narrative that should feel like a punch in the face. Build momemtum towards them, lowering the tone of the previous paragraphs to make them smoother, and then deliver that punch with maximum impact. Rewrite those key sentences to make them more impactful, ressonant, bolder. Drop boring qualifiers: instead of \"very important\" say \"crucial\". Remove any brakes and cushions. Finally, find the most relevant idea you want to deliver. Go back and add hints here and there to build towards it: drop an open question early on, or a subtle mention in a related passage. Repeat it a few times to make it ressonate. And that's it. When you're done with this third draft, your text is ready to publish. Put it out there and move on to the next piece. I purposefully left out anything concerning getting feedback, and not because it's irrelevant. On the contrary, getting feedback early on is critical. But this is a sufficiently complex topic on its own, so I'll leave it for another ocasion.","title":"Polish for impact"},{"location":"newsletter/06-ssl/","text":"\ud83d\udd96 Welcome to another issue of the Mostly Harmless AI newsletter. Machine learning is the hottest topic in AI these days. One of the biggest challenges, though, is that it requires vast amounts of data. Finding good labelled data is very hard, and using unlabelled data is difficult. In this issue, we'll take a look at a promising machine learning technique that sits between these two extremes and promises to bring the best of both paradigms: self-supervised learning . \ud83d\uddde\ufe0f What's new \u00b6 Facebook recently released a blog post about their new project, Learning from Videos . One of these is SEER , a new vision model that beats state-of-the-art in several well-known benchmarks. These are all instances of the same underlying technique: self-supervised learning . What is it, and why is it hitting the news? https://twitter.com/facebookai/status/1370420003690983430 You can read a deep dive from Yann LeCun itself , but the TL;DR is this. Self-supervised learning (SSL) is a technique for leveraging vast amounts of unlabeled data by using the data's own structure as supervised labels. Let's unpack that. The most famous self-supervised models are probably modern Transformers: BERT, GPT, and company. They are trained on lots of lots of text, but, here is the key part: we don't labels. Instead, we take a sentence, hide some words, and train the model to predict the unknown words from the known ones. Pretty clever, right? By forcing the model to predict the missing bits, we are implicitly making it learn the underlying structure of the data. With text, it is pretty straightforward (well, after they tell you about it...) but with images is a whole different story. This is where Facebook's new SEER model enters the picture. It is a major breakthrough, akin to what BERT and similar models meant for NLP a few years ago. And the best part, they open-sourced it . \ud83d\udcda For learners \u00b6 If you're interested in learning more about self-supervised learning, check out this awesome Github list of resources. It has a large collection of papers and resources on self-supervised learning in several domains, from images to audio, to natural language processing. And very up-to-date. You can also take a look at PapersWithCode section on self-supervision , which collects some of the most relevant papers with links to their Github implementations. And finally, here's a not-so-recent talk from Yann LeCun that you'll enjoy on the topic. \ud83d\udee0\ufe0f Tools of the trade \u00b6 Probably the best all-in-one resource for self-supervised learning in computer vision is Facebook's VISSL project . It contains implementations of the most popular SSL models in Pytorch. If you prefer Tensorflow, here's Google implemenation of SimCLR , one of those state-of-the-art models. \ud83c\udf7f Recommendations \u00b6 Taking a step away from the hard technical topic, today I want to recommend one of the best sci-fi series in recent times, and probably the best modern take on the issue of artificial consciousness: Westworld . If you haven't seen it yet, I cannot recommend it enough. The ethical and philosophical topics are deeply treated, and the drama is top-notch, including some mind-blowing storyline twisting and turning that will leave you confused at times, but always impressed. \ud83c\udfa4 Word of mouth \u00b6 This week's AMA was not as crowded as usual, but also packed with interesting questions. We talked about Explainable AI , how to keep up with tech , the value of getting a Tensorflow certification , why not to do a PhD , and more. https://twitter.com/AlejandroPiad/status/1370777555360419840 And tying back to self-supervise learning, here's an interesting discussion in HackerNews about the challenges and potential issues with this paradigm. As usual, take it with a grain of salt. \ud83d\udc65 Community \u00b6 In this issue, I want to recommend you to follow these two Twitter accounts. They are still small (in Twitter numbers), but very productive, and I've enjoyed interacting with them both a lot this last few months: Tolani @JaiyeTikolo , and Dimas @DreamOnShadows . Give them both the chance to fill your timeline with interesting stuff all around AI and related tech. \u2615 Homebrew \u00b6 On my end, I've been working on a lot of stuff, including new scripts for podcast episodes. If you want to weigh in, I would love to get your opinion on what topics to touch on first. https://twitter.com/AlejandroPiad/status/1370435757094141957 Finally, I've cooked up a small and dirty script to schedule Twitter threads from Trello, using Python. Here's a short thread about it: https://twitter.com/AlejandroPiad/status/1370767653971881985 And here's the script . Feel free to remix it and reuse it as you see fit. It's 100% open-source code. \ud83d\udc4b That\u2019s it for now. Please let me know what do you think of this issue, what would you like to see more or less of, and any feedback you want to share. If you liked this newsletter, consider subscribing (in case you\u2019re not) and forwarding it to those you love. It\u2019s \ud83d\udcaf free!","title":"06 ssl"},{"location":"newsletter/06-ssl/#whats-new","text":"Facebook recently released a blog post about their new project, Learning from Videos . One of these is SEER , a new vision model that beats state-of-the-art in several well-known benchmarks. These are all instances of the same underlying technique: self-supervised learning . What is it, and why is it hitting the news? https://twitter.com/facebookai/status/1370420003690983430 You can read a deep dive from Yann LeCun itself , but the TL;DR is this. Self-supervised learning (SSL) is a technique for leveraging vast amounts of unlabeled data by using the data's own structure as supervised labels. Let's unpack that. The most famous self-supervised models are probably modern Transformers: BERT, GPT, and company. They are trained on lots of lots of text, but, here is the key part: we don't labels. Instead, we take a sentence, hide some words, and train the model to predict the unknown words from the known ones. Pretty clever, right? By forcing the model to predict the missing bits, we are implicitly making it learn the underlying structure of the data. With text, it is pretty straightforward (well, after they tell you about it...) but with images is a whole different story. This is where Facebook's new SEER model enters the picture. It is a major breakthrough, akin to what BERT and similar models meant for NLP a few years ago. And the best part, they open-sourced it .","title":"\ud83d\uddde\ufe0f What's new"},{"location":"newsletter/06-ssl/#for-learners","text":"If you're interested in learning more about self-supervised learning, check out this awesome Github list of resources. It has a large collection of papers and resources on self-supervised learning in several domains, from images to audio, to natural language processing. And very up-to-date. You can also take a look at PapersWithCode section on self-supervision , which collects some of the most relevant papers with links to their Github implementations. And finally, here's a not-so-recent talk from Yann LeCun that you'll enjoy on the topic.","title":"\ud83d\udcda For learners"},{"location":"newsletter/06-ssl/#tools-of-the-trade","text":"Probably the best all-in-one resource for self-supervised learning in computer vision is Facebook's VISSL project . It contains implementations of the most popular SSL models in Pytorch. If you prefer Tensorflow, here's Google implemenation of SimCLR , one of those state-of-the-art models.","title":"\ud83d\udee0\ufe0f Tools of the trade"},{"location":"newsletter/06-ssl/#recommendations","text":"Taking a step away from the hard technical topic, today I want to recommend one of the best sci-fi series in recent times, and probably the best modern take on the issue of artificial consciousness: Westworld . If you haven't seen it yet, I cannot recommend it enough. The ethical and philosophical topics are deeply treated, and the drama is top-notch, including some mind-blowing storyline twisting and turning that will leave you confused at times, but always impressed.","title":"\ud83c\udf7f Recommendations"},{"location":"newsletter/06-ssl/#word-of-mouth","text":"This week's AMA was not as crowded as usual, but also packed with interesting questions. We talked about Explainable AI , how to keep up with tech , the value of getting a Tensorflow certification , why not to do a PhD , and more. https://twitter.com/AlejandroPiad/status/1370777555360419840 And tying back to self-supervise learning, here's an interesting discussion in HackerNews about the challenges and potential issues with this paradigm. As usual, take it with a grain of salt.","title":"\ud83c\udfa4 Word of mouth"},{"location":"newsletter/06-ssl/#community","text":"In this issue, I want to recommend you to follow these two Twitter accounts. They are still small (in Twitter numbers), but very productive, and I've enjoyed interacting with them both a lot this last few months: Tolani @JaiyeTikolo , and Dimas @DreamOnShadows . Give them both the chance to fill your timeline with interesting stuff all around AI and related tech.","title":"\ud83d\udc65 Community"},{"location":"newsletter/06-ssl/#homebrew","text":"On my end, I've been working on a lot of stuff, including new scripts for podcast episodes. If you want to weigh in, I would love to get your opinion on what topics to touch on first. https://twitter.com/AlejandroPiad/status/1370435757094141957 Finally, I've cooked up a small and dirty script to schedule Twitter threads from Trello, using Python. Here's a short thread about it: https://twitter.com/AlejandroPiad/status/1370767653971881985 And here's the script . Feel free to remix it and reuse it as you see fit. It's 100% open-source code. \ud83d\udc4b That\u2019s it for now. Please let me know what do you think of this issue, what would you like to see more or less of, and any feedback you want to share. If you liked this newsletter, consider subscribing (in case you\u2019re not) and forwarding it to those you love. It\u2019s \ud83d\udcaf free!","title":"\u2615 Homebrew"},{"location":"newsletter/07-automl/","text":"\ud83d\udd96 Welcome to another issue of the Mostly Harmless AI newsletter. Machine learning is becoming more and more a fundamental component in the products and services we consume everyday. But the craft is still very much in its infancy, and remains a challenging topic for newcommers, especially those without a formal education. In this issue, we'll take a look at an emerging paradigm that promises to bring a revolution to the machine learning field, much like compilers where in the 70s and 80s: Automated Machine Learning . \ud83c\udf1f The topic \u00b6 Everyone who's done even a bit of machine learning has had to struggle with the intricacies of getting a model to work outside toy problems. There are literally hundreds if not thousands of knobs you can turn to make training more efficient, more robust, or even just to make it converge. Machine learning as a science has come very far in the last couple of decades, but as an engineering it is still in its infancy. Enter AutoML, or Automated Machine Learning. Sounds almost oxymoronic, or tautological: what does it even mean to automate machine learning? AutoML is an emerging subfield of machine learning that attempts to progresively optimize most of the process for solving a typical machine learning problem. At a very high level, we can think of this process as a series of steps such as data collection and preparation, feature engineering, model selection and tuning, deployment, monitoring, and so on. Each of these steps includes a lot of technical decisions with a myriad of options. Just take model selection: are we going full-in with neural networks, or should we try some of the basic models first? Which ones, for the matter? And don't get me started with neural networks, we have to pick among layers, activation functions, regularizers, optimizers... If we look at data preparation, again, there's a plethora of options: how to impute missing values (or whether to do it at all), how to encode the data, which filters to apply... You get the point. With AutoML we can reduce the cognitive load of these million decisions significantly, though it's no silver bullet, of course. One of the most successful subtasks in AutoML is model selection and tuning. Say you want to try all the classifiers in scikit-learn . You can loop through all the classes (fortunately they share a common API), but you also have to try different parameters in each class, whose valid value ranges are different for each class. And then each model has to be trained and cross-validated, how many times? Instead, you can use something like auto-sklearn , a library that wraps all scikit-learn models under a single class AutoClassifier . Jumping to the deep learning world, the model selection problem is often framed as Neural Architecture Search (NAS) . There are literally hundreds of different techniques but in the end it all boils down to the same idea: an intelligent search over the possible architectures of neural networks. The world of AutoML is already huge, and keeps growing. There are lots of open source libraries you can use today, and the big players in the industry are already packing their cloud platforms with these tools. Be aware, though, that the field is still in its infancy, and it's no free lunch. As AutoML is basically machine learning on steroids, all the existing challenges are multiplied. The cost of training an AutoML model is orders of magnitude larger than training a single model, although a lot of research is being put into reducing it. And two major issues that remain ahead are related to the explainability and the intrinsic biases encoded in these models. In the meantime, next time you have to train a somewhat conventional model on a somewhat conventional dataset, make yourself a favor and use of the many AutoML libraries available. You'll be surprised of the results and, even if you still need to tinker with details, you will at least start with a very good baseline to build upon. \ud83d\udcda For learners \u00b6 The best resource for getting up-to-date with the field is the AutoML.org website . You'll find links to papers, tools, and a very good introductory book . You can also check this awesome list on Github with links to hundreds of papers, books, tools, blog posts, slides, and more. \ud83d\udee0\ufe0f Tools of the trade \u00b6 It's impossible to list all the incredible AutoML libraries out there, so I'll share just two most-known that will get you pretty far. autosklearn is an AutoML wrapper for scikit-learn that gives you a black-box classifier. Under the hood, it's powered by Bayesian optimization, a super cool technique to efficiently explore large and complex spaces of parameters with a non-trivial structure. https://automl.github.io/auto-sklearn/master/ autokeras is keras -based AutoML framework. You'll find a few high-level models, like an image or a text classifier, which when fit perform a neural architecture search over a space of sensibly predefined architectures. https://autokeras.com/ auto-pytorch is a similar tools but for pytorch . You'll find a few high-level models, like an image or a text classifier, which when fit perform a neural architecture search over a space of sensibly predefined architectures. (Not all AutoML libraries are named following the regex pattern auto-?(\\w+) , but it's pretty common...) \ud83c\udf7f Recommendations \u00b6 The last couple of weeks I've been reading Stephen King's \"On Writing\" . It's both a short autobriography and a manual for writing better from the genius of the horror genre. Whether you like his books or not, you gotta concede he can write, in a way that makes many of his readers (myself include) incapable of not rendering in their minds his imaginations. If you want to know the secret sauce behind the success of his narrative art, and how to apply those same tools to your own writing, whether fiction or not, this book pretty much summarizes it. \ud83c\udfa4 Word of mouth \u00b6 Last Friday I challenged you to share with us an intriguing philosophical issue you couldn't stop thinking about. Lots of you answered, with mindblowing questions that sparked discussions still alive today. https://twitter.com/AlejandroPiad/status/1372850346243072001 I also submitted one such dilemma, a contrived scenario where a self-driven car would have to choose in an impossible situation between different equally bad outcomes. The purpose was to surface the need to think deeply about morality and how to encode it on our AI systems because, regardless of our technological advancement, we are still humans, and AI is reaching the point where it will inevitably have to deal with fundamental human issues. https://twitter.com/AlejandroPiad/status/1372880546381103227 \ud83d\udc65 Community \u00b6 This week I want you to follow three researchers from the team that created auto-sklearn . The have been doing a tremendous job organizing workshops, writing survey papers, and working on the basic science, to bring the field of AutoML into frontline research. https://twitter.com/KEggensperger https://twitter.com/FrankRHutter https://twitter.com/__mfeurer__ I was fortunate to participate in one of their workshops and meet them, and I can tell you they also happen to be geniunely good people, driven by the desire to make machine learning available to as many of us as possible. \u2615 Homebrew \u00b6 Since this issue is on the topic of AutoML, I want to tell you about yet another framework, but this time one I'm personally involved with. https://autogoal.github.io/ autogoal is a different approach to AutoML. Instead of wrapping an existing library with a transparent API (like auto-sklearn ) or give you high-level constructs (like autokeras ), we tried to mix and match the most common ML tools you already use under a single unified API. The library is a still in its infancy, but it's quickly growing. It currently include hundreds of algorithms from sklearn , spacy , gensim , nltk , keras , pytorch , that you can use as black-box machine learning models. The most interesting part, however, is that under the hood everything is powered by a very flexible DSL that you can adapt to any new library. It would mean to world to me if you could come to our Github and leave us star, or give us a follow at Twitter . \ud83d\udc4b That\u2019s it for now. Please let me know what do you think of this issue, what would you like to see more or less of, and any feedback you want to share. If you liked this newsletter, consider subscribing (in case you\u2019re not) and forwarding it to those you love. It\u2019s \ud83d\udcaf free!","title":"07 automl"},{"location":"newsletter/07-automl/#the-topic","text":"Everyone who's done even a bit of machine learning has had to struggle with the intricacies of getting a model to work outside toy problems. There are literally hundreds if not thousands of knobs you can turn to make training more efficient, more robust, or even just to make it converge. Machine learning as a science has come very far in the last couple of decades, but as an engineering it is still in its infancy. Enter AutoML, or Automated Machine Learning. Sounds almost oxymoronic, or tautological: what does it even mean to automate machine learning? AutoML is an emerging subfield of machine learning that attempts to progresively optimize most of the process for solving a typical machine learning problem. At a very high level, we can think of this process as a series of steps such as data collection and preparation, feature engineering, model selection and tuning, deployment, monitoring, and so on. Each of these steps includes a lot of technical decisions with a myriad of options. Just take model selection: are we going full-in with neural networks, or should we try some of the basic models first? Which ones, for the matter? And don't get me started with neural networks, we have to pick among layers, activation functions, regularizers, optimizers... If we look at data preparation, again, there's a plethora of options: how to impute missing values (or whether to do it at all), how to encode the data, which filters to apply... You get the point. With AutoML we can reduce the cognitive load of these million decisions significantly, though it's no silver bullet, of course. One of the most successful subtasks in AutoML is model selection and tuning. Say you want to try all the classifiers in scikit-learn . You can loop through all the classes (fortunately they share a common API), but you also have to try different parameters in each class, whose valid value ranges are different for each class. And then each model has to be trained and cross-validated, how many times? Instead, you can use something like auto-sklearn , a library that wraps all scikit-learn models under a single class AutoClassifier . Jumping to the deep learning world, the model selection problem is often framed as Neural Architecture Search (NAS) . There are literally hundreds of different techniques but in the end it all boils down to the same idea: an intelligent search over the possible architectures of neural networks. The world of AutoML is already huge, and keeps growing. There are lots of open source libraries you can use today, and the big players in the industry are already packing their cloud platforms with these tools. Be aware, though, that the field is still in its infancy, and it's no free lunch. As AutoML is basically machine learning on steroids, all the existing challenges are multiplied. The cost of training an AutoML model is orders of magnitude larger than training a single model, although a lot of research is being put into reducing it. And two major issues that remain ahead are related to the explainability and the intrinsic biases encoded in these models. In the meantime, next time you have to train a somewhat conventional model on a somewhat conventional dataset, make yourself a favor and use of the many AutoML libraries available. You'll be surprised of the results and, even if you still need to tinker with details, you will at least start with a very good baseline to build upon.","title":"\ud83c\udf1f The topic"},{"location":"newsletter/07-automl/#for-learners","text":"The best resource for getting up-to-date with the field is the AutoML.org website . You'll find links to papers, tools, and a very good introductory book . You can also check this awesome list on Github with links to hundreds of papers, books, tools, blog posts, slides, and more.","title":"\ud83d\udcda For learners"},{"location":"newsletter/07-automl/#tools-of-the-trade","text":"It's impossible to list all the incredible AutoML libraries out there, so I'll share just two most-known that will get you pretty far. autosklearn is an AutoML wrapper for scikit-learn that gives you a black-box classifier. Under the hood, it's powered by Bayesian optimization, a super cool technique to efficiently explore large and complex spaces of parameters with a non-trivial structure. https://automl.github.io/auto-sklearn/master/ autokeras is keras -based AutoML framework. You'll find a few high-level models, like an image or a text classifier, which when fit perform a neural architecture search over a space of sensibly predefined architectures. https://autokeras.com/ auto-pytorch is a similar tools but for pytorch . You'll find a few high-level models, like an image or a text classifier, which when fit perform a neural architecture search over a space of sensibly predefined architectures. (Not all AutoML libraries are named following the regex pattern auto-?(\\w+) , but it's pretty common...)","title":"\ud83d\udee0\ufe0f Tools of the trade"},{"location":"newsletter/07-automl/#recommendations","text":"The last couple of weeks I've been reading Stephen King's \"On Writing\" . It's both a short autobriography and a manual for writing better from the genius of the horror genre. Whether you like his books or not, you gotta concede he can write, in a way that makes many of his readers (myself include) incapable of not rendering in their minds his imaginations. If you want to know the secret sauce behind the success of his narrative art, and how to apply those same tools to your own writing, whether fiction or not, this book pretty much summarizes it.","title":"\ud83c\udf7f Recommendations"},{"location":"newsletter/07-automl/#word-of-mouth","text":"Last Friday I challenged you to share with us an intriguing philosophical issue you couldn't stop thinking about. Lots of you answered, with mindblowing questions that sparked discussions still alive today. https://twitter.com/AlejandroPiad/status/1372850346243072001 I also submitted one such dilemma, a contrived scenario where a self-driven car would have to choose in an impossible situation between different equally bad outcomes. The purpose was to surface the need to think deeply about morality and how to encode it on our AI systems because, regardless of our technological advancement, we are still humans, and AI is reaching the point where it will inevitably have to deal with fundamental human issues. https://twitter.com/AlejandroPiad/status/1372880546381103227","title":"\ud83c\udfa4 Word of mouth"},{"location":"newsletter/07-automl/#community","text":"This week I want you to follow three researchers from the team that created auto-sklearn . The have been doing a tremendous job organizing workshops, writing survey papers, and working on the basic science, to bring the field of AutoML into frontline research. https://twitter.com/KEggensperger https://twitter.com/FrankRHutter https://twitter.com/__mfeurer__ I was fortunate to participate in one of their workshops and meet them, and I can tell you they also happen to be geniunely good people, driven by the desire to make machine learning available to as many of us as possible.","title":"\ud83d\udc65 Community"},{"location":"newsletter/07-automl/#homebrew","text":"Since this issue is on the topic of AutoML, I want to tell you about yet another framework, but this time one I'm personally involved with. https://autogoal.github.io/ autogoal is a different approach to AutoML. Instead of wrapping an existing library with a transparent API (like auto-sklearn ) or give you high-level constructs (like autokeras ), we tried to mix and match the most common ML tools you already use under a single unified API. The library is a still in its infancy, but it's quickly growing. It currently include hundreds of algorithms from sklearn , spacy , gensim , nltk , keras , pytorch , that you can use as black-box machine learning models. The most interesting part, however, is that under the hood everything is powered by a very flexible DSL that you can adapt to any new library. It would mean to world to me if you could come to our Github and leave us star, or give us a follow at Twitter . \ud83d\udc4b That\u2019s it for now. Please let me know what do you think of this issue, what would you like to see more or less of, and any feedback you want to share. If you liked this newsletter, consider subscribing (in case you\u2019re not) and forwarding it to those you love. It\u2019s \ud83d\udcaf free!","title":"\u2615 Homebrew"},{"location":"newsletter/08-languagemodels/","text":"\ud83d\udd96 Welcome to another issue of the Mostly Harmless AI newsletter. It's been a while since the last issue, and the reason is I've been struggling with finding the motivation to write something worth of sharing. Don't get me wrong, I have a ton of ideas (even I dare to say, one or two good ones), but most of them are better shaped for Twitter threads. Plus, there are already LOTS of great newsletters out there, both for long-form and short-form topics. So I decided that if I wanted to give a shot to this newsletter, it had to be something slightly more personal, something that only could come from me. Thus, I will be focusing more on sharing my journey, the things I'm working on, the problems I'm concerned about. Of course, all of this is tightly related to AI, which remains to be my primary (well, secondary) love. I hope these topics are something you find useful or at least mildly interesting. Most of it is mostly harmless, anyway. \ud83e\udd2c Language models are full of biases \u00b6 A couple threads ago I talked about computational language models. These are, in a nutshell, compressed representations of human language that assign a likelihood to every possible sentence. As a black box, you can imagine a language model as some kind of Python function that receives a sentence and outputs a number from 0 to 1, the higher the most likely that sentence actually \"exists\". They are used anywhere we need a computer to deal with natural language: automatic translation, speech-to-text, search engines, There are many ways to implement something like this, and you can take a look at this thread for some ideas: https://apiad.net/tweetstorms/mindblowingmonday/languagemodels Anyway, what I wanted to talk now is not that much about technical details, but rather about some problems that arise from the deployment of huge language models by big tech companies. You see, language models are often trained in an unsupervised (or self-supervised) form, fed with massive chunks of text mined from the Internet. This is a very cool idea in principle, because we have access to a vast collection of human language where basically everything we know about can be found. GPT-3 and BERT are just two examples of very different language models trained of huge amounts of text (they are in completely different leagues, though, in terms of training data). So, if you ask one of these language model the probability of a sentence like \"Leonardo da Vinci painted the Mona Lisa\" it should give it a very high score. However, if you ask it \"Alejandro Piad painted the Mona Lisa\", the score should be close to zero. The reason is very simple, there are far more examples of the first sentence than the second in the Internet. The model doesn't really know who painted the Mona Lisa, it just knows that many more people think it was Leonardo (keep in mind, though, that both you and me also reason like this a lot of times...) Now, if only the Internet was a place where all that's true was massively more common that what's false. But it isn't. It is full with conspiracy theories and fake news. So we must be careful in using frequency of something appearing in the Internet as a proxy for truthfulness. The big problem, though, comes not from purposefully misleading stuff, but from the subtle biases that creep into all of our conversations. For example, what happens when you ask a language model \"He is a programmer\" vs \"She is a programmer\"? Naturally, both sentences should be exactly equal in terms of likelihood. But a carelessly trained LM will very likely give a higher score to the first one. Why? Because the Internet has many more examples of programmer boys than girls! Why does this matter? In some applications this kind of biases pop up immediately. For example, if you Google translate a long paragraph including \"she is a programmer\" back and forth between English and a language without gender you can get \"he is a programmer\" back. But these are not the worst cases. You can use a language model with these biases as an internal component of another system, say, to evaluate candidates for job applications, or to assess the reliability of a legal claim, or to estimate if a person will forfeit a mortage, or to pre-screen papers submitted to a research journal. In these cases, you may have no idea how these biases are messing with the final prediction. As a very simplistic example, you could be rejecting women applying for programming jobs more often than men because their profile has less \"fit\" with the job description. So here comes the mandatory discussion about \"but that's the real data!\". Yes, it is. And that doesn't make it right. Reality is full of biases, full of wrong decisions, full of things we want to change. Letting those things creep into our models of reality unnoticed is a recipe for keeping ourselves in the place we are today, not in the place we want to be. Now, there's light at the end of the tunnel. Ethics and fairness is a big issue in the AI research community today. The most brilliant minds in our field our working in the detection and mitigation of these problems. The solution is not to vilify and stop using these technologies altogether. Language models are a very powerful tech that can boost some of the most interesting and useful applications of the next decade. The solution is to understand their limitations and deploy them with the necessary care in those scenarios where they're most likely to cause harm. \ud83d\udcda For learners \u00b6 If you want to learn more about some of the biggest issues in AI ethics today, there is a wonderful book, The Alignment Problem , by Brian Christian . It goes way beyond language biases, into the realm of reinforcement learning and the value alignment problem. \ud83d\udee0\ufe0f Tools of the trade \u00b6 If what you're looking for is to play with pre-trained language models, use them in your own app, or fine-tune them in your own dataset, then what you want is the transformers library by huggingface.co . \ud83d\udc65 Cool people to meet \u00b6 This week I want to recommend you to follow Talia . She's working on some of the coolest research in the intersection of programming languages and software engineering. Plus, she is incredibly energetic and overall a very nice person to talk with about some of the most difficult topics we are facing today. Another cool friend I made recently is Prashant . We've been talking a lot about some of the most intriguing philosophical questions around AI, consciousness, free will, you mention it. He's very active on Twitter and he shares a lot of interesting bits and resources about machine learning. Plus, he loves books! \u2615 Homebrew \u00b6 Finally, this week I want to share with you a small project I did a few months ago, auditorium . It's a slideshow generator based on the awesome reveal.js , which adds a Pythonic layer with which you can craft cool interactive slideshows with pure Python code. It's a bit rough around the edges, though, so it would be very cool if you could take it for a spin and let me know what you think of it! \ud83d\udc4b That\u2019s it for now. Please let me know what do you think of this issue, what would you like to see more or less of, and any feedback you want to share. If you liked this newsletter, consider subscribing (in case you\u2019re not) and forwarding it to those you love. It\u2019s \ud83d\udcaf free!","title":"08 languagemodels"},{"location":"newsletter/08-languagemodels/#language-models-are-full-of-biases","text":"A couple threads ago I talked about computational language models. These are, in a nutshell, compressed representations of human language that assign a likelihood to every possible sentence. As a black box, you can imagine a language model as some kind of Python function that receives a sentence and outputs a number from 0 to 1, the higher the most likely that sentence actually \"exists\". They are used anywhere we need a computer to deal with natural language: automatic translation, speech-to-text, search engines, There are many ways to implement something like this, and you can take a look at this thread for some ideas: https://apiad.net/tweetstorms/mindblowingmonday/languagemodels Anyway, what I wanted to talk now is not that much about technical details, but rather about some problems that arise from the deployment of huge language models by big tech companies. You see, language models are often trained in an unsupervised (or self-supervised) form, fed with massive chunks of text mined from the Internet. This is a very cool idea in principle, because we have access to a vast collection of human language where basically everything we know about can be found. GPT-3 and BERT are just two examples of very different language models trained of huge amounts of text (they are in completely different leagues, though, in terms of training data). So, if you ask one of these language model the probability of a sentence like \"Leonardo da Vinci painted the Mona Lisa\" it should give it a very high score. However, if you ask it \"Alejandro Piad painted the Mona Lisa\", the score should be close to zero. The reason is very simple, there are far more examples of the first sentence than the second in the Internet. The model doesn't really know who painted the Mona Lisa, it just knows that many more people think it was Leonardo (keep in mind, though, that both you and me also reason like this a lot of times...) Now, if only the Internet was a place where all that's true was massively more common that what's false. But it isn't. It is full with conspiracy theories and fake news. So we must be careful in using frequency of something appearing in the Internet as a proxy for truthfulness. The big problem, though, comes not from purposefully misleading stuff, but from the subtle biases that creep into all of our conversations. For example, what happens when you ask a language model \"He is a programmer\" vs \"She is a programmer\"? Naturally, both sentences should be exactly equal in terms of likelihood. But a carelessly trained LM will very likely give a higher score to the first one. Why? Because the Internet has many more examples of programmer boys than girls! Why does this matter? In some applications this kind of biases pop up immediately. For example, if you Google translate a long paragraph including \"she is a programmer\" back and forth between English and a language without gender you can get \"he is a programmer\" back. But these are not the worst cases. You can use a language model with these biases as an internal component of another system, say, to evaluate candidates for job applications, or to assess the reliability of a legal claim, or to estimate if a person will forfeit a mortage, or to pre-screen papers submitted to a research journal. In these cases, you may have no idea how these biases are messing with the final prediction. As a very simplistic example, you could be rejecting women applying for programming jobs more often than men because their profile has less \"fit\" with the job description. So here comes the mandatory discussion about \"but that's the real data!\". Yes, it is. And that doesn't make it right. Reality is full of biases, full of wrong decisions, full of things we want to change. Letting those things creep into our models of reality unnoticed is a recipe for keeping ourselves in the place we are today, not in the place we want to be. Now, there's light at the end of the tunnel. Ethics and fairness is a big issue in the AI research community today. The most brilliant minds in our field our working in the detection and mitigation of these problems. The solution is not to vilify and stop using these technologies altogether. Language models are a very powerful tech that can boost some of the most interesting and useful applications of the next decade. The solution is to understand their limitations and deploy them with the necessary care in those scenarios where they're most likely to cause harm.","title":"\ud83e\udd2c Language models are full of biases"},{"location":"newsletter/08-languagemodels/#for-learners","text":"If you want to learn more about some of the biggest issues in AI ethics today, there is a wonderful book, The Alignment Problem , by Brian Christian . It goes way beyond language biases, into the realm of reinforcement learning and the value alignment problem.","title":"\ud83d\udcda For learners"},{"location":"newsletter/08-languagemodels/#tools-of-the-trade","text":"If what you're looking for is to play with pre-trained language models, use them in your own app, or fine-tune them in your own dataset, then what you want is the transformers library by huggingface.co .","title":"\ud83d\udee0\ufe0f Tools of the trade"},{"location":"newsletter/08-languagemodels/#cool-people-to-meet","text":"This week I want to recommend you to follow Talia . She's working on some of the coolest research in the intersection of programming languages and software engineering. Plus, she is incredibly energetic and overall a very nice person to talk with about some of the most difficult topics we are facing today. Another cool friend I made recently is Prashant . We've been talking a lot about some of the most intriguing philosophical questions around AI, consciousness, free will, you mention it. He's very active on Twitter and he shares a lot of interesting bits and resources about machine learning. Plus, he loves books!","title":"\ud83d\udc65 Cool people to meet"},{"location":"newsletter/08-languagemodels/#homebrew","text":"Finally, this week I want to share with you a small project I did a few months ago, auditorium . It's a slideshow generator based on the awesome reveal.js , which adds a Pythonic layer with which you can craft cool interactive slideshows with pure Python code. It's a bit rough around the edges, though, so it would be very cool if you could take it for a spin and let me know what you think of it! \ud83d\udc4b That\u2019s it for now. Please let me know what do you think of this issue, what would you like to see more or less of, and any feedback you want to share. If you liked this newsletter, consider subscribing (in case you\u2019re not) and forwarding it to those you love. It\u2019s \ud83d\udcaf free!","title":"\u2615 Homebrew"},{"location":"podcast/","text":"Welcome to the Mostly Harmless AI podcast. In this podcast I'll share with you interesting bits about artificial intelligence, what it is, how it works, and why you should care about it. We'll talk a lot about the most exciting and novel discoveries. We will turn to history sometimes, and share some funny, enlightening, or emotive moments. And, we will discuss about the implications that these technologies can have on your daily life, and how can you make informed decisions on your interaction with AI systems today. You know, mostly harmless discussions.","title":"\u2753 About podcasts"},{"location":"podcast/what-is-ai/","text":"An origin story for Artificial Intelligence \u00b6 \ud83d\udd09 Listen on Anchor.fm Intro \u00b6 Artificial Intelligence stands at an intriguing crossroad between math, engineering, and philosophy. At its core, it's all about trying to answer some of the most transcendental questions: what is intelligence, what takes for a system to be able to sustain it, and whether can we build such a system. What is Artificial Intelligence about? To try and answer this question, let's start at the beginning, or at a beginning because, like every good story, this one has many versions. In this episode, I want to tell you an origin story for Artificial Intelligence. Is the story of how the dream of a fearless man revolutionized our comprehension of what it means to be, after all, human. Welcome to Mostly Harmless AI. The Imitation Game \u00b6 Our origin story starts in 1950, England, home of Alan Turing. Turing is very well known for at least three different things, each of which would independently be enough to warrant him a major place in the history of Computer Science. When taken together, his contributions make him, in the eyes of many, the most relevant figure in the whole field. His first major contribution to Computer Science was the definition of a mathematical model for an abstract kind of machine that could potentially perform any sort of computation. He basically defined the minimum requirements to make a computer. This model is now known by the very creative name of \"Turing machine\", and it lays the theoretical foundations of what can and, more importantly, what cannot be done with a computer, regardless of how powerful technology ever becomes. Had he ended there, he would have been remembered as the theoretical father of Computer Science. But he went further. During World War 2, he worked on a super-secret project to decipher Nazi cryptography. It was supposed to be uncrackable, but Turing teamed-up with some of the smartest people he could find, and they cracked it. And in doing so, they also built the first physical embodiment of an actual Turing machine, the first real working computer. So, he both created the foundational theory of the field and engineered the first computer. And then he turned his attention to what he considered was the ultimate question of this new science: can these machines ever think? In \"Computer Machinery and Intelligence\", a short technical paper written in 1950, Turing described what came to be known as the Turing Test, a hypothetical experiment to determine whether artificial intelligence could be considered, indeed, intelligent. He called it \"The Imitation Game\". The basic idea is something like this: you put both a human and a computer behind closed doors, from where they can communicate with another human, a judge, by a text interface only, like in a chat. The judge's job is to determine who is the human and who is the computer, and both interviews will do their best to convince the judge that they are human. So the real human can write whatever he or she wants: \"The human is me\", \"Don't trust the other\", anything. But here is the thing, the computer can also do the same, so in a sense, it has to deceive another human to be considered \"intelligent\". The judge can ask questions to trick the computer to reveal itself. Maybe ask it to solve some complex mathematical problem, something that no human could achieve. But the computer can just say \"hey, that's impossible to do!\", just like any human would. Turing thought the only way for a computer to successfully convince any potential judge, no matter how tricky the questions asked, would be to display such a wide range of creative and human-like responses that we would have to agree, for all practical means, it was indeed showing intelligence. The Turing Test gives us one possible definition for Artificial Intelligence: a machine that displays human-like responses in every conceivable conversation. It's kind of a tautological answer to the question of \"what is intelligence\". It's intelligence if you cannot effectively differentiate it from other things that you agree to call intelligence. There are a lot of issues with this definition, though, and we'll examine a few of them next. The issue of human biases \u00b6 First, we have to say that Turing conceived this test as a thought experiment, a hypothetical setup to force us into thinking about how can we even start to define what intelligence is about. Turing's idea was not that we would actually implement this test as he described it. It's more a philosophical definition of intelligence than a pragmatic one. Despite this, we did take his words at face value and implemented the test. The Loebner contest is probably the most famous one. It's a yearly contest in which teams of programmers submit chatbots that are evaluated against real humans in a very similar setting to what Turing proposed. One pragmatic argument against Turing's Test is that it relies heavily on the ability of a human to accurately judge whether those responses seem human-like, which is a very subjective thing to do. And we know for a fact that humans are lousy at subjectivity. We are full of biases, one of which is precisely our tendency to \"anthropomorphize\", that is, to see human features in non-human things. We do it with our pets, with inanimate objects, with symbols, ..., we even do it with characters in videogames! So of course we could anthropomorphize the computer behind that text message and assign it a higher degree of intelligence that it really has. This has happened in the Loebner contest over and over. Programmers design a chatbot that makes spelling mistakes on purpose and justifies itself as a non-native speaker to avoid answering complex questions. These techniques have allowed systems that are, for whatever definition of intelligence you have, definitely not intelligent, to win over 30% of the times. The Turing test, at least implemented in this simplistic scenario in the Loebner contest, seems to be very fragile to human biases. The issue of semantics \u00b6 Beyond practical or methodological problems, some thinkers have argued that there are fundamental issues with this definition of intelligence. Probably the most famous argument is \"The Chinese Room\", posed by philosopher John Searle. It's a thought counter-experiment in which a human is placed inside a room with a book that contains instructions for translating messages from Chinese to English, messages that enter the room through a small window. The man, who speaks nothing of Chinese, reads the message, finds the symbols in the book, and writes the corresponding answer. If the book is written in such a way that for every incoming message it will correctly produce the correct translation, any external observer would be convinced the \"room\" speaks Chinese, while it is obvious, according to Searle, that neither the man nor the book actually understands Chinese. According to Searle, this thought experiment shows that a system can display the ability to solve a cognitive task without actually possessing the intelligence necessary to do it. It's supposed to show that imitation of intelligence is not the same as actual intelligence. Some have argued, though, that even if neither the man nor the book understands Chinese, there is a sense in which the whole system, the room with all its content, does understand Chinese. It is the same sense in which Turing's test defines intelligence: if the room (with man and book inside) is indeed able to provide a perfectly plausible translation for any incoming message, what else do we need to convince ourselves it does understand Chinese? The point is, this kind of argument rests on agreeing on the definition of what \"knowing\" or \"understanding\" means. If your definition of \"understanding\" is such that only humans are capable of doing it, then, by your own definition, you cannot believe in Artificial Intelligence. Turing was a functionalist in this regard. He believed that intelligence is best defined in terms of what it can do, regardless of, for example, how is it made or what is it composed of. In his view, intelligence is being capable of maintaining a coherent conversation about any general topic, independently of the hardware (electronic or biological) that sustains that intelligence. You might disagree, and lots of very smart people do. Some believe intelligence has to be biological in nature. Others, that there is some layer of unknown physics at play inside our brains that cannot be simulated in a classical computer. Turing believed the brain was just a very powerful computer, and thus, in principle, nothing can stop us from eventually making an electronic substitute capable of harbouring intelligence. The issue of purpose \u00b6 Even if we agree philosophically with this definition of intelligence, it is still problematic in another sense. If we define intelligence as something at the level of what humans can currently do, isn't it kind of pointless to try and develop that? We already have humans, why do we need another system with the same capabilities? Wouldn't it be better if artificial intelligence can solve the problems that we humans cannot solve today? An argument in favour of Turing could be that his definition imposes no limitation to what this intelligence can do. It need not be as dumb as a human. In fact, a super-intelligent being should be perfectly capable of passing the test, the same way an adult can play with a child without actually believing in fairies. In a sense, we could say that being able to succeed at another human's judgement of character is the ultimate intelligence test because intelligence evolved precisely in this context: we were a bunch of primates in the savannah who needed to understand and trust each other to survive. But this anthropocentric view is not without issues as well. Who's to say that human intelligence is not a dead end? Couldn't we, by trying to imitate us, humans, be actually taking a detour on the path to superintelligence? Maybe our biases are just the evidence that, if there is higher intelligence in the universe, it is not like us. But if that's the case, could we even be able to recognize that intelligence? And could they, or it, recognize us? Turing's Test is far from perfect. It has practical, philosophical and ethical implications regarding how we define intelligence. In a sense, it is also about how we define ourselves since intelligence is arguable the human trait we are the proudest of. How we choose to define it also dictates how we attempt to imitate it, and how we go about recognizing it in others. Our biases are an intrinsic part of our intelligence, whatever that is. Conclusions? \u00b6 Turing's imitation game is the first attempt to formally define an overarching goal for artificial intelligence and the then nascent science of computation. However, this goal wasn't set in stone. In the next decades, different paradigms emerged, some of them more focused on what we today call general, or strong, AI, and others more focused on narrow, or weak, AI. The first group of paradigms attempts to solve AI more or less in the sense that Turing envisioned. To create an artificial intelligence so powerful that it can perform any cognitive task a human can, and then some we can't. The second attempts to solve concrete problems, such as vision, or language, or driving a car, that seem to require advanced levels of cognition but are not entirely general. For most of the history of AI, the narrow approaches have been the most successful. We are today capable of detecting tons of objects in images; translate between most mainstream languages; and play chess, Go, and even StarCraft better than any human that's ever lived. However, we are still very, very far from Turing's vision of achieving near-human capabilities at open-ended conversation. In a sense, we have walked a very long road, and yet we are still at the very beginning of this journey. And despite the dominating pragmatism, and the focus on solving narrow tasks, if you ask most of the people working today in the field, they will tell you that, deep down, they also share Turing's dream. Turing's life ended sadly. He was a gay man in a terribly unforgiving society, and he suffered physical and psychological humiliation for being unwilling to fit the narrow definition of human that others had decided. His greatest achievement was the dream he left us to pursue. A dream of a future in which humanity is no longer alone in the cognitive universe. A future we will share with our intellectual children, to whom we will show the marvels of the universe, and they will, in turn, help us unravel its deepest mysteries. Outro \u00b6 Thank you for listening. If you enjoyed this episode, feel free to leave a review, and share it with your loved ones. If you have any questions or suggestions, I would love to hear from you. You can find me on Twitter, and if you're listening on the Anchor app, you can leave a voice message right here. This was the Mostly Harmless AI podcast. I'll be back very soon with another episode on the fascinating world of Artificial Intelligence. Until then, stay curious, and stay safe...","title":"An origin story for AI"},{"location":"podcast/what-is-ai/#an-origin-story-for-artificial-intelligence","text":"\ud83d\udd09 Listen on Anchor.fm","title":"An origin story for Artificial Intelligence"},{"location":"podcast/what-is-ai/#intro","text":"Artificial Intelligence stands at an intriguing crossroad between math, engineering, and philosophy. At its core, it's all about trying to answer some of the most transcendental questions: what is intelligence, what takes for a system to be able to sustain it, and whether can we build such a system. What is Artificial Intelligence about? To try and answer this question, let's start at the beginning, or at a beginning because, like every good story, this one has many versions. In this episode, I want to tell you an origin story for Artificial Intelligence. Is the story of how the dream of a fearless man revolutionized our comprehension of what it means to be, after all, human. Welcome to Mostly Harmless AI.","title":"Intro"},{"location":"podcast/what-is-ai/#the-imitation-game","text":"Our origin story starts in 1950, England, home of Alan Turing. Turing is very well known for at least three different things, each of which would independently be enough to warrant him a major place in the history of Computer Science. When taken together, his contributions make him, in the eyes of many, the most relevant figure in the whole field. His first major contribution to Computer Science was the definition of a mathematical model for an abstract kind of machine that could potentially perform any sort of computation. He basically defined the minimum requirements to make a computer. This model is now known by the very creative name of \"Turing machine\", and it lays the theoretical foundations of what can and, more importantly, what cannot be done with a computer, regardless of how powerful technology ever becomes. Had he ended there, he would have been remembered as the theoretical father of Computer Science. But he went further. During World War 2, he worked on a super-secret project to decipher Nazi cryptography. It was supposed to be uncrackable, but Turing teamed-up with some of the smartest people he could find, and they cracked it. And in doing so, they also built the first physical embodiment of an actual Turing machine, the first real working computer. So, he both created the foundational theory of the field and engineered the first computer. And then he turned his attention to what he considered was the ultimate question of this new science: can these machines ever think? In \"Computer Machinery and Intelligence\", a short technical paper written in 1950, Turing described what came to be known as the Turing Test, a hypothetical experiment to determine whether artificial intelligence could be considered, indeed, intelligent. He called it \"The Imitation Game\". The basic idea is something like this: you put both a human and a computer behind closed doors, from where they can communicate with another human, a judge, by a text interface only, like in a chat. The judge's job is to determine who is the human and who is the computer, and both interviews will do their best to convince the judge that they are human. So the real human can write whatever he or she wants: \"The human is me\", \"Don't trust the other\", anything. But here is the thing, the computer can also do the same, so in a sense, it has to deceive another human to be considered \"intelligent\". The judge can ask questions to trick the computer to reveal itself. Maybe ask it to solve some complex mathematical problem, something that no human could achieve. But the computer can just say \"hey, that's impossible to do!\", just like any human would. Turing thought the only way for a computer to successfully convince any potential judge, no matter how tricky the questions asked, would be to display such a wide range of creative and human-like responses that we would have to agree, for all practical means, it was indeed showing intelligence. The Turing Test gives us one possible definition for Artificial Intelligence: a machine that displays human-like responses in every conceivable conversation. It's kind of a tautological answer to the question of \"what is intelligence\". It's intelligence if you cannot effectively differentiate it from other things that you agree to call intelligence. There are a lot of issues with this definition, though, and we'll examine a few of them next.","title":"The Imitation Game"},{"location":"podcast/what-is-ai/#the-issue-of-human-biases","text":"First, we have to say that Turing conceived this test as a thought experiment, a hypothetical setup to force us into thinking about how can we even start to define what intelligence is about. Turing's idea was not that we would actually implement this test as he described it. It's more a philosophical definition of intelligence than a pragmatic one. Despite this, we did take his words at face value and implemented the test. The Loebner contest is probably the most famous one. It's a yearly contest in which teams of programmers submit chatbots that are evaluated against real humans in a very similar setting to what Turing proposed. One pragmatic argument against Turing's Test is that it relies heavily on the ability of a human to accurately judge whether those responses seem human-like, which is a very subjective thing to do. And we know for a fact that humans are lousy at subjectivity. We are full of biases, one of which is precisely our tendency to \"anthropomorphize\", that is, to see human features in non-human things. We do it with our pets, with inanimate objects, with symbols, ..., we even do it with characters in videogames! So of course we could anthropomorphize the computer behind that text message and assign it a higher degree of intelligence that it really has. This has happened in the Loebner contest over and over. Programmers design a chatbot that makes spelling mistakes on purpose and justifies itself as a non-native speaker to avoid answering complex questions. These techniques have allowed systems that are, for whatever definition of intelligence you have, definitely not intelligent, to win over 30% of the times. The Turing test, at least implemented in this simplistic scenario in the Loebner contest, seems to be very fragile to human biases.","title":"The issue of human biases"},{"location":"podcast/what-is-ai/#the-issue-of-semantics","text":"Beyond practical or methodological problems, some thinkers have argued that there are fundamental issues with this definition of intelligence. Probably the most famous argument is \"The Chinese Room\", posed by philosopher John Searle. It's a thought counter-experiment in which a human is placed inside a room with a book that contains instructions for translating messages from Chinese to English, messages that enter the room through a small window. The man, who speaks nothing of Chinese, reads the message, finds the symbols in the book, and writes the corresponding answer. If the book is written in such a way that for every incoming message it will correctly produce the correct translation, any external observer would be convinced the \"room\" speaks Chinese, while it is obvious, according to Searle, that neither the man nor the book actually understands Chinese. According to Searle, this thought experiment shows that a system can display the ability to solve a cognitive task without actually possessing the intelligence necessary to do it. It's supposed to show that imitation of intelligence is not the same as actual intelligence. Some have argued, though, that even if neither the man nor the book understands Chinese, there is a sense in which the whole system, the room with all its content, does understand Chinese. It is the same sense in which Turing's test defines intelligence: if the room (with man and book inside) is indeed able to provide a perfectly plausible translation for any incoming message, what else do we need to convince ourselves it does understand Chinese? The point is, this kind of argument rests on agreeing on the definition of what \"knowing\" or \"understanding\" means. If your definition of \"understanding\" is such that only humans are capable of doing it, then, by your own definition, you cannot believe in Artificial Intelligence. Turing was a functionalist in this regard. He believed that intelligence is best defined in terms of what it can do, regardless of, for example, how is it made or what is it composed of. In his view, intelligence is being capable of maintaining a coherent conversation about any general topic, independently of the hardware (electronic or biological) that sustains that intelligence. You might disagree, and lots of very smart people do. Some believe intelligence has to be biological in nature. Others, that there is some layer of unknown physics at play inside our brains that cannot be simulated in a classical computer. Turing believed the brain was just a very powerful computer, and thus, in principle, nothing can stop us from eventually making an electronic substitute capable of harbouring intelligence.","title":"The issue of semantics"},{"location":"podcast/what-is-ai/#the-issue-of-purpose","text":"Even if we agree philosophically with this definition of intelligence, it is still problematic in another sense. If we define intelligence as something at the level of what humans can currently do, isn't it kind of pointless to try and develop that? We already have humans, why do we need another system with the same capabilities? Wouldn't it be better if artificial intelligence can solve the problems that we humans cannot solve today? An argument in favour of Turing could be that his definition imposes no limitation to what this intelligence can do. It need not be as dumb as a human. In fact, a super-intelligent being should be perfectly capable of passing the test, the same way an adult can play with a child without actually believing in fairies. In a sense, we could say that being able to succeed at another human's judgement of character is the ultimate intelligence test because intelligence evolved precisely in this context: we were a bunch of primates in the savannah who needed to understand and trust each other to survive. But this anthropocentric view is not without issues as well. Who's to say that human intelligence is not a dead end? Couldn't we, by trying to imitate us, humans, be actually taking a detour on the path to superintelligence? Maybe our biases are just the evidence that, if there is higher intelligence in the universe, it is not like us. But if that's the case, could we even be able to recognize that intelligence? And could they, or it, recognize us? Turing's Test is far from perfect. It has practical, philosophical and ethical implications regarding how we define intelligence. In a sense, it is also about how we define ourselves since intelligence is arguable the human trait we are the proudest of. How we choose to define it also dictates how we attempt to imitate it, and how we go about recognizing it in others. Our biases are an intrinsic part of our intelligence, whatever that is.","title":"The issue of purpose"},{"location":"podcast/what-is-ai/#conclusions","text":"Turing's imitation game is the first attempt to formally define an overarching goal for artificial intelligence and the then nascent science of computation. However, this goal wasn't set in stone. In the next decades, different paradigms emerged, some of them more focused on what we today call general, or strong, AI, and others more focused on narrow, or weak, AI. The first group of paradigms attempts to solve AI more or less in the sense that Turing envisioned. To create an artificial intelligence so powerful that it can perform any cognitive task a human can, and then some we can't. The second attempts to solve concrete problems, such as vision, or language, or driving a car, that seem to require advanced levels of cognition but are not entirely general. For most of the history of AI, the narrow approaches have been the most successful. We are today capable of detecting tons of objects in images; translate between most mainstream languages; and play chess, Go, and even StarCraft better than any human that's ever lived. However, we are still very, very far from Turing's vision of achieving near-human capabilities at open-ended conversation. In a sense, we have walked a very long road, and yet we are still at the very beginning of this journey. And despite the dominating pragmatism, and the focus on solving narrow tasks, if you ask most of the people working today in the field, they will tell you that, deep down, they also share Turing's dream. Turing's life ended sadly. He was a gay man in a terribly unforgiving society, and he suffered physical and psychological humiliation for being unwilling to fit the narrow definition of human that others had decided. His greatest achievement was the dream he left us to pursue. A dream of a future in which humanity is no longer alone in the cognitive universe. A future we will share with our intellectual children, to whom we will show the marvels of the universe, and they will, in turn, help us unravel its deepest mysteries.","title":"Conclusions?"},{"location":"podcast/what-is-ai/#outro","text":"Thank you for listening. If you enjoyed this episode, feel free to leave a review, and share it with your loved ones. If you have any questions or suggestions, I would love to hear from you. You can find me on Twitter, and if you're listening on the Anchor app, you can leave a voice message right here. This was the Mostly Harmless AI podcast. I'll be back very soon with another episode on the fascinating world of Artificial Intelligence. Until then, stay curious, and stay safe...","title":"Outro"},{"location":"tweetstorms/","text":"I recently discovered that Twitter can be an amazing place to share short, actionable pieces of content that convey one specific message. These are topics that I haven't developed enough to become an essay, and probably never will, but I still find them interesting and worthy of sharing. Not everything I write on Twitter is saved here, only those pieces of content that have enough value I think someone might be interested in bookmarking. But this content is only the tip of the iceberg, the real value is in the conversation they spark on Twitter. So, after reading them, make sure to click on the link at the end which will take you to the thread on Twitter, and there you'll find a lot of opinions, discussion, and insights. If you want to read more thoughtful stuff, I also have some short essays where I take a longer time to explore more complex ideas. Some of these threads are organized into five topics which, for silly reasons, I named after the days of the week. So, every week I try to do one or two of these, and according to the day in which they are published, the topic is more or less consistent. \ud83e\udd2f Mindblowing Mondays are about cool ideas in Computer Science that are just awesome. \ud83e\udd13 Technical Tuesdays are about practical tools or libraries you can start using right away. \ud83e\udd29 Wisdom Wednesdays are about tips and hints to help you improve and grow your career. \ud83e\uddd0 Theory Thursdays are about theoretical aspects of Computer Science that are intriguing to me. \ud83e\udd14 Philosophy Fridays are about philosophical aspects of Computer Science for which there is no right answer, just interesting discussions. I also started a somewhat regular series of tweetstorms about the foundations of Machine Learning and another about AI in general. These are more informative in nature,but I always try to give a personal point of view about any of these topics, some intuitive explanation that I think might be useful, or some tips on how to better approach them. There is also a category for rants that didn't fit anywhere else. These are brain-dumps that are not polished enough to become essays but I still felt they should be out there.","title":"\u2753 About tweetstorms"},{"location":"tweetstorms/ai/what-is-ai/","text":"Most of the time you hear Artificial Intelligence, it's actually Machine Learning what people are talking about. This is a series on #FoundationsOfAI beyond machine learning. \u2753 Let's start by answering what is Artificial Intelligence about... \ud83e\uddf5\ud83d\udc47 Almost all definitions of AI involve some sort of definition for intelligence. A common phrasing would be something like this: \ud83d\udcdd AI deals with designing software that can solve problems that seem to require some degree of intelligence. See the issue, though? \ud83d\udc47 The first issue is to try and define what the heck \"intelligence\" is about. \ud83d\udc49 Many have tried, from Turing's Imitation Game to Chollet's Abstraction and Reasoning Corpus. One problem is that it is very hard to think of intelligence from a non-anthropocentric point of view. Another issue with this definition is what many call \"The Moving Goal of AI\". \ud83d\udc49 Once a problem is (sufficiently) solved by software (e.g., chess), we no longer believe it requires intelligence, and it ceases to be considered AI. Let's explore a different point of view... \ud83d\udc47 Let's start by organizing problems in different levels of complexity. \ud83d\udd39 A Level-0 problem is, for example \"add 1 + 3\" or \"factorize 45\" or \"sort [4,3,1,5]\", or \"find the way out of this labyrinth\"... That is, any concrete problem instance . Now let's up one level. \ud83d\udd37 A Level-1 problem is, for example, \"add any two numbers\", or \"sort any list\" or \"exit any labyrinth\". That is, a specific class of problems. I hope at this point we can agree that Computer Science is (mostly) about solving Level-1 problems. \ud83d\udc49 When you code an algorithm, say QuickSort or Dijkstra, you are giving a solution to an infinite number of instances of a specific problem class. Every Science is concerned with problems in a specific domain of reality (matter, energy, substance, life, people, ...). \ud83d\udca1 CS is concerned with the domain of problem-solving itself. It asks questions about which strategies are better for solving which classes of problems. For example, for the class of problems of sorting numbers, we can ask which algorithm is the best in terms of time, space, stability, etc. These questions are not always easy to answer, and often there isn't a clear best. Now let's jump up another level of abstraction. \ud83d\udc47 Finding a route from A to B in a city, a set of moves to solve a Rubik cube, and a proof for a logic theorem, are three different problem classes. But they all share a similar underlying structure, a common theme if you will. \ud83d\udc47 You can see these three problem classes as instances of a meta-class: finding an optimal path in a graph of states. And we can ask if there is a general way to solve any problem class of this nature without knowing many more details about a specific problem class. Similarly, packing boxes in a small space, organizing a schedule with no conflicts, and safely distributing N queens on NxN chess board, are all instances of a problem meta-class: assign values to a set of variables ensuring that some constraints are respected. \ud83d\udca0 These are Level-2 problems. Problems that generalize a very broad (actually an infinite) set of seemingly disparate problem classes with a more abstract definition. \ud83e\udd2f Solving a Level-2 problem will solve an infinite number of problem classes. In AI, we're concerned with Level-2 problems. We ask not whether we can solve a problem class, but whether we can solve an infinitely large family of problem classes that share a common abstract structure. \ud83d\udca1 The questions of AI are about which strategies for problem-solving are general enough that can be applied to a broad range of problem classes, potentially even to any problem class we can think of. \u270f\ufe0f Side note: by virtue of their very abstract nature, most if not all Level-2 problems in AI contain NP-Complete and/or NP-Hard problem classes. Hence, almost all general-purpose solutions in AI involve some sort of heuristic search. Do you see why is it called \"Artificial Intelligence\" now? We could have called it something as boring as \"Automatic General-Purpose Problem-Solving\", but that name isn't as catchy as Artificial Intelligence, of course. Notice that we avoided trying to define intelligence altogether. And not because this question is not important, but because it is very hard to try and answer what is intelligence without unconsciously resorting to antropomorphic biases. Our definition isn't perfect though, because we rely on a fuzzy distinction between Level-1 and Level-2 problems, and some things that seem AI for some people will seem \"just regular CS\" for others. One way to try to clarify this distinction is by pushing Level-2 to the extreme. \u2753 Can we find a universal algorithm , one that can solve any given computable problem? \ud83d\udc49 This goal is sometimes called Artificial General Intelligence (AGI) or strong AI. However, while we work on that, we may as well solve not all, but a significantly large set of problem classes in a general way. \ud83d\udc49 This is often called narrow AI: solving problems like question answering, object recognition, automatic theorem proving, playing games, etc. To summarize, we can say that AI is the field of CS that focuses on finding general-purpose problem solving strategies that can be applied to Level-2 problems. These strategies, when performed by humans, are often part of what we recognize as \"intelligence\". And that's it for now. Next time, we'll examine some common Level-2 problems that AI can help solving. \ud83e\uddf5 You can read this thread online at https://apiad.net/tweetstorms/ai/what-is-ai/","title":"What is AI?"},{"location":"tweetstorms/mindblowingmonday/gan/","text":"Hey, today is #MindblowingMonday \ud83e\udd2f! A day to share with you amazing things from every corner of Computer Science. Today I want to talk about Generative Adversarial Networks \ud83d\udc47 \ud83c\udf6c But let's begin with some eye candy. Take a look at this mind-blowing 2-minute video and, if you like it, then read on, I'll tell you a couple of things about it... Generative Adversarial Networks (GAN) have taken by surprise the machine learning world with their uncanny ability to generate hyper-realistic examples of human faces, cars, landscapes, and a lot of other stuff, as you just saw. Want to know how they work? \ud83d\udc47 There are many variants, but the core idea is to have 2\ufe0f\u20e3 neural networks: \u2699\ufe0f a generator network \u2696\ufe0f a discriminator network Both networks are connected in a sort of adversarial game, where each is trying to outperform the other. \u2696\ufe0f The discriminator is a regular neural network whose job is to determine if a specific sample (say, an image of a face) is real or generated. This network's architecture depends on the classification task, as usual, e.g., lots of convolutions and pooling for images. \u2699\ufe0f The generator network is a decoder network, whose job is to transform an input of random values to whatever you want to generate. In images, for example, you'll have deconvolution layers and upsampling, i.e., the \"reverse\" of an image classification network. \ud83c\udfa9 All the magic happens in the training. You train the discriminator by alternatively showing it real and generated images, and minimizing some classification loss (e.g., binary cross-entropy). The generator is trained to try and \"fool\" the discriminator. But this is not easy, so the trick involves letting it \"see\" the discriminator loss function. \ud83d\udca1 It's like showing you my brain while you perform a magic trick, so you can understand how I can be fooled best. This is the basic idea, but the devil is in the details. Two common problems with GANs are: 1\ufe0f\u20e3 The discriminator learns much faster, so the generator never gets a chance to catch up. 2\ufe0f\u20e3 The generator gets complacent and just produces the same good examples over and over. \ud83e\udd14 Finally, beyond the technical challenges, the possibility of suddenly creating very realistic content opens a can of worms of ethical issues such as disinformation. But technology itself is neither good nor bad, it is just a tool. It's on ourselves what we do with it. As usual, if you like this topic, have any questions, or just want to discuss, reply in this thread or @ me any time. I'll be listening. This thread is available in plain format here, forever: https://apiad.net/tweetstorms/mindblowingmonday-gan/ Stay curious: \ud83c\udfa5 https://apiad.net/to/#gan-video \ud83c\udfeb https://deepgenerativemodels.github.io/ \ud83d\udcd8 https://www.manning.com/books/gans-in-action \ud83d\udcc3 https://arxiv.org/abs/1710.07035 \ud83d\udcbb https://github.com/nightrome/really-awesome-gan \ud83d\udde8\ufe0f You can see this tweetstorm as originally posted in this Twitter thread .","title":"Generative Adversarial Networks"},{"location":"tweetstorms/mindblowingmonday/languagemodels/","text":"Hey, today is #MindblowingMonday \ud83e\udd2f! I want to tell you about Language Models, a type of machine learning techniques that are behind most of the recent hype in natural language processing. \u2753 Want to know more about them? \ud83e\uddf5\ud83d\udc47 A language model is a computational representation of human language that captures which sentences are more likely to appear in a given language. \ud83c\udfa9 Formally, a language model is a probability distribution over the sentences in a language. \u2753 What are they used for? \ud83d\udc47 \u2699\ufe0f Language models allow computers to understand and manipulate language at least to some degree. They are used in machine translation, speech to text, optical character recognition, text generation, and many more applications! They come in many flavors \ud83d\udc47 The simplest language model is the unigram model , also called a bag of words (BOW). \ud83d\udc49 In BOW, each word is assigned a probability Pi, and the probability of a sentence is computed assuming all words are independent. But of course, this isn' true. For example, \"water\" is a more commonly used word than \"philosophy\", but the phrase \"philosophy is the mother of science\" is arguably much more likely than the phrase \"water is the mother of science\". \ud83d\udca1 The likelihood of a phrase depends upon all its words. This dependency can be modelled with an n-gram model, in which the likelihood of a word is computed w.r.t. the words before in a given phrase (in a window of size n). \ud83d\udca1 If we start a phrase with \"philosophy\", is more likely to see the word \"science\" than \"shark\". \u261d\ufe0f The problem with n-gram models is that the total number of parameters you need to store grows exponentially with n. If you want to capture phrases of length n=10, you need N^10 numbers, where N is the number of words in the language! \u2b50 Neural language models (aka continuous space language models) are a solution to this exponential explosion. They try to learn jointly a vectorial representation for all words (aka an embedding) and some mathematical operation among them that approximates the likelihood. \u2699\ufe0f Neural language models are built by training a neural network to predict some relationships between words and the phrases in which they appear. The most popular neural language model is possibly word2vec , trained in predicting a word given a small window around it. \ud83d\udc49 Modern neural language models have more complex neural network architectures. Popular examples are BERT and the family of GPT models, of which GPT-3 recently took Twitter by surprise with its ability to speak nonstop about anything, often without much sense. \ud83d\ude07 The nice thing about language models is that they can be trained independently of any NLP problem and then used inside specific applications with a little fine-tunning. \ud83d\ude07 They also improve efficiency. A big company (like OpenAI or Google) can train a big language model and then the rest of us mortals can use them without having to pay millions in GPU training time. \u26a0\ufe0f But they don't come without issues \ud83d\udc47 \ud83e\udd14 Language models encode \"common\" language used, so all human bias is implicitly stored in them. For example, the phrase \"boy is a programmer\" is considered more likely by a model than \"girl is a programmer\", simply because the Internet has more examples of the first phrase. \u261d\ufe0f If used without care, these language models will introduce subtle biases in your application that are very hard to discover and debug. Understanding and fixing these biases is one of the most exciting and important issues in AI safety! As usual, if you like this topic, reply in this thread or @ me at any time. Feel free to \u2764\ufe0f like and \ud83d\udd01 retweet if you think someone else could benefit from knowing this stuff. \ud83e\uddf5 Read this thread online at https://apiad.net/tweetstorms/mindblowingmonday/languagemodels Stay curious \ud83d\udd96: \ud83d\udcc3 https://en.wikipedia.org/wiki/Language_model \ud83d\uddde\ufe0f https://arxiv.org/abs/2005.14165 \ud83d\udcbb https://github.com/huggingface/transformers \ud83c\udfa5 https://youtu.be/89A4jGvaaKk \ud83c\udfa5 https://youtu.be/_x9AwxfjxvE","title":"Language Models"},{"location":"tweetstorms/ml/generative-vs-discriminative/","text":"This is a Twitter series on #FoundationsOfML. \u2753 Today let's look at two fundamental modelling paradigms that are used throughout the whole ML landscape. Let's dive into Generative vs Discriminative models... \ud83d\udc47\ud83e\uddf5 1/20 Say we want to learn to recognize \ud83d\udc36dogs and \ud83d\ude3acats. Let's not worry about input format right now and instead think in terms of abstract features, like, does it have pointy ears? There are at least two ways in which we can do it. \ud83d\udc47 2/20 1\ufe0f\u20e3 We can try to learn what is a dog and what is a cat, independently . That is, which are the fundamental characteristics that best define each one of those classes. \ud83d\udc47 3/20 \ud83d\udc15 For example, we can learn that dogs have (generally) four legs, large noses, cute eyes, round ears, fur, and long tongues. \ud83d\udc08 Similarly, we can learn that cats have (generally) four legs, small noses, sneaky eyes, pointy ears, fur, and short tongues. \ud83d\udc47 4/20 To classify a new animal \ud83d\ude3c we can then look at its features, and say: \u2753 If this is a dog, what are the odds of seeing this kind of fur, this kind of legs, this kind of ears, ... \ud83d\udc47 5/20 Likewise, we can ask: \u2753 If this is a cat, what are the odds of seeing this kind of fur, this kind of legs, this kind of ears, ... Then we compare how surprised we are between seeing a \ud83d\udc15 and \ud83d\udc08 with these specific features. \ud83d\udc47 6/20 \ud83d\udd36 This is a Generative Model. These models learn what are the fundamental features of a given class. Formally, these models estimate what is the probability P(f1,f2,...,C) of observing these specific features f1, f2, ..., in any given class C. \ud83d\udc47 7/20 The reason they are called Generative is that they try to learn explicitly how an example of a given class is made . \ud83d\udca1 You can often use these models to generate random examples of each class, by sampling from P(fi...|C). \ud83d\udc47 8/20 2\ufe0f\u20e3 Alternatively, we can try to learn directly what makes a dog different from a cat. That is, which are the fundamental characteristics that best discriminate between those classes. \ud83d\udc47 9/20 We can learn that the larger the nose or the tongue the more likely to be a \ud83d\udc15, or that the pointier the ears and the sneakier the eyes, the more likely it is to be a \ud83d\udc08. And we do not care, for example, about the number of legs or the fur. \ud83d\udc47 10/20 To classify, we look at the features and say: \u2753 Given these ears, and this nose, and these eyes, how likely is this to be a dog or a cat? We compare the two results and answer the one we are most confident about. \ud83d\udc47 11/20 \ud83d\udd36 This is a Discriminative Model. These models learn which are the features that separate different classes. Formally, they are estimating explicitly what is the probability P(C|f1,f2,...) of seeing class C given that we observe the features f1, f2, ... \ud83d\udc47 12/20 The reason they're called Discriminative is that they try to learn what makes a class different from all others. \ud83d\udca1 You can often use these models to compute feature importance by looking at which features best separate different classes. \ud83d\udc47 13/20 \ud83d\udd39A classic example of a generative model is Naive Bayes. \ud83d\udd39A classic example of a discriminative model is Logistic Regression (and most neural networks). \ud83d\udc47 14/20 \u2753 Does this difference matter? If you mostly care about performance, then no, there is no intrinsically best modelling paradigm, and only experimentation can tell you what to use. However, depending on how you want to use the model, it can matter. \ud83d\udc47 15/20 Discriminative models are better at answering why (they think) this is the correct answer than generative models. They will focus on the important features for the task, and disregard anything that doesn't help them to score better at answering. \ud83d\udc47 16/20 Discriminative models learn not what we want, but what's useful, which can often be something completely off-track, like spurious correlations or harmful biases in the training set. \ud83d\udc47 17/20 Generative models often encode stronger inductive biases because they represent a hypothesis (ours) about how the data is created. This can make them more robust and controllable but if that hypothesis is too far from reality they may not learn anything useful. \ud83d\udc47 18/20 \u2b50 As usual, there is no silver bullet. You need to ask the right questions and be mindful of your assumptions. \u26a1 And always test your hypotheses! \ud83d\udc47 19/20 \ud83d\udd16 You can read this thread online at https://apiad.net/tweetstorms/ml/generative-vs-discriminative/ . \u2764\ufe0f If you liked this thread, please consider retweeting, following, and liking it, if you think I've earned it. And make sure to read the whole #FoundationsOfML series. It starts here: https://twitter.com/AlejandroPiad/status/1348840452670291969","title":"Generative vs Discriminative"},{"location":"tweetstorms/ml/metrics/","text":"This is a Twitter series on #FoundationsOfML. Today, I want to talk about another fundamental question: \u2753 What makes a metric useful for Machine Learning? Let's take a look at some common evaluation metrics and their most important caveats... \ud83d\udc47\ud83e\uddf5 Remember our purpose is to find some optimal program P for solving a task T, by maximizing a performance metric M using some experience E. https://apiad.net/tweetstorms/ml-what-is We've already discussed different modeling paradigms, and different types of experience. \ud83d\udc49 But arguably, the most difficult design decision in any ML process is which evaluation metric(s) to use. There are many reasons why choosing the right metric is crucial. \ud83d\udd11 If you cannot measure progress, you cannot objectively decide between different strategies. This is true when solving any problem, but in ML the consequences are even bigger: \ud83d\udc47 \ud83d\udd36 In Machine Learning, the metric you choose directly determines the type of solution you end up with. Remember that \"solving\" a problem in ML is actually about searching , between different models, the one that maximizes a given metric. \ud83d\udcdd Hence, metrics in Machine Learning are not auxiliar tools to evaluate a solution. They are what guides the actual process of finding a solution. Let's talk about some common metrics, focusing on classification for simplicity (for now) \ud83d\udc47 1\ufe0f\u20e3 Accuracy measures the percentage of the objects for which you guessed their category right. It's probably the most commonly used metric in the most common type of ML problem. \ud83d\udc4d The main advantage of accuracy is that it is very easy to interpret in terms of the business domain, and it is often aligned with what you actually want to achieve in classification: be right as many times as possible. \ud83d\udc4e One caveat is that accuracy is not differentiable, and cannot be used directly as target in gradient-based optimization process, such as neural networks, but there are easy solutions for this problem. \ud83d\udc4e Arguably the biggest problem of accuracy is that it counts every error as equal. This is not often the case. It can be far worse to tell a sick person to go home than to tell a healthy person to take the treatment (depending on the treatment, of course). \ud83d\udc4e On the other hand, if the number of elements in each category is not similar, you can be making a very large mistake (in relative terms) on the less populated category. You can get >99% accuracy if you just tell everyone you find on the street that they don't have COVID. The problem with Accuracy is that it smooths away different types of errors under the same number. \ud83d\udc49 If you care one specific class more than the rest, measure 2\ufe0f\u20e3Precision and 3\ufe0f\u20e3Recall instead, as they tell you more about the nature of the mistakes you're making. In general, there are two types of errors we can make when deciding the category C of an element: \ud83d\udd39 We can say one element belongs to C when it doesn't (type I). \ud83d\udd39 We can fail to say an element belongs to C when it does (type II). So how about we measure both errors separately \ud83d\udc47 2\ufe0f\u20e3 Precision measures the percentage of times when you say the category is C, and you're right (type I). 3\ufe0f\u20e3 Recall measures the percentage of elements of category C that you correctly identified (type II). By looking at these metrics separately, you can better identify what kind of error you're making. \ud83d\udcdd If you still want a kind of average that weights both, you can use the F-Measure, that allows to prioritize precision vs recall in any desired degree. Precision and Recall are also very intuitive to interpret, but they still don't tell us the whole story. \ud83d\udc49 When we have more than two categories, we can fail at any one of them by confusing it with any other. Here, again, precision and recall are too general. \u270f\ufe0f A confusion matrix tells us how many times we confuse each category C1 with any other category Ci across a test set. It looks something like this. \u270f\ufe0f Every number in a diagonal is a prediction we got right, and every other number is a prediction we got wrong. Accuracy, precision, and recall, are easy to compute from the confusion matrix (I'll leave you that as an exercise \ud83d\ude1c). \ud83d\udc4d The matrix itself shows a larger picture. It can tell us, for example, where we should focus on gathering more data. \ud83d\udc4e However, confusion matrices don't give us a single number we can optimize for, and are thus harder to interpret. The story we've seen here is common all over Machine Learning. \ud83d\udd38 We can have simple, high-level, interpretable metrics, that hide away the nuance. \ud83d\udd38 Or we can have low-level metrics that tell a bigger picture, but require more effort to interpret. There is a lot more to tell about metrics and evaluation in general, and we've just focused on a very small part of the problem. Some of the other issues that need to be kept in mind \ud83d\udc47: \ud83d\udd25 The metric we would like to optimize might not be optimizable at all, either because it's hard to evaluate (e.g., it's expensive or requires a human evaluator) or because it's not compatible with our optimization process (e.g., it's not differentiable). \ud83d\udd25 We can have multiple contradictory objectives, and no trivial way to weight them into a single metric. \ud83d\udd25 And sometimes we can judge a solution intuitively, but we have no idea know how to write a mathematical formulation for a metric that encodes that intuition. \ud83e\udd1c It's hard to overstate how important this topic is. Almost every alignment problem in AI can be traced back to a poorly defined metric. For example, maximizing engagement is arguably a large part of the reason why social media is as broken as it is. \u261d\ufe0f There is no objective way to decide what's the best metric for a problem by looking at the data alone. We have to decide what we want to aim for, and that in turn will define the problem we are actually solving. \u23f3 Next time, we'll talk about some common problem types.","title":"Metrics"},{"location":"tweetstorms/ml/models/","text":"This is a Twitter series on #FoundationsOfML. Today, I want to talk about another fundamental question: \u2753 What is a model in Machine Learning? Let's take a bird's eye view at different modeling paradigms... \ud83d\udc47\ud83e\uddf5 Remember our purpose is to find some \"optimal\" program P for solving a task T, by maximizing a performance metric M using some experience E. \ud83d\udcdd In ML lingo, that program P is called a \"model\", or alternatively, a \"hypothesis\". \ud83d\udc49 The first step in coming up with a suitable model, is to think of a model family , or hypothesis space . We can think of it as a class of programs , all very similar, such that one instance of that class is the actual program P that solves our task T optimally. All the possible models of a given model family share a common structure, and differentiate themselves in specific decisions (or instructions, if you will) inside this common template. A useful distinction here is between 1\ufe0f\u20e3 Parametric and 2\ufe0f\u20e3 Non-Parametric models. 1\ufe0f\u20e3 Parametric models Different models inside our model family have a fixed number of parameters (or weights), and differentiate from each other in the specific values of each parameter. \ud83d\udcdd Most of the models you've probably heard about, including logistic regression, naive Bayes, and all neural networks (with fixed architecture) are in this category. \ud83d\udd25 We can cast the problem of finding the best model as the problem of finding the optimal value for each parameter, such that some performance metric M is maximized (or some error metric M' is minimized). 2\ufe0f\u20e3 Non-parametric models Different models of the same model family have a variable number of parameters that often depends on the size of the training set. \ud83d\udcdd Some simple examples of non-parametric models are K-nearest neighbors, support vector machines (with non-linear kernels), and decision trees. 1\ufe0f\u20e3\ud83d\udc4d One advantage of parametric models is that they often have very efficient training algorithms, since you can exploit the structure of the parameter space (e.g., you have gradients to follow). 1\ufe0f\u20e3\ud83d\udc4d Another advantage is that the size of the model is independent of the size of the training set, and is often proportional to the number of features, i.e., these models \"compress\" the training set into a fixed-size \"formula\" of sorts. 1\ufe0f\u20e3\ud83d\udc4d Additionally, parametric models are often easy to regularize by adding some cost associated with the model complexity (e.g., the number of non-zero values or their magnitude). In short, parametric models are mathematically elegant and very malleable. 1\ufe0f\u20e3\ud83d\udc4e The biggest downside is that every model family implies significant assumptions about the data, and if we assume the wrong ones, we're very likely to underfit. On the contrast, non-parametric models are much more ad-hoc. 2\ufe0f\u20e3\ud83d\udc4d The main advantage of these models is they often imply weaker assumptions about the data, and can adapt to difficult datasets easier than similarly complex parametric models. 2\ufe0f\u20e3\ud83d\udc4e The biggest downside is that each model family is its own world, with ad-hoc training algorithms and lots of model-specific decisions to fine-tune. 2\ufe0f\u20e3\ud83d\udc4e Another downside is that the model size is sometimes proportional to the training set size, which makes them less suitable for learning from very large datasets. \u2753 Which is better? The answer is, of course, it depends. \ud83d\udc49 It depends on the nature of the problem to solve, the amount and quality of the available data, and what you intend to do with that solution. \ud83d\udd39 If you know the data well, and you're clear of your assumptions (e.g., linear relationships), go for the parametric model that best suits those assumptions. \ud83d\udd39 If you know nothing, Jon Snow, some of the most powerful non-parametric models (e.g., decision trees and SVMs) will often perform near the state-of-the-art with little fine-tuning. \u261d\ufe0f In any case, there is no substitute for experimentation. Make sure to evaluate different models and decide based on actual performance rather than intuition. \u23f3 We'll talk more about evaluation in a later thread.","title":"Modeling Paradigms"},{"location":"tweetstorms/ml/types-of-learning/","text":"This is a Twitter series on #FoundationsOfML. \u2753 Today, I want to start discussing the different types of Machine Learning flavours we can find. This is a very high-level overview. In later threads we'll dive deeper into each paradigm... \ud83d\udc47\ud83e\uddf5 Last time we talked about how Machine Learning works. \u23f3 https://apiad.net/tweetstorms/ml-what-is Basically, it's about having some source of experience E for solving a given task T, that allows us to find a program P which is (hopefully) optimal w.r.t. some metric M. According to the nature of that experience, we can define different formulations, or \"flavors\", of the learning process. A useful distinction is whether we have an explicit goal or desired output, which gives rise to the definitions of 1\ufe0f\u20e3 Supervised and 2\ufe0f\u20e3 Unsupervised Learning \ud83d\udc47 1\ufe0f\u20e3 Supervised Learning In this formulation, the experience E is a collection of input/output pairs, and the task T is defined as a function that produces the right output for any given input. \ud83d\udc49 The underlying assumption is that there is some correlation (or, in general, a computable relation) between the structure of an input and its corresponding output, and that it is possible to infer that function or mapping from a sufficiently large number of examples. The output can have any structure, including a simple atomic value. In this case there are two special sub-problems: \ud83c\udd70\ufe0f Classification, when the output is a category out of a finite set. \ud83c\udd71\ufe0f Regresion, when the output is a continuous value, bounded or not. 2\ufe0f\u20e3 Unsupervised Learning In this formulation, the experience E is just a collection of elements, and the task is defined as finding some hidden structure that explains those elements and/or how they relate to each other. \ud83d\udc49 The underlying assumption is that there is some regularity in the structure of those elements that helps explaining their characteristics with a restricted amount of information, hopefully significantly less than just enumerating all elements. Two common sub-problems are associated with where do we want to find that structure, inter- or intra-elements: \ud83c\udd70\ufe0f Clustering, when we care about the structure relating different elements. \ud83c\udd71\ufe0f Dimensionality reduction, when we care about the structure internal to each element. One of the fundamental differences between supervised and unsupervised learning problems, is this: \u261d\ufe0f In supervised problems is easier to define an objective metric of success, but it is much harder to get data, which almost always implies a manual labelling effort. Even though the distinction between supervised and unsupervised is kind of straightforward, it is still somewhat fuzzy, and there are other learning paradigms that don't fit neatly into these categories. Here's a short intro to three of them \ud83d\udc47 3\ufe0f\u20e3 Reinforcement Learning In this formulation, the experience E is not an explicit collection of data. Instead, we define an environment (a simulation of sorts) where an agent (a program) can take actions and observe their effect. \ud83d\udcdd This paradigm is useful when we have to learn to perform a sequence of actions, and there is no obvious way to define the \"correct\" sequence beforehand, other than trial and error, such as training artificial players for videogames, robots or self-driven cars. 4\ufe0f\u20e3 Semi-supervised Learning This is kind of a mixture between supervised and unsupervised learning, in which you have explicit output samples for just a few of the inputs, but you have a lot of additional inputs where you can try, at least, to learn some structure. \ud83d\udcdd Examples are almost any supervised learning problem, when we hit the point where getting additional labeled data (with both inputs and outputs) is too expensive, but it is easy to get lots of unlabelled data (just with inputs). 5\ufe0f\u20e3 Self-supervised Learning This is another paradigm that's kind of in-between supervised and unsupervised learning. Here we want to predict an explicit output, but that output is at the same time part of other inputs. So in a sense the output is also defined implicitely. \ud83d\udcdd A straightforward example is in language models, like BERT and GPT, where the objective is (hugely oversimplifying) to predict the n-th word in a sentence from the surrounding words, a problem for which we have lots of data (i.e., all the text in the Internet). All of these paradigms deserve a thread of their own, perhaps more, so stay tuned for that! \u231b But before getting there, next time we'll talk a bit about the fundamental differences in the kinds of models (or program templates) we can try to train.","title":"Types of Learning"},{"location":"tweetstorms/ml/what-is/","text":"I'm starting a Twitter series on #FoundationsOfML. Today, I want to answer this simple question. \u2753 What is Machine Learning? This is my preferred way of explaining it... \ud83d\udc47\ud83e\uddf5 Machine Learning is a computational approach to problem-solving with four key ingredients: 1\ufe0f\u20e3 A task to solve T 2\ufe0f\u20e3 A performance metric M 3\ufe0f\u20e3 A computer program P 4\ufe0f\u20e3 A source of experience E You have a Machine Learning solution when: \ud83d\udd11 The performance of program P at task T, as measured by M, improves with access to the experience E. That's it. Now let's unpack it with a simple example \ud83d\udc47 Let's say the task is to play chess \u265f\ufe0f. A good performance metric could be, intuitively, the number of games won against a random opponent. \ud83d\udc49 The \"classic\" approach to solve this problem is to write a computer program that encodes our knowledge of what a \"good\" chess player is. \ud83e\udd14 This could be in the form of a huge number of IF/ELSEs for a bunch of classic openings and endings, plus some heuristics to play the mid-game, possibly based on assigning points to each piece/position and capturing the pieces with the highest points. And this works, but... It is tremendously difficult to, first, build, and then, maintain that program as new strategies are discovered. And we'll never know if we're playing the optimal strategy. Now here is the Machine Learning approach to this problem \ud83d\udc47 You have to ask yourself first: is there a source of experience from which one can reasonably learn to play chess? \ud83d\udc49 For instance, a huge database of world-class chess games? With that experience at hand, how do we actually code a Machine Learning program? Details vary, but the bottom line is always the same. \ud83d\udd11 Instead of directly coding the program P that plays chess, what we write is kind of a meta-program, or \"trainer\", call it Q, that will itself give birth to P, by using that source of experience. To do that, we have to predefine some sort of \"mold\" or \"template\" out of which P will come out. \ud83d\ude43 As a simple example, let's assume there are some scores we can assign to each piece/position so we can compute the \"value\" of any given board. So P will be a very simple program: Generate every possible board after the current one, applying all valid moves. For each board, compute its value using those (still unknown) scores. Return the move that leads to the highest valued board. The question is, of course, how do actually find the optimal program P? That is, how do we discover that assignment of scores that leads to optimal gameplay? \u2b50 We will write another program Q to find them! \u2753 How do we know we found the best P? Here is where the metric M comes at hand. The best chess program P is the one whose score distribution makes it play such that it wins the most number of games. \u2753 And how do we actually find those points? The easiest way to do it is to simply enumerate all possible instances of P, by trying all combinations of scores for all possible piece/position configurations. \ud83d\udca9 But this might take forever! A better approach is to use a bit of clever math. \ud83e\udd2f If we design those scores the right way, we can come up with sort of an equation system, where all those scores are variables, and we can very quickly find the values that give us the optimal P! And here is where the experience comes to play. \ud83e\udd14 To write that equation system, which is huge, we can use each board in each gameplay as a different equation, that basically says \"this board is a winning board, so it should sum 100\" or \"this board is a losing board, so it should sum 0\". \u2697\ufe0f After this, there is a piece of mathematical magic that tells us how we should assign the scores, such that the vast majority of \"winning boards\" sum close to 100 and the \"losing boards\" sum close to 0. And we just made a machine \"learn\" how to play chess! To summarize... \ud83c\udfa9 In a \"classic\" approach we would: Define a desired output, i.e., the best move. Think very hard about the process to compute that output. Write the program P that produces the output. \ud83e\udd16 In a Machine Learning approach, instead we: Assume there is a \"template\" that any possible program P follows, parameterized with some unknown values. Write a program Q that finds the best values according to some experience E. Run Q on E to find the best program P. In conclusion, there is a BIG paradigm shift in the Machine Learning approach. \ud83c\udf1f Instead of directly writing a program P to solve task T, you actually code a \"trainer\" program Q that, when run on suitable experience E, finds the best program P (according to some metric M). \ud83d\udd25 The reason this paradigm is so hot now, is because there is an incredible amount of tasks for which we don't know how to write P directly, but it's fairly straightforward how to write Q, provided we have enough experience (read: data) to train on. \ud83d\udcdd In ML lingo, a \"template\" for P is called a \"model\" or a \"hypothesis space\", and the actual instance of P, after training, is called the \"hypothesis\". Q is any one of a large number of Machine Learning algorithms: decision trees, neural networks, naive Bayes... \u231b Next time, we'll talk about the different flavours of \"experience\" we can have, and how they define what type of \"learning\" we can actually attempt to do.","title":"What is Machine Learning?"},{"location":"tweetstorms/philosophyfriday/computability/","text":"Today is #PhilosophyFriday \ud83e\udd14! Today we'll be talking about the philosophical problem that kick-started all of Computer Science! \u2753 Can we answer all mathematical questions? \ud83e\uddf5\ud83d\udc47 \ud83d\udcc6 At the turn of the 20th century, the success of geometry, algebra, analysis and logic was hinting that it should be possible to unify all mathematics under a single theory. Each of these fields had proven extremely useful, and there were seemingly not many problems left. So they got together in 1900 to discuss the future of mathematics, and decide which were the most important problems left. \ud83c\udfa9 David Hilbert proposed 23 problems that he said should be solved by the end of the century. Today, all but 6 are either resolved or proven unsolvable. But there is one that had far-reaching implications beyond mathematics. The question is this: \ud83d\udc49 Prove that the axioms of arithmetic are consistent. Stated like this, it seems mostly harmless. But what does it mean? Intuitively, an axiomatic system is consistent if there is no way you can prove a contradictory fact using logic. This is super important, because if a theory allows internal contradictions to exist, then you could prove whatever crazy thing you want! (\ud83e\udd14 Politics, anyone?) Hilbert was convinced that, of course, arithmetic has to be consistent! \u261d\ufe0f Otherwise, math itself, the purest intellectual invention of humanity, would be nothing different from the daily rambling of crazy people. The answer, it turns, is absolutely mind-blowing. In 1931, Kurt G\u00f6del proved something incredible: \ud83e\udd2f In any sufficiently strong mathematical theory, there are always some true theorems that nevertheless cannot be proven within that theory. Moreover, the question \"is this theory consistent?\" is one of them. This was the first nail in the coffin of mathematical self-righteousness. \ud83d\udc49 If arithmetic, analysis, or algebra is consistent (which we hope they are), then that cannot be proven by mathematical means using arithmetic, analysis, or algebra. The second nail came a couple of years later. \ud83e\udd2f In 1933 Alfred Tarski proved that, in any sufficiently strong mathematical theory, the concept of \"truth\" cannot be formally defined inside that theory. The final nail was put simultaneously by Alan Turing and Alonso Church in 1936. \ud83e\udd2f They both showed, by different means, that it is impossible to write an algorithm to determine if any given theorem can be proved from a set of axioms. These results collectively showed that there are very important questions in mathematics that we cannot answer within mathematics. \ud83d\udca1 We cannot objectively define what truth is. \ud83d\udca1 We cannot prove all truths. \ud83d\udca1 We cannot be sure we don't have internal contradictions. Up to 1930, most mathematics believed there was no such thing as an \"unsolvable\" problem. Now we know there are plenty. Many are esoteric problems, but some are very concrete. One of the most relevant ones is this: \ud83d\udc49 There is no way to be 100% sure a program is correct. What does this mean, philosophically speaking? \u2b50 It shows that there is an inherent gap between what is true, and what can be known. Mathematics, and by extension, Computer Science, cannot give us all the answers. Is up to you to choose what this means for you. As usual, if you like this topic, reply in this thread or @ me at any time. Feel free to \u2764\ufe0f like and \ud83d\udd01 retweet if you think someone else could benefit from knowing this stuff. \ud83e\uddf5 Read this thread online at https://apiad.net/tweetstorms/philosophyfriday-computability Stay curious \ud83d\udd96: \ud83c\udfa5 https://youtu.be/O4ndIDcDSGc \ud83c\udfa5 https://youtu.be/macM_MtS_w4 \ud83d\udcc4 https://en.wikipedia.org/wiki/Hilbert%27s_problems \ud83d\udcc4 https://en.wikipedia.org/wiki/G%C3%B6del%27s_incompleteness_theorems \ud83d\udde8\ufe0f You can see this tweetstorm as originally posted in this Twitter thread .","title":"Computability"},{"location":"tweetstorms/philosophyfriday/consciousness/","text":"Today is #PhilosophyFriday \ud83e\udd14! Let's take a break from pragmatism and discuss one of the deepest philosophical questions: \u2753 Who are you? Are you your body, your brain, your thoughts, your actions? And how does this question connects with AI? \ud83e\uddf5\ud83d\udc47 Almost all of use agree on some basic assumptions about individuality. We all have the perception of free will, the fact that we make some decisions by ourselves . And we all have this notion of a uniqueness property that applies to the self. There is but one \"me\". We know we are ourselves, in part, because of the continuity of our conscious experience. \u261d\ufe0f I am the same person I was a fraction of a second ago, only a fraction of a second older. And I know this because I haven't stopped thinking during that time. But every night when you go to sleep, you pause your conscious experience for a few hours. \u2753 How do you know the person that wakes up the next day is actually you , and not just someone that happens to believe it's you? And yet, we all go to sleep happily every night. But what if I ask you to go to sleep for a thousand years? \ud83e\udd76 We would freeze your body to a state in which all celular processes stop, including sinapsis, and wake you up when there's technology to ensure you can be safely brought back to life. \u2753 Is it you who wakes up? Once there is a discontinuity in our conscious experience of reality, we might not longer be sure that we are actually the same person, instead of someone else with that person's memories. Let's stretch this idea to see where it breaks for you. \ud83d\udc47 Suppose there is a technology to transplant the brain, like physically moving it, from your old dying body to a younger clon of yourself that has been raised in-vitro and kept in comma. \u2753 Would you say the person that wakes up is actually you ? Now, instead of actually moving the brain, suppose we can brain-scan you and \"move\" whatever information is in there to a new brain in a new body. Lets assume the old brain gets irreversibly destroyed in the process, and copying is physically impossible. \u2753 Is it still you? Now, instead of a biological body, suppose we can move that information to a computer program inside a sufficiently powerful hardware, that is somewhow capable of \"holding\" consciousness in all it's mighty complexity. \u2753 Would you still do it? The purpose of these questions is to understand what is your notion of \"self\". \ud83e\udd14 At which point you think the discontinuity is large enough that a consciousness simply ceases to exist if it makes that jump. The reason this is relevant for AI, is that it shapes whether you think is possible to create a fully conscious artificial intelligence or not. \u261d\ufe0f We will be able move that intelligence from an old hardware to a new hardware. Depending on what you answered before, you have to agree with one of these: 1\ufe0f\u20e3 There is no way to achieve artificial consciousness. 2\ufe0f\u20e3 Artificial consciousness dies when moved to a new hardware. 3\ufe0f\u20e3 You can be uploaded and live in a virtual simulation. If you believe number 1\ufe0f\u20e3 then you have to consider there is something special about biological brains that cannot be replicated in silicon. If you believe number 2\ufe0f\u20e3 then you have to consider the morals of upgrading an AI, since that would mean killing it. And if you believe number 3\ufe0f\u20e3 then you have to consider the possibility that this reality could very well be just a sufficiently advanced simulation. And that has a bunch of crazy implications as well. As usual, if you like this topic, reply in this thread or @ me at any time. Feel free to \u2764\ufe0f like and \ud83d\udd01 retweet if you think someone else could benefit from knowing this stuff. \ud83e\uddf5 Read this thread online at https://apiad.net/tweetstorms/philosophyfriday-consciousness","title":"Who are You?"},{"location":"tweetstorms/philosophyfriday/intelligence/","text":"Hey, today is #PhilosophyFriday \ud83e\udd14! How about discussing some philosophical aspects of Computer Science? Today's question is: \u2753 How do you know someone (or something) else is intelligent? \ud83e\uddf5\ud83d\udc47 Before even asking the question of whether machines can or not ever become \"intelligent\", we need to ask ourselves: if they do, how would we know? The underlying question is, of course, what is intelligence? \ud83d\udc47 There are many ways to answer this question, and not a single one will be complete, of course. You have to consider psychology, neurobiology, even philosophy and ethics... From the computational point of the view, the most famous one is the Turing Test \ud83d\udc47 It goes like this: A \ud83d\udc69human and a \ud83d\udcbbcomputer take turns to chat with a \ud83d\udc69\u200d\u2696\ufe0fjudge. \ud83d\udc69\u200d\u2696\ufe0f is free to ask anything to anyone, even \u00bfare you a computer? Both \ud83d\udc69 and \ud83d\udcbb are trying to convince the judge they are human. \ud83d\udc49 If \ud83d\udc69\u200d\u2696\ufe0f fails to identify \ud83d\udc69 as the human, we are forced to recognize \ud83d\udcbb exhibits intelligence. \ud83d\udca1 The intuition behind this test is that, even if we cannot precisely define what intelligence is, we can all recognize it in others. \u26a0\ufe0f There has been a lot of criticism to Turing's Test, invoking among other issues: 1\ufe0f\u20e3 The judge may be easily fooled with cheap tricks, unless well trained. There are plenty of examples of simple chatbots that appear human because we tend to anthropomorphise things. 2\ufe0f\u20e3 Human intelligence might not be equivalent to general intelligence, so an intelligent computer (or an alien individual) might not be recognized as such, simply because it doesn't fit our narrow conception of intelligence. 3\ufe0f\u20e3 And there is the issue of whether \"simulating intelligence\" is the same as \"being intelligent\". This is John Searle's argument, commonly known as the \"Chinese Room\", which takes a philosophical stance against the computational nature of intelligence. \ud83e\udd14 The philosophical question we have to ask ourselves here is about the nature of intelligence. \u2753 Is intelligence a purely computational process, independent of the underlying hardware? Or does it depend on something inherent to biological brains? \ud83e\udde0 If you believe intelligence requires biological brains, then you are \"biological naturalist\", as John Searle. You don't think is possible for our computers to become truly intelligent irrespective of how powerful they become. At most they could simulate intelligence. \ud83e\udd16 Otherwise, you are a \"computationalist\", or \"functionalist\", as Alan Turing. You believe that minds are \"just\" sufficiently powerful computers, so nothing in principle stops our computers from becoming truly intelligent. It's just a matter of time. \ud83d\udc49 Whatever you chose to believe, some very smart people will agree with you. Either way, even if there are some examples of \"winning the Turing Test\", if all serious computer scientists agree on something, that is we are still far away from Strong AI. As usual, if you like this topic, reply in this thread or @ me at any time. Feel free to \u2764\ufe0f like and \ud83d\udd01 retweet if you think someone else could benefit from knowing this stuff. \ud83e\uddf5 Read this thread online at https://apiad.net/tweetstorms/philosophyfriday-intelligence Stay curious \ud83d\udd96: \ud83d\udcda https://www.csee.umbc.edu/courses/471/papers/turing.pdf \ud83d\udcc3 https://en.wikipedia.org/wiki/Turing_test \ud83d\udcc3 https://iep.utm.edu/chineser/ \ud83c\udfa5 https://www.youtube.com/watch?v=Qbp3LJvcX38 \ud83d\udde8\ufe0f You can see this tweetstorm as originally posted in this Twitter thread .","title":"The Turing Test"},{"location":"tweetstorms/philosophyfriday/mary-room/","text":"Hey, today is #PhilosophyFriday \ud83e\udd14! How about discussing some philosophical aspects of Computer Science? Today I want to talk about Mary's Room \ud83d\udc47 This is a very interesting thought experiment that is related to the nature of knowledge and the difference between knowing something and knowing about something. There are many variants of the experiment, but my favorite one goes like this \ud83e\uddf5: Mary is a renowned neuroscientist who has studied everything there is to study about human vision. She understands every little bit of science regarding how light enters the eye, how it is received, and how it is processed in the brain... \ud83d\udc47 Furthermore, Mary understands perfectly all the physics of color, she can say exactly which portion of the spectrum is called \"red\", \"green\", \"blue\", or any other color you can mention... \ud83d\udc47 Even more, Mary has interviewed thousands of people, scanned their brains, measured their retinas. She understands all the physical phenomena that happen when everyone else sees a color... \ud83d\udc47 But... Mary is absolutely color blinded. She has a rare genetic disease that makes her see everything in monochrome. So, being the top researcher in the whole world regarding the human experience of color, she has never \"experienced\" color herself... \ud83d\udc47 One day, a miracle of science allows Mary to regain the ability to see color, in all its glory. And for the first time in her life, Mary looks up, and she experiences the blue sky \u2601\ufe0f! \u2753 At this point, does Mary gain any new knowledge that she didn't previously have? Think carefully. There is no right or wrong answer, just let me know at this point what do believe and, more importantly, why. Before you leave, I have a few more comments to make \ud83d\udc47 \ud83c\udd70\ufe0f If you think Mary doesn't learn anything new, then you believe in \"physicalism\": the notion that the physical world is everything there is. In particular, all human experience is ultimately caused by physics and there is nothing above it. \ud83c\udd71\ufe0f If you think there is something new she attained after experiencing color, that she couldn't possibly have known before, then you believe in the existence of \"qualia\": subjective experiences that cannot be explained or understood without experiencing them. \ud83e\udd14 Most philosophers and scientists (including computer scientists) describe themselves as physicalists or at least materialists. However, it is very hard to explain what I experience as \"being red\" or \"being cold\" or \"being painful\", without having you experience it. \"But what does this have to do with computers\", you ask? Well, if qualia exist, then it is very hard to believe in strong artificial intelligence. Since computers will not experience the world the way we do, how will you know that they know what \"pain\" or \"love\" is? And that's it. Now I just want to leave the discussion open. There is no right or wrong answer here since philosophers themselves don't even agree (this is hardly surprising, though, put N philosophers in a room and you will have N+1 opinions \ud83d\ude06). If you want to read this in a more \"classic\" format or bookmark it for later, check it out here: https://apiad.net/tweetstorms/philosophyfriday-mary-room/ \ud83d\udde8\ufe0f You can see this tweetstorm as originally posted in this Twitter thread .","title":"Mary's Room"},{"location":"tweetstorms/philosophyfriday/orth/","text":"Today is #PhilosophyFriday \ud83e\udd14! I want to talk about the ethical implications of using AI, but this is a HUGE topic. So today I'll focus on one specific issue: \u2753 Will super-intelligent beings be nice to us? \ud83e\uddf5\ud83d\udc47 \ud83d\udc7d One of the great fears of humanity, and science fiction authors, in particular, is what would happen if we encounter super-intelligent alien species in the near future. Would they destroy us, adopt us, teach us, or ignore us? \ud83d\ude07 Some pretty clever people think that sufficiently advanced civilizations have to be \"good\", otherwise, they would have probably destroyed themselves. Being a dick would be kind of a Great Filter. Violent civilizations would not survive long enough to conquer the Galaxy. \ud83d\ude08 Others have the opposite view. Super-intelligent civilizations would have to be \"nasty\". Otherwise, they would have been destroyed by their enemies. In this view, being a dick would be evolutionarily inevitable. \ud83d\ude11 Others have the view that intelligence and moral are two orthogonal dimensions. You could have super-intelligent beings which are super good, super bad or anything in-between. Or they could have moral values completely incomparable to ours, unclassifiable as good or bad. \ud83d\uddfa\ufe0f If history tells us anything, there is no evidence that technologically advanced civilizations are any nicer than their less advanced neighbours. Such an encounter has never ended well for the least advanced civilization. But who's to say there isn't a technological level after which being good is a condition for further progress? We may very well be at the brinks of this transition right now. \u2753 What do you think? Why is this relevant? If we invent super-intelligent AI someday, they could be like aliens to us. \ud83d\udc4d If goodness is a necessary condition of super-intelligence, we have nothing to worry about. \u26a0\ufe0f Otherwise, we may already have little time left to solve this problem. And if there is even a small chance that we end up being destroyed by our god-children, shouldn't we be paying more attention to this problem? \u261d\ufe0f There are lots of pressing issues with AI today, though. Deciding what to prioritize is also important. As usual, if you like this topic, reply in this thread or @ me at any time. Feel free to \u2764\ufe0f like and \ud83d\udd01 retweet if you think someone else could benefit from knowing this stuff. \ud83e\uddf5 Read this thread online at https://apiad.net/tweetstorms/philosophyfriday-orth Stay curious \ud83d\udd96: \ud83d\udcc4 https://en.wikipedia.org/wiki/Existential_risk_from_artificial_general_intelligence \ud83d\udcda https://www.goodreads.com/book/show/44767248-human-compatible \ud83d\udcda https://www.goodreads.com/book/show/20527133-superintelligence \ud83c\udfa5 https://youtu.be/hEUO6pjwFOo \ud83c\udfa5 https://youtu.be/8nt3edWLgIg \ud83d\udde8\ufe0f You can see this tweetstorm as originally posted in this Twitter thread .","title":"Orthogonality Thesis"},{"location":"tweetstorms/philosophyfriday/scientific-method/","text":"Today is #PhilosophyFriday \ud83e\udd14! Today I want to talk about a very meta question: \u2753 How do you know that what you know is true? \ud83e\uddf5\ud83d\udc47 We all have some intuitive idea of the difference between opinions and facts. We know there are some things we believe in, but others might disagree, and still be right. And then there are some things that are simply impossible to object to. These last, we call them \"facts\", objective statements about the world that are simply, undeniably, true. Or are they? How can we be sure, like, sure-sure? And would we be able to recognize if they weren't? Let say I want to convince you that some idea I have about how reality works is true. Let's call that idea a \"hypothesis\". \ud83d\udc49 For example: \"there is an attraction force between Earth and everything else\". \u2754 How can I convince you? A possible way is trying to make some predictions from that hypothesis, and see if those predictions holds. \u2697\ufe0f For example, if such force exists, then I can lift a small rock with my hand, let it loose, and it should fall downwards, right? So we make the experiment, and indeed, the rock fell! Is that a proof that my hypothesis is true? \ud83e\udd14 Well, it could be, but there are lots of other explanations that might produce the same evidence. For example, rocks in particular could be attracted to Earth, not everything. So, in order to convince you I would have to drop everything and see that it falls? You see the problem with that... \ud83d\udd11 A sufficiently complex hypothesis can never be fully proven by evidence. Now, let's suppose I make a prediction, and it fails. \ud83d\udca9 For example: \"the Earth is flat\", so, if I shoot a sufficiently powerful laser at a big mirror, say, 100 KMs away, without obstacles (e.g., at the sea), I should see the reflection. (\ud83d\ude43 Go ahead, give it a shot) \ud83d\udc49 A failed prediction automatically renders a hypothesis false. Maybe not completely false, but at least you have to go back to the drawing board, and change something in the hypothesis that produces predictions which are more compatible with observations. This is, at its core, the Scientific Method. \ud83d\udd11 It's not about about proving that you're right. It's about trying very hard that to prove that your wrong, and consistently fail to do so. To this end, we ask any scientific hypothesis to be falsifiable : If it is false, there should be a way to prove it. \ud83d\udc49 No hypothesis is ever experimentally proven right. At most, it remains consistent with observations for a very long time, which makes it increasingly likely to be correct. Any currently accepted hypothesis is likely wrong, and we'll find an experiment that falsifies it. And then, from its ashes, we'll come up with a better, more realistic hypothesis. \u2b50 This is the purpose of Science, to build an incrementally better understanding of reality. What happens with unfalsifiable hypotheses? \ud83e\udd2f For example, if I say we are living inside a simulated universe run by some God-like intelligence that creates on-demand the expected effects of any observation, how can you prove me wrong? These type of hypotheses, at the very least, are useless as a means to achieve a better understanding of reality. They can also be harmful, by giving us a false expectation that we have figured it all out. Keep in mind though, that several important questions fall in this category: what is the meaning of life? can the universe be completely understood? is there a correct moral system? ... For some fundamental questions, Science cannot help. This doesn't mean that these questions are less important, though. \ud83c\udf1f But it does mean that, whatever answer you have, just keep in mind that you cannot objectively convince me that you're right. At most you can persuade me that I like your answer better than mine. So next time someone is trying to convince of something they belief, ask them: \ud83d\udd25 If that belief of yours where false, how could I show it to you? As usual, if you like this topic, reply in this thread or @ me at any time. Feel free to \u2764\ufe0f like and \ud83d\udd01 retweet if you think someone else could benefit from knowing this stuff. \ud83e\uddf5 Read this thread online at https://apiad.net/tweetstorms/scientific-method \ud83d\udde8\ufe0f You can see this tweetstorm as originally posted in this Twitter thread .","title":"Scientific Method"},{"location":"tweetstorms/philosophyfriday/simulation/","text":"Hey, today is #PhilosophyFriday \ud83e\udd14! How about discussing some philosophical aspects of Computer Science? Today's question is: \u2753 How do you know you're not living in a simulation? \ud83e\uddf5\ud83d\udc47 This idea is called the simulation hypothesis and it exists, in some form, way back in ancient Greek, Mayan, and Indian philosophy, passing through Descartes and Berkeley, and more recently popularized by Nick Bostrom. The modern (and simplified) version goes like this \ud83d\udc47 Either: 1\ufe0f\u20e3 it is impossible to simulate a high-fidelity universe; or 2\ufe0f\u20e3 any civilization capable of doing so has no interest in doing it; or 3\ufe0f\u20e3 we are living in a simulated universe. \u2753 Stop now and tell me what do you think? Let's analyze each alternative. 1\ufe0f\u20e3 It may well be that simulating a universe is impossible because of quantum mechanics or other unknown physical restrictions that we today don't understand. That is, high-fidelity physics might be uncomputable. \u261d\ufe0f But, there is little evidence for this, since we are already capable of simulating a nice chunk of physics, and it seems all we need is more computing power. The advent of quantum computers might make this possible, even if very costly. 2\ufe0f\u20e3 If it's possible to simulate a high-fidelity universe, then any civilization that can do it in principle will do it, unless: \ud83c\udd70\ufe0f it is way too costly to be of any value; or \ud83c\udd71\ufe0f it is morally forbidden, out of a sense of respect for the simulated individuals' lives. \u261d\ufe0f Sadly we have little evidence that morals would stop the human civilization from simulating other individuals, giving our history. Maybe \"mature\" civilizations are different. 3\ufe0f\u20e3 If it's technically feasible and interesting for any civilization advanced enough to simulate a high-fidelity universe, then we are almost certainly living in one. \u2753 Why? Either we are simulated or we are not. In either case, with enough computing power, we will be able to simulate a universe ourselves, since this universe is either real or a high-fidelity simulation. \ud83d\udca1 So there should be arbitrarily deep hierarchies of simulated universes. Hence, what is more likely? That we just happen to be the one civilization at the top of the hierarchy, or that we are somewhere inside the hierarchy? Without further prior evidence we must believe that all steps in the hierarchy are equally likely. \u261d\ufe0f So, we are simulated. \ud83e\udd2f Think about the implications! Either you believe simulating a universe is physically impossible or universally tabu, or you have to believe you are living inside a simulation! \u2753 Tell me again, what do you think now? There are plenty of objections to this thought experiment but, in principle, if we were living inside a simulation, we could be unable to prove it, since any evidence we gather could be part of the simulation. \ud83d\ude2c So, science appears to have no way to attack this problem. Recently, a compelling argument from @neiltyson has brought peace to my mind again, turning the hierarchy logic on its head. \u261d\ufe0f Given that almost all movies are based on the modern era, if we are simulated, why would we exist in the pre-simulation era? \ud83d\ude01 In any case, if we are living inside a simulation, maybe it's better if we don't find out. That could be the \"game over\" after which the simulation is restarted. As usual, if you like this topic, reply in this thread or @ me at any time. Feel free to \u2764\ufe0f like and \ud83d\udd01 retweet if you think someone else could benefit from knowing this stuff. \ud83e\uddf5 Read this thread online at https://apiad.net/tweetstorms/philosophyfriday-simulation Stay curious \ud83d\udd96: \ud83d\udcda https://www.simulation-argument.com/simulation.pdf \ud83d\udcc3 https://en.wikipedia.org/wiki/Simulation_hypothesis \ud83d\udcc3 https://en.wikipedia.org/wiki/Simulated_reality \ud83c\udfa5 https://youtu.be/nnl6nY8YKHs \ud83c\udfa5 https://youtu.be/pmcrG7ZZKUc \ud83d\udde8\ufe0f You can see this tweetstorm as originally posted in this Twitter thread .","title":"The Simulation Hypothesis"},{"location":"tweetstorms/philosophyfriday/trolley-cars/","text":"Today is #PhilosophyFriday \ud83e\udd14! The year is 2035. You're sitting comfortably in your L5 self-driven car, zooming across the highway. Suddenly, the truck in front drops a big boulder. In a split second the car AI has to make a choice: brake or dodge... \ud83e\uddf5\ud83d\udc47 \u26a0\ufe0f Even with a full brake you're not guarantee to survive the crash. \ud83d\ude99 To your left there's a van driven by a human. \ud83d\udeb4\u200d\u2642\ufe0f To your right there's an unprotected biker. \u2753 What should your AI do? \u26a0\ufe0f If the AI chooses to brake, the boulder will still hit you, with potentially disastrous consequences. And it's very hard to sell you an AI that will make the choice with the least chance to save your life. \ud83d\ude99 If it chooses to hit the van, you'll most likely survive, and the other driver has a pretty decent chance too. \ud83d\udeb4\u200d\u2642\ufe0f If it chooses to hit the biker, he/she will almost certainly die, but you'll maximize your chances of not getting hurt. If the AI is trying to save you at all costs, it will kill the biker. That's the optimal solution for you. But we probably don't want that, so let's program it to hit the van, the choice that minimizes the odds of someone getting killed, right? \ud83d\udc49 Now, let's suppose it's two bikers, one each side, but here's the catch: one of them is wearing a helmet. What's the option that minimizes the odds of someone dying? Even if by a very small chance, now hitting the biker with a helmet seems the best option. \ud83e\udd14 Suddenly, doing the right thing as a biker (wearing a helmet) makes you more likely to be hit by a car. We probably don't want that either, or everyone will be biking with the least possible protection. But wait, it gets worse... \ud83d\udc47 \ud83d\udc49 What if there's two identical bikers, but one is carrying a baby, the other is carrying a pregnant women, and you're carrying two small children? What do you value more, a potential life lost or a certain life lost? Does it matter what is the expected lifetime left? If you're the one driving, you'll do whatever your instincts tell you. And whatever that decision is, any court or judge will have the consider the fact that you didn't have enough time to think. But an AI had enough time to think. It made a conscious decision. \ud83d\udc49 That decision was either preprogrammed, or computed from a preprogrammed formula, or learnt from data, or reinforced, ... In any case, there are humans behind it who had plenty of time to think carefully about it. And they made a conscious choice, or didn't they? A possible solution is to refuse to make a choice at all. \ud83c\udfb2 In an impossible situation let the AI flip a coin, so whatever happens it was due to luck. But is it morally correct to refuse to solve a literal life-or-death problem if we have even a slight chance to solve it? \u2753 Why is this question so hard? Well, moral is tricky. It feels immoral to even consider there might be a predefined answer for who's life is worth more. But an AI needs a formula in this case, even if it's \"choose random\", and we better come up with one we can live with. As usual, if you like this topic, reply in this thread or @ me at any time. Feel free to \u2764\ufe0f like and \ud83d\udd01 retweet if you think someone else could benefit from knowing this stuff. \ud83e\uddf5 Read this thread online at https://apiad.net/tweetstorms/philosophyfriday/trolley-cars","title":"The Self-Driven Car Dilemma"},{"location":"tweetstorms/technicaltuesday/automl/","text":"Today is #TechnicalTuesday \ud83e\udd13! Let's talk about practical technologies that you can use today. In this thread I will tell you about AutoML \ud83e\uddf5\ud83d\udc47 AutoML stands for Automated Machine Learning . It encompasses a bunch of technologies and paradigms to gradually automate the process of creating machine learning solutions. \ud83d\udca1 AutoML is about raising the abstraction level in ML and reducing the grunt work. \u2753 What can AutoML do today? Getting a machine learning solution to work takes a few steps: 1\ufe0f\u20e3 collecting data 2\ufe0f\u20e3 sanitizing that data 3\ufe0f\u20e3 finding the best model 4\ufe0f\u20e3 training that model 5\ufe0f\u20e3 and beyond, actually build the product\u2757 Most current AutoML frameworks today focus on 3\ufe0f\u20e3, i.e., helping you select among the plethora of machine learning models which is the best for your problem. This problem is often framed in terms of: \ud83c\udd70\ufe0f model selection \ud83c\udd71\ufe0f hyperparameter optimization \ud83c\udd70\ufe0f Model selection is about deciding, e.g., if logistic regression, decision trees or SVM is better, or whether to encode with word2vec or TF-IDF. The \"manual\" way of doing this is to actually try each algorithm a bunch of times in your data and collect some statistics. \ud83c\udd71\ufe0f Hyperparameter optimization is about selecting the exact value for each tunable thing in your algorithm. How many neurons? How much dropout? Which activation function? Which regularization factor? ... If you combine both problems, then you realize there are literally thousands (and potentially infinite) different algorithms you can try on your data. If you were to do this yourself, the simplest solution is something like this: \u2b50 Actually, AutoML algorithms are way smarter and faster than random search. \ud83d\udca1 AutoML frames this problem as an optimization loop on top of the training loop and applies a lot of clever optimization tricks. \ud83d\udc49 AutoML frameworks hide away all that complexity behind an interface that looks as if you are training a single model, but it is ultimately doing the search and optimization loop under the hood. Let's see a couple of examples \ud83d\udc47: \u2764\ufe0f Auto-Sklearn is an AutoML framework compatible with scikit-learn. \ud83d\udd17 https://automl.github.io/auto-sklearn/master/ You can basically replace standard scikit-learn code with a generic Auto-Sklearn classifier and suddenly you are evaluating thousands of models: \u2764\ufe0f Auto-Keras is an AutoML framework specifically designed for deep learning with Keras. \ud83d\udd17 https://autokeras.com/ Instead of manually designing a neural network, you can use Auto-Keras predefined \"meta-models\" and it will take care of finding the best architecture: Yeah, I know \ud83e\udd2f! And AutoML is much more than model selection and hyperparameter search. It can also include automating: data preprocessing feature engineering feature selection dataset augmentation model distillation and more... \ud83d\udd11 If you are working on a practical problem today there is no reason not to use AutoML. \ud83d\udd11 Even if you are working on research, AutoML will make you more productive by taking care of the dumb tasks and letting you focus on the important parts. \u2757However, this is no silver bullet. There are a lot of challenges to make AutoML production-ready. Data cleaning is a major bottleneck still, far from automated. And we need to understand how these methods exacerbate data bias. Finally, if you are feeling adventurous, you can try @auto_goal , an experimental AutoML framework that goes beyond \"standard\" AutoML. \u2b50 Check it out in https://autogoal.github.io ! As usual, if you like this topic, reply in this thread or @ me at any time. Feel free to \u2764\ufe0f like and \ud83d\udd01 retweet if you think someone else could benefit from knowing this stuff. \ud83e\uddf5 Read this thread online at https://apiad.net/tweetstorms/technicaltuesday-automl Stay curious \ud83d\udd96 \ud83d\udd17 https://www.automl.org/automl/ \ud83d\udcda https://www.automl.org/book/ \ud83c\udfa5 https://youtu.be/3c0FoQrsJxo \ud83c\udf81 https://github.com/windmaple/awesome-AutoML \ud83d\udde8\ufe0f You can see this tweetstorm as originally posted in this Twitter thread .","title":"AutoML"},{"location":"tweetstorms/technicaltuesday/streamlit/","text":"Today is #TechnicalTuesday \ud83e\udd13! Let's talk about practical technologies that you can use today. In this thread, I will tell you about @streamlit \ud83e\uddf5\ud83d\udc47 \ud83d\udca1 Streamlit is a Python framework to make data science apps blazingly fast. The core value proposition is that you don't need any HTML, CSS or JavaScript, just pure Python. Take a look: \u2753 Why do you want this? As a data scientist, you spend most of your time designing, experimenting and testing models. You probably use Jupyter Notebooks a lot, right? What happens when you have to show those models working? If you're working for another data scientist, you can probably just share your Python scripts or Notebooks, but at some point in the chain, someone who doesn't ready code will need to see your model in action. At this point, you (or someone) has to build a minimal application, an MVP, that at least shows some input controls, allows you to play with some model parameters, and lets you render some graphs. This often takes the form of a Flask application with a minimal HTML frontend, possibly with a minimum JavaScript. And don't forget CSS. \u2753 The question is, who builds this MVP? \ud83d\udc49 We the data scientists either don't have the skills or we have to spend a ridiculous amount of time dealing with frontend details we couldn't care less (since this is not a product, it's just a demo). \ud83d\udc49 The frontend team is usually busy with, you know, building the real product, and it's ineficient to use their time for building this app which, again, is not the final product. \ud83e\udd37 But someone has to do it. Enter @streamlit . The frontend framework for pragmatic data scientists. \ud83d\udc4d Forget about writing any HTML, CSS or JavaScript. Everything is pure Python code, and you get a nice reactive app for free. \ud83d\udc4d Forget about routes, templates, sessions and global state. Your code looks exactly like a Python script because it is a Python script. Everything is executed top-to-bottom every time the user changes something. \ud83d\udc4d And you get a super powerful caching mechanism as a simple Python decorator that you can use in any heavy method (e.g., downloading data, training a model). So forget about having to manually store all that data. There is one main caveat: \u26a0\ufe0f You have very little customization for layouts or styling. But you don't care about that, remember, this is not the final product, is just a demo to showcase some prototype model. If you're like me, you just want to have a frontend to show today with the least possible throw-away effort. \u2b50 Take @streamlit for a spin today. I promise it'll be worth it. As usual, if you like this topic, reply in this thread or @ me at any time. Feel free to \u2764\ufe0f like and \ud83d\udd01 retweet if you think someone else could benefit from knowing this stuff. \ud83e\uddf5 Read this thread online at https://apiad.net/tweetstorms/technicaltuesday-streamlit Stay curious \ud83d\udd96 \ud83d\udd17 https://streamlit.io/ \ud83d\udcda https://github.com/streamlit/streamlit \ud83c\udfa5 https://www.youtube.com/channel/UC3LD42rjj-Owtxsa6PwGU5Q \ud83d\udde8\ufe0f You can see this tweetstorm as originally posted in this Twitter thread .","title":"Streamlit"},{"location":"tweetstorms/theorythursday/algorithm-complexity/","text":"Hey, guess what, today is #TheoryThursday \ud83e\uddd0! A silly excuse I just invented to share with you random bits of theory from some dark corner of Computer Science and make it as beginner-friendly as possible \ud83d\udc47 Today I want to talk about Algorithmic Complexity . To get started, take a look at the following code. How long do you think it will take to run it? Let's make that question more precise. How long do you think it will take to run it in the worst-case scenario? We can see that the code will run slower if: \ud83d\udc49 your computer is older; \ud83d\udc49 the array is longer; or \ud83d\udc49 x happens to be further to back, or not present at all. Can we turn these insights into an actual formula? We will have to get rid of ambiguous stuff like \"old computers\". 1\ufe0f\u20e3 First, let's consider an abstract computer in which all \"atomic\" operations take exactly 1 unit of time. \ud83e\udd14 Defining exactly what is an \"atomic\" operation is far from trivial. For now, assume it's things like arithmetic operations, indexing, invocation. 2\ufe0f\u20e3 Second, we'll count the number of operations with respect to the size of an arbitrary array. We will say something like \"this will cost 2 units of time for each element of the array\". 3\ufe0f\u20e3 Finally, we will consider the worst-case scenario. So we assume, in this example, that the element x is not in the array. More generally, we will always think about the maximum number of operations that could potentially happen. With these ideas in mind, we are ready to define the Algorithm Complexity of this algorithm. Let's count how many operations are performed in each step, assuming our array has length N: Depending on how detailed you want to be counting, you could say we have something like \ud83d\udd253*N+1\ud83d\udd25 operations in the ultimate worst-case scenario. \u2753 Now, why do we care about this? The reason is that we can now compare different algorithms. For example, if your implementation takes \ud83d\udd255*N+3\ud83d\udd25, then it is worse, right? Well, it's not \ud83d\ude1d Here's the deal: we have been assuming that all \"atomic\" operations are equally costly, but this is not true... Hence, it makes no sense to compare my implementation with your implementation by looking at those tiny differences. My 5 * N could be faster than your 3 * N if my \"atomic\" operations are simpler. \ud83d\udca1 We want a complexity measure that smooths away all implementation details. To achieve this, we will take away everything unimportant when N becomes very large. We will consider that: \ud83d\udc49 N+a and N+b are the same; \ud83d\udc49 a N and b N are the same; ... for any finite values a and b. \ud83d\udd11 And instead of saying 3*N+4, we will say the asymptotic algorithmic complexity is O(N). This is called big-O notation. \ud83d\udca1 We call this linear complexity because the number of operations grows linearly with respect to the size of the array. \ud83e\uddd0 Formally, it means that your function's cost is something that is bounded by a linear function. \ud83d\udca1 Intuitively, what this means is that in the long run, small differences like specific operations matter less than the capacity your algorithm has to scale with more data. The reason is simple, an algorithm with a lower asymptotic complexity will eventually win. Take for example binary search. https://en.wikipedia.org/wiki/Binary_search_algorithm It takes a bit of thinking, but we can prove the asymptotic notation to be O(log N). Binary search is doing much more work in each iteration than linear search. It could be 20 * log N vs 3 * N. Hence, with very small arrays, linear search could be better. \ud83d\udd11 But there is always a value of N after which binary search will win, and in any hardware. \u2764\ufe0f And that's it. We have just arrived at the intuitive notion of asymptotic complexity! Calculating it can be daunting for some non-trivial algorithms, but here are some tips for estimating it: 1\ufe0f\u20e3 Every nested for loop from beginning to end usually means another exponent. For example, two nested loops usually mean O(N^2), three nested loops, O(N^3), and four nested loops means you really need to take a break and, afterward, please refactor that code. 2\ufe0f\u20e3 An invocation to function F inside a loop means you have to multiply N times the complexity of F. For example, if we call binary search for each element of the array, the resulting algorithm is O(N log N) 3\ufe0f\u20e3 In recursive methods, if you split at the middle and recurse down only one branch, that's O(log N). If you recurse down both branches, you usually have O(N log N). These are special cases of a more general rule for recursive methods: https://en.wikipedia.org/wiki/Master_theorem_(analysis_of_algorithms) Finally, we have just scratched the surface in this thread. Algorithmic complexity is a fascinating topic that touches all fields in Computer Science. The most important problem in all of CS comes from here, the infamous \ud83d\udd25P vs NP\ud83d\udd25. But that's a story for another Thursday \ud83d\ude09. \ud83d\udde8\ufe0f If you like this idea, then get into the conversation! Steal this #TheoryThursday hashtag an tell me about your favourite piece of dark magic. \ud83d\udd14 Just @ me and I'll make sure to weigh in. \ud83d\uddd2\ufe0f Here is the full thread source: https://apiad.net/tweetstorms/theorythursday-algorithm-complexity/ \ud83d\udde8\ufe0f You can see this tweetstorm as originally posted in this Twitter thread .","title":"Algorithm Complexity"},{"location":"tweetstorms/theorythursday/cfg/","text":"Today is #TheoryThursday \ud83e\uddd0! Allow me share with you a previous thread of mine about formal language theory: \u2753 Why can't you use regular expressions to process HTML? \ud83e\uddf5\ud83d\udc47 You probably know a bunch of different \"programming\" languages, right? Python, Java, C++, JavaScript, HTML... There have different keywords, expressions, punctuation rules, braces, ... \u2753 What is the one thing they all have in common? \ud83d\udc49 They can be infinitely nested. In all these languages you have some constructions (e.g., expressions) which can be as complex as you desire. These constructions are defined recursively. For example: \u270f\ufe0f An expression is either a number, or a sum, subtraction, multiplication, or division of two expressions. \u2753 But is HTML a programming language or not? Who cares? HTML has recursive constructions as well. From the syntax perspective, which is what we are discussing here, it is of the same nature as all the other. (\u261d\ufe0f We are talking about syntax, not semantics here) \u2b50 These are all called Context-Free languages. Their fundamental characteristic is that they can be defined with a formalism called Context-Free Grammars (CFG). (\u261d\ufe0f There are types of languages which are not context-free) Informally, a CFG is a set of transformation rules that define which sequences of symbols are valid in a given language. For example, take arithmetic expressions: \ud83d\udc4d ( 31 + ( 245 * 15 ) ) -- is a valid sequence \ud83d\udc4e 63 + ) 23 * ( -- is an invalid sequence Here is one possible grammar for expressions. Starting with the symbol <Expression> and applying randomly any rule (called a production ), you will always reach some combination of NUMBER and the symbols + , - , * , / , ( , ) that is valid. This is called a derivation . \ud83d\udc49 What's more, any possible arithmetic expression can be formed by some sequence of applications of that grammar rules. Even better, in this particular example, there is exactly one such sequence for any valid expression. \u2753 Why is this relevant? Because we can analyze the syntactic complexity of all programming languages just by analyzing the language for expressions. They are all equally complex. The fundamental question we want to answer is this \ud83d\udc47 \u2753 What is the simplest program than can recognize all (and only) valid expression? This is called \"parsing\" an expression. I'm sure you've heard the term. \ud83d\udc49 Parsing is finding the sequence of transformations that goes from <Expression> to whatever you want to parse. If that sequence exists, then we know that it is a valid expression, and even more, we know everything we need to evaluate it. \ud83d\udca1 Parsing is the first step that all compilers and interpreters do, from Python to C++, to yes, the HTML parser in Chrome. Now we come round back to regular expressions. \ud83d\udc49 It turns out that standard regular expressions are computationally insufficient to parse any language that requires a context-free grammar. (\ud83d\ude22 While proving this is not hard, sadly it is too long for this thread) You can say, intuitively, that regular expressions cannot \"count\" the matching opening and closing parenthesis. This is because regexes, deep down, are just overpriced finite automata. (\ud83d\ude44 Yes, I know this is more nuanced, I'm talking about standard regexes only) So, next time someone suggests you to \"just use a regex to parse that HTML\", you can \ud83e\udd26 and answer, in formal language lingo: \"no, regexes cannot parse context-free languages\". You won't make any new friends, but at least you'll be right \ud83d\ude1d As usual, if you like this topic, reply in this thread or @ me at any time. Feel free to \u2764\ufe0f like and \ud83d\udd01 retweet if you think someone else could benefit from knowing this stuff. \ud83e\uddf5 Read this thread online at https://apiad.net/tweetstorms/theorythursday/cfg Stay curious \ud83d\udd96: \ud83d\udcc4 https://en.wikipedia.org/wiki/Context-free_grammar \ud83d\udcda https://suif.stanford.edu/dragonbook/ \ud83c\udfeb https://www.edx.org/course/compilers \ud83d\ude06 https://stackoverflow.com/questions/1732348/ \ud83d\uddbc\ufe0f https://xkcd.com/208/","title":"Context-Free Grammars"},{"location":"tweetstorms/theorythursday/np-complete/","text":"Today is #TheoryThursday \ud83e\uddd0! The most important question in all of Computer Science is probably whether P equals NP. That is, are all problems easy to validate also easy to solve? How can we even start to ponder into this question? \ud83e\uddf5\ud83d\udc47 \ud83d\udd16 In case you're just hearing about this, here's a short refresher on what we are talking about: https://apiad.net/tweetstorms/theorythursday-np-complete Let's think now about what would it mean to properly answer this question. \ud83d\udc47 Say we want to answer \"yes\". How can we do that? Do we have to check that all NP problems are actually in P? \ud83d\udc49 To begin, there are infinitely many problems in Computer Science. There must a better way... \ud83e\udd14 What if we could prove that just answering yes to one problem would automatically answer yes to any other problem ? Imagine that, a CS problem that would represent them all, such that solving that problem would immediately give you a solution to any other (NP) problem. Say we have two problems A and B. For example, say A = sorting an array (SORT), and B = finding the minimum element (MIN). How can we prove that B is at least as easy as A? In other words, if we have a polynomial algorithm for solving A, we must also have one for solving B. \ud83d\udc47 What we need is a polynomial-time reduction from B to A. This sounds complicated, but it's very intuitive. \ud83d\udc49 What we want is a way to implement MIN provided that you can call SORT, and that implementation has to be of polynomial complexity. For example: Now we can definitely say that MIN is at least as easy as SORT. Maybe it's even easier (and we know it is, you can solve MIN in O(N) and SORT only in O(N log N) in the general case). \ud83d\udd11 The question now is, can we do this with one problem A for all other problems B (in NP)? It turns out, this is exactly what Stephen Cook and Leonid Levin, independently, both showed around 1971. But how does this work? What does it mean to find such a problem? \ud83d\udc47 The problem in question is Boolean Satisfiability (SAT), and it works like this: \u2753 Given a Boolean formula, with variables and Boolean operators, such as x & y | z , is there any assignment of True/False to each variable that makes the formula True? Sometimes the formula is extremely easy, but when it involves several copies if the same variable, some positive and some negative, it is not trivial to, just looking at the formula, decide if it's satisfiable. \ud83d\udc49 However, it is very easy to validate, so SAT is clearly in NP. \ud83e\udd14 Now, how can you go on proving that SAT is a problem to which any other problem in NP is polynomially-reducible to? First, remember we are concerned only with decision problems, i.e., problems whose answer is True or False. Here are some intuitions... \ud83d\udc47 Say we have a decision problem B that is in NP. This means there is a polynomial algorithm V to verify if a solution of B for a specific input I is indeed True. \ud83e\udd2f We can create a Boolean formula F that basically says \"B answered True to input I\", for every possible input I. This formula is quite large, but it is always possible to build it for a specific input I, by using the code of the verification algorithm. \u261d\ufe0f That means (believe me here) that the formula remains polynomial with respect to the length of I. \ud83d\udc49 An intuitive way to see that such formula must be possible to build, is to consider that whatever V is, it is a program composed in the end of a bunch of basic instructions. \u261d\ufe0f We can always build a formula, in principle, that says \"this instruction was executed\" and so kind of trace the execution of V in a Boolean form. Formally, this is done with the Turing-machine definition of V, but intuitively, think of it as disassembling the code of V. For any input I, we can build in polynomial time a formula F that is satisfiable iif B would answer True to I. We can then run SAT(F), and we have a polynomial-time solution for B(I). \u2b50 And that's it! We have one problem, SAT, that can solve any other problem in NP. This problem, SAT, is the first of the so-called NP-Complete problems. A problem in NP that is so general, that any other problem in NP could be solved in polynomial time if you could solve that one problem. But is not the only one! \ud83d\udc47 The most surprising finding is not SAT itself, but rather that is there is a huge number of such problems, seemingly very different, but in their core all equivalent: \ud83d\udd39 Traveling salesman, \ud83d\udd39 Scheduling, \ud83d\udd39 Knapsack, \ud83d\udd39 Clique, \ud83d\udd39 Graph coloring, \ud83d\udd39 Hamiltonian cycles, and more! As usual, if you like this topic, reply in this thread or @ me at any time. Feel free to \u2764\ufe0f like and \ud83d\udd01 retweet if you think someone else could benefit from knowing this stuff. \ud83e\uddf5 Read this thread online at https://apiad.net/tweetstorms/theorythursday-np-complete Stay curious \ud83d\udd96: \ud83d\udcda https://dl.acm.org/doi/10.1145/800157.805047 \ud83d\udcda https://people.eecs.berkeley.edu/~luca/cs172/karp.pdf \ud83d\udcc4 https://en.wikipedia.org/wiki/Cook%E2%80%93Levin_theorem \ud83d\udcc4 https://en.wikipedia.org/wiki/Karp%27s_21_NP-complete_problems \ud83c\udfa5 https://www.youtube.com/watch?v=dJUEkjxylBw","title":"NP-Completeness"},{"location":"tweetstorms/theorythursday/pnp/","text":"Today is #TheoryThursday \ud83e\uddd0! Allow me to share with you a previous thread about a fundamental question in Computer Science: \u2753 Does P equal NP? If you have heard of this and want to learn a bit more, read on... \ud83e\uddf5\ud83d\udc47 Computer Science is all about finding clever ways to solve difficult problems. We have found clever algorithms for a bunch of them: sorting stuff, finding shortest paths, solving equations, simulating physics... But some problems seem to be way too hard \ud83d\udc47 One example is the Travelling Salesman problem. \u2753 Find a cycle starting in your city to visit all major cities in your country and return home with the least fuel cost. This is the kind of problem we expect computers to solve easily, right? That's what computers are for! \ud83d\ude48 Well, very smart people have tried, and no one has come up with an algorithm that is always better than simply trying all possible cycles. The problem is that the number of cycles grows exponentially faster than the number of cities! Let's make it even easier, what about if I simply ask: \u2753 Is it possible to visit all cities spending less than X dollars in fuel? \ud83d\ude48 No one still knows an algorithm to answer that question precisely for any value of X without trying all cycles, which again is exponential. \ud83d\ude2c So, are we simply dumb, or is this problem so complex that it is impossible to find a clever algorithm to solve it, in the general case ? This is the root of possibly the most important question in all of Computer Science: P vs NP. Answering this question is way harder than it seems. You see, most questions in CS are about objects: how to sort, how to compare, how to process... But this is a meta-question: \u2753 Are there questions about objects that are intrinsically very hard to solve? Stephen Cook tried to answer this question in the early days of Computer Science. He came up with the following definitions: Suppose we have a question of the form: \u2753 Is there an object X with a given property Q? \ud83d\udc49 We want to define how hard is this question to answer. An example of an easy question of this type is the following: \u2753 Given an array of N elements, is there an element smaller than X? Answering this question is easy. Look at each element, one by one, and compare it with X. It takes at most N steps, for any possible array. This is an example of a problem in P. \ud83d\udd11 P here means \"Polynomial-Time Complexity\". Intuitively, a problem is in P if there is an algorithm to compute a correct answer in polynomial time. Now back to the Travelling Salesman, suppose I give you an answer: \ud83d\udc49 Yes, here, this is a cycle with cost less than X. How can you verify that answer is correct? You just add the costs of all edges in the cycle. It takes again N steps. This is an example of a problem in NP. \ud83d\udd11 NP here means \"Non-deterministic Polynomial-Time Complexity\". Intuitively, a problem is in NP if there is an algorithm to verify a correct answer in polynomial time. P problems are easy to solve. NP problems, we don't know yet, but at least they are easy to verify. That's the key idea. \u26a0\ufe0f Note that P problems are also NP. Now, the P vs NP question, formally, is this: \u2753 Are there problems in NP that are not in P? P vs NP is basically asking if there are problems that are inherently harder to answer than to verify, independently of how smart we become in the future. \ud83e\udd14 Think about what this means for a second, and ask yourself what's your intuition about it. What's the right answer? We still don't know, but most computer scientists believe that P is not equal to NP. The reasons are mostly philosophical but there is also evidence that, if P were equal to NP, a lot of weird things would happen. This question is at the core of Computer Science because it talks about the nature of computation and its inherent limits, regardless of technological improvements. We'll end here for now, but there is so much left to talk about (like NP-completeness). So stick around \ud83d\udc4b... As usual, if you like this topic, reply in this thread or @ me at any time. Feel free to \u2764\ufe0f like and \ud83d\udd01 retweet if you think someone else could benefit from knowing this stuff. \ud83e\uddf5 Read this thread online at https://apiad.net/tweetstorms/theorythursday/pnp Stay curious \ud83d\udd96: \ud83d\udcda https://dl.acm.org/doi/10.1145/800157.805047 \ud83d\udcda https://dl.acm.org/doi/10.1145/1562164.1562186 \ud83d\udcc4 https://en.wikipedia.org/wiki/P_versus_NP_problem \ud83c\udfa5 https://youtu.be/dJUEkjxylBw","title":"P vs NP"},{"location":"tweetstorms/wisdomwednesday/_topdown-vs-bottomup/","text":"Today is #WisdomWednesday \ud83e\udd29! Two opposite but complementary problem-solving paradigms are \u23ecTop-Down and \u23ebBottom-Up. Challenging problems require both, but most of us are tuned to one of these. \u2753 How do they work and how can we effectively use them? \ud83e\uddf5\ud83d\udc47 Any problem-solving strategy starts with two extremes, two dots we have to connect: What we need \ud83c\udf1f \ud83d\udd39\ud83d\udd39\ud83d\udd39 \u2699\ufe0f What we have In \u23ecTop-Down, we start with the problem and work our way down We ask ourselves, if we cannot solve X, can it be divided into some Y and Z such that solving them would help solving X? We keep dividing and descending until we reach a small problem we know how to solve. In \u23ebBottom-Up, we start with what we have, what we know, and work our way up. We say, given that we know X, what problem","title":" topdown vs bottomup"},{"location":"tweetstorms/wisdomwednesday/automate/","text":"\u2753 Are you prone to spending 6 hours automating something that takes you 10 minutes to do by hand? I am, and I don't think that's a bad thing. Hear me out... \ud83d\udc47\ud83e\uddf5 1/15 \ud83e\udd14 Conventional wisdom seems to agree we should only automate a task if it will pay out in the long run by reducing the aggregated time we spend on it over a period of time. Mandatory @xkcd comic: https://xkcd.com/1205/ \ud83d\udc47\ud83e\uddf5 2/15 This is a very pragmatic view, and it makes sense. If automating a (boring) task will save time in the long run, of course I'm all for it! \ud83d\udc49 But even if it doesn't, I argue there are still 3 powerful reasons to automate it. \ud83d\udc47\ud83e\uddf5 3/15 These reasons are: 1\ufe0f\u20e3 Productivity, 2\ufe0f\u20e3 Synergies, and 3\ufe0f\u20e3 Learning. \ud83d\udc47\ud83e\uddf5 4/15 1\ufe0f\u20e3 Productivity \ud83d\udc49 Nothing kills productivity like context switching. A simple, boring task that only takes 10 minutes can completely break your flow and cost you much more than those 10 measurable minutes. \ud83d\udc47\ud83e\uddf5 5/15 Focused work compounds, the third hour might an be order of magnitude more valuable than the first two. \ud83e\udd29 By automating boring tasks we reduce the chance of losing our flow when we are in a highly productive streak. \ud83d\udc47\ud83e\uddf5 6/15 2\ufe0f\u20e3 Synergies \ud83d\udc49 Automated stuff connect to each other Once you make the first step towards automating a task, you'll discover a universe of opportunities to connect it to. \ud83d\udc47\ud83e\uddf5 7/15 Automation creates synergies in two dimensions: \u2194\ufe0f Horizontally, as more people benefit from the same automation. \u2195\ufe0f Vertically, as more tasks become automatable by relying on other tasks. \ud83d\udc47\ud83e\uddf5 8/15 \u2194\ufe0f There is a non-neglible chance that your boring task is also boring for a lot of people. \ud83d\udca1 A fair share of products are born as solutions to their makers' problems. Those bored people are your first potential clients! \ud83d\udc47\ud83e\uddf5 9/15 \u2195\ufe0f A manual task scales close to linearly with respect to effort. \ud83d\udca1 An automated task scales closer to exponentially, as your main resource is computing power, and computing power is cheaper than human power! \ud83d\udc47\ud83e\uddf5 10/15 3\ufe0f\u20e3 Learning \ud83d\udc49 The best learning environments are those where is safe to fail. Everyone knows that you learn more by doing stuff than just reading stuff, but doing stuff can be risky. \ud83d\udc47\ud83e\uddf5 11/15 \ud83e\udd29 By automating boring tasks you get both a chance to try new things, new tools, new paradigms; and, a safety cushion if/when those things don't work. You can always keep doing it manually if you failed to automate it, but at least you learned something \ud83d\ude05! \ud83d\udc47\ud83e\uddf5 12/15 There are times, though, when a boring manual task comes in handy. \ud83e\udd14 If you're experiencing burnout, maybe boring and easy tasks that you can cross off will give you that much needed sense of success. \ud83d\udc47\ud83e\uddf5 13/15 What do you think? \u2753 Do you see any value in automating stuff even if it will take you much longer than doing it manually? \ud83d\udc47\ud83e\uddf5 14/15 \ud83d\udd16 You can read this thread online at https://apiad.net/tweetstorms/wisdomwednesday/automate/ . \u2764\ufe0f If you liked this thread, please consider retweeting, following, and liking it, if you think I've earned it \ud83d\ude19. \u2693\ud83e\uddf5 15/15","title":"Automate"},{"location":"tweetstorms/wisdomwednesday/designthinking/","text":"Today is #WisdomWednesday \ud83e\udd29! Have you ever had an amazing idea, built a product around it, and found out no one wanted it? I have, plenty of times. Part of the issue is we started from the solution, but there is a better way. \u2b50 This is a thread on Design Thinking \ud83e\uddf5\ud83d\udc47 Creating a product, whether a software, a physical thing, or a service, is all about solving a problem. \ud83d\udc49 Someone has a problem, and you offer a solution. But we often start with a solution, in the hope that someone wants it, and we end up solving an inexisting problem. This happens because, in real life, problems are not cleanly written in some kanban board. \u26a0\ufe0f We often know that something is not right, and we think we know how to solve it but, actually, we don't even have a clear idea of what's wrong. Enter Design Thinking \ud83d\udc47 Everything stems from these principles: 1\ufe0f\u20e3 The problem is never well defined. 2\ufe0f\u20e3 The problem and the solution must co-evolve together. 3\ufe0f\u20e3 The solution is never completed. And one key idea: \u2b50 The human being is the most important element of the solution. Based on these principles, the process itself is a continuous cycle of five steps: \ud83d\udd2d Understand \ud83d\udcdd Define \ud83d\udca1 Ideate \ud83d\udee0\ufe0f Prototype \u2697\ufe0f Test \ud83d\udd2d You start by observing people, your potential users or clients. Ask yourself: \u2753 How are they solving the problem now? \u2753 What is their critical pain in doing so? \u2753 Why is that happening? \ud83e\udd14 These questions sound familiar to you? Useful tools and practices: \ud83d\udc49 Observation. Just watch people in their routine and take notes. \ud83d\udc49 Interviews, but not with standardized questionnaires; instead, talk openly with people, and listen. \ud83d\udc49 Ask \"why\" at least 5 times to get to the core of the problems. \ud83d\udcdd Then you have to clearly define one problem to solve. Focus is key here. A good definition is actionable. It describes a situation that is easy to validate when it has been solved. \ud83d\udd2c Go deep, you don't want to solve the superficial problem but the underlying cause. Here are some techniques: \ud83d\udc49 Use a board (physical or virtual) and put a note for every potential insight or problem. \ud83d\udc49 Make clusters, group similar problems and insights together. \ud83d\udc49 Select (or vote on) one cluster and write a clear definition of the problem it represents. \ud83d\udca1 Now it's the fun part, it's time to come up with ideas! This is the creative part of the process, so don't be afraid to have too many ideas. This is a rare case of quantity before quality. \ud83c\udf29\ufe0f The best tool is a brainstorming session. A good brainstorm has these qualities: \ud83d\udc49 Only one person talks at a time, no interruption allowed. \ud83d\udc49 Set a maximum time for each intervention, around 1 minute. \ud83d\udc49 No idea is too crazy or impossible. \ud83d\udc49 Build on top of others: instead of \"no, because\", just say \"yes, and\". \ud83d\udee0\ufe0f Next, it's time to finally build something! Select one, or a few related ideas, and turn it into a prototype. You want to build a Minimum Viable Product (MVP), the smallest piece of functionality that lets you learn something new. \ud83d\udd25 Don't overengineer. You know you are overengineering when: \ud83d\udc49 You care about how it looks (unless that's the problem). \ud83d\udc49 You care about how fast it is (unless that's the problem). \ud83d\udc49 You feel sad if you have to scrap that prototype. \u2697\ufe0f Finally, you have to test that prototype. If your problem was clearly defined, you should know what a successful evaluation looks like. \ud83d\udcac This is not about software testing. Test the prototype with real people. Things to keep in mind: \ud83d\udc49 Shut up and watch, people should discover how the thing works by themselves. \ud83d\udc49 Test with a variety of different profiles. \ud83d\udc49 Write down every complaint you get, but don't mind about the proposed solutions. Once you reach this phase, you should have an answer to the question \"is this problem solved?\", which probably is: \ud83d\udd38 Somewhat, but the problem was not so well-defined as you thought. That great! You just completed one cycle and learnt something new. \ud83d\udd01 Now start over. \u26a1 Design Thinking applies not only to software but to any creative process. Whether you are planning an event, writing a book, building a physical gadget, or launching a service, you can use this strategy. This is not a bureaucratic methodology you have to follow, nor is a silver bullet. The key idea is this: \ud83d\udd11 You're trying to solve someone's problem, so you have to understand them first. Everything else is just tools to help you do that. As usual, if you like this topic, reply in this thread or @ me at any time. Feel free to \u2764\ufe0f like and \ud83d\udd01 retweet if you think someone else could benefit from knowing this stuff. \ud83e\uddf5 Read this thread online at https://apiad.net/tweetstorms/wisdomwednesday-designthinking Stay curious \ud83d\udd96 \ud83d\udd17 https://www.ideo.com/post/design-thinking \ud83d\udcc4 https://en.wikipedia.org/wiki/Design_thinking \ud83c\udfa5 https://youtu.be/UAinLaT42xY \ud83d\udde8\ufe0f You can see this tweetstorm as originally posted in this Twitter thread .","title":"Design Thinking"},{"location":"tweetstorms/wisdomwednesday/teams/","text":"Today is #WisdomWednesday \ud83e\udd29! A moment to share ideas to help you improve. Today I want to talk about team-building. \u2753 How to build an effective team? It turns out, it's easier than you think \ud83e\uddf5\ud83d\udc47 \u2753 Let's start by answering, why do you need a team at all? Whether you're building a new product or service, writing a research paper, or just experimenting, teammates will be invaluable. Here are some non-obvious advantages to working with others rather than alone \ud83d\udc47 1\ufe0f\u20e3 You are less likely to quit On one hand, your teammates will support you when you feel like quitting. On the other hand, you'll feel responsible for them, and also kind of guilty for letting them down, which will encourage you to suck it up and keep going. 2\ufe0f\u20e3 You are less likely to really screw it Whatever the probability of discovering a very bad decision is, often is enough that someone else spots it and it becomes evident. More people means the chance of missing a fatal flaw is quickly multiplying down to zero. 3\ufe0f\u20e3 You'll enjoy the ride more There is simply nothing better than to show others what you've come up with. Having a team means you'll always have an audience to showcase whatever dumb or smart idea you have. \u2753 Ok, but, what kind of team are we talking about here? It doesn't matter if you have a tightly hierarchized organization or a very loose group of like-minded people. You only really have a team if everyone in the team agrees that it is a team. A team is a set of people with a single shared idea: \u2b50 Everyone in the team believes being part of the team makes them better than being alone. \u2753 So, how can you build an effective team? Well, according to our definition, it seems pretty straightforward. \ud83d\udc49 You just have to make sure everyone in the team wants to stay in the team, right? So what does this mean? \ud83d\udc47 \ud83c\udd70\ufe0f There is something only the team can give me Maybe other members have skills I lack. Maybe it takes more than one person to achieve something. In any case, there has to be something that I cannot do alone. Otherwise, I don't need the team at all. \ud83c\udd71\ufe0f There is something only I can give the team Maybe I have a unique skill, or maybe I just bring a different mindset, or maybe I'm the one who makes everyone laugh. Whatever it is, I need to feel the team needs as much as I need the team. \ud83e\udd14 As you can see, we haven't talked about common goals, strong leadership, or organizational methodologies. All of that is great to have, but not strictly necessary. Here's why \ud83d\udc47 \u274e Effective teams don't need a common goal. We just need our objectives to be complementary to each other. While trying to achieve mine, I help you achieve yours, and vice-versa. In an effective team, the net force is positive, even if not all goals are aligned. \u274e Effective teams don't need strong leaders. A strong leader can be incredibly helpful, but the right leader for a given situation can be a different person. In an effective team, the right leaders naturally emerge when the task demands it. \u274e Effective teams don't need explicit methodologies. SCRUM and agile methodologies are great, but not strictly necessary. Every task may require a different structure. In an effective team, communication channels grow organically among members and the structure is flexible. I started saying that building an effective team is easy. That was a blatant lie \ud83d\ude48. Building an effective team is hard, very hard. But the benefits are even greater. \ud83d\udcad Now share your thoughts. What are your experiences with teams you've been part of? As usual, if you like this topic, reply in this thread or @ me at any time. Feel free to \u2764\ufe0f like and \ud83d\udd01 retweet if you think someone else could benefit from knowing this stuff. \ud83e\uddf5 Read this thread online at https://apiad.net/tweetstorms/wisdomwednesday-teams/ \ud83d\udde8\ufe0f You can see this tweetstorm as originally posted in this Twitter thread .","title":"Team Building"},{"location":"tweetstorms/wisdomwednesday/technical-writing/","text":"Technical writing is a skill anyone can learn and perfect with practice. It doesn't require any talent, muse, or divine inspiration; just focus, purpose, and a few iterations. Here's a process that works for me. \ud83d\udc47\ud83e\uddf5 {current}/{total} The main blockers I've struggled with during technical writing are: 1\ufe0f\u20e3 Lack of original ideas. 2\ufe0f\u20e3 Missing a clear and coherent structure. 3\ufe0f\u20e3 Running out of ideas or motivation halfway through. 4\ufe0f\u20e3 Dull and boring writing, lacking impact. \ud83d\udc47 {current}/{total} \ud83d\udd11 The key ingredient to overcoming these blockers is to iterate a lot. It is impossible to come up with original, coherent, and impactful ideas, all at once. Instead, you start with a bunch of unconnected ideas, and you relentlessly edit, rewrite, and improve. \ud83d\udc47 {current}/{total} Two reasons why this is hard: 1\ufe0f\u20e3 It is hard to edit your own words once you get too attached to them. 2\ufe0f\u20e3 Language is contextual, every sentence is connected with its surroundings. The longer a sentence lives, there harder it is to refactor. \ud83d\udc47 {current}/{total} \ud83d\udc49 What wee need is to delay the moment when ideas grow roots to as late as possible. We want them to float freely and bounce around as long as possible. And we want them to grow roots and entrench themselves into a coherent narrative at just the right time. \ud83d\udc47 {current}/{total} Here's a losely-defined process for this purpose: \ud83d\udca1 Brain-dumping the key ideas \ud83e\uddf5 Thread a structure \ud83d\udca7 Fill in with content \ud83c\udf40 Prune for clarity \ud83e\udd29 Polish for impact \ud83d\udc47 {current}/{total} \ud83d\udca1 Brain-dumping the key ideas Come up with as many ideas as possible, regardless of their quality. Forget about structure, order, story, coherence. The key insight is to not spend too much time thinking about any particular idea. \ud83d\udc47 {current}/{total} \ud83e\uddf5 Thread a structure Reorder ideas so that similar topics start to cluster and a structure begins to emerge. Cut anything that doesn't fit that structure. Resist explaining or refining the ideas. Keep them slim. \ud83d\udc47 {current}/{total} \ud83d\udca7 Fill in with content Now it's time to let ideas flourish. Expand every idea into as long as you need. You're only interested in quantity now, not quality. Spend as little time as possible thinking about any given sentence. \ud83d\udc47 {current}/{total} \ud83c\udf40 Prune for clarity Now aim to cut every paragraph down to half its size. Try and remove every paragraph, and if you don't miss it, it's gone for good. Rewrite what's left to make every sentence as clear-cut as possible. \ud83d\udc47 {current}/{total} \ud83e\udd29 Polish for impact Identify the key ideas in your narrative that should feel like a punch in the face and build momemtum towards them. Find the most relevant idea you want to deliver. Repeat them a few times to make them ressonate. \ud83d\udc47 {current}/{total} When you're done with this third draft, your text is ready to publish. Put it out there and move on to the next piece. If you want to read a slightly deeper dive, check this out: https://apiad.net/guides/technical-writing \u2693 {total}/{total}","title":"Technical writing"},{"location":"tweetstorms/wisdomwednesday/vsnc/","text":"Convincing an audience to care for your idea, project, product, or business requires being persuasive when writing and talking. In this thread you'll learn about the VSN-C framework, a simple way to structure a talk or writen piece for maximum persuasion. \ud83e\uddf5\ud83d\udc47 {current}/{total} Patrick Winston, legendary professor of AI at MIT, was a marvelous lecturer and communicator. Part of his magic recipe was the VSN-C framework, a way of structuring an exposition or talk for maximizing the chance of convincing your audience. This is how it works. \ud83d\udc47 {current}/{total} Divide your exposition or talk into these sections: \ud83d\udca1 Vision \u2705 Steps \ud83d\uddde\ufe0f News \ud83d\udcdd Main content \ud83c\udf81 Contributions You don't need to explicitly name sections this way. They can be implicitely weaved into your narrative. What matters is their intention. \ud83d\udc47 {current}/{total} \ud83d\udca1 Begin with a clear vision to make the audience sympathetic to your cause. Your vision combines a problem your audience cares about together with a grand idea or hypothesis you have for solving it. You have to show you care deeply about that problem too. \ud83d\udc47 {current}/{total} \u270f\ufe0f Example (hypothetical): If we want to really disrupt industry with artificial intelligence, we need to make it as easy as possible for any domain expert, regardless of their knowledge of AI, to quickly find state of the art models for their business problems. \u2705 Then layout the steps that are necessary to achieve that vision. You are not expected to have them all done. You just need to show that you have a solid plan to achieve that vision, even if its a really difficult plan. \ud83d\udc47 {current}/{total} \u270f\ufe0f Example (hypothetical): This requires having a repository of known problems and solutions, an automatic mechanism for characterising a new problem, and a powerful search engine to quickly find the best model to that new problem based on previous experience. \ud83d\uddde\ufe0f Next, tease the audience with a recent news of your success. This news is a breakthrough related with just one of those steps. It's what shows you've done something worthy of knowing about. The rest of your talk will for explaining exactly how you did it. \ud83d\udc47 {current}/{total} \u270f\ufe0f Example (hypothetical): By leveraging meta-learning techniques, we have achieved a new state-of-the-art performance in zero-shot architecture search in a restricted domain. \ud83d\udcdd Now you can expose your ideas, explain all the details, present your results, and discuss their implications. This is the point where you tell what really matters. (\ud83d\udde8\ufe0f I'll talk more about different strategies for this part in future threads.) \ud83d\udc47 {current}/{total} \ud83c\udf81 Finally, present your contributions clearly and succintly. Remember the audience exactly what you've achieved. Repeat the main takeaways you want them to take home. Make it not about what you've learned, but what you've done. \ud83d\udc47 {current}/{total} \u270f\ufe0f Example (hypothetical): Our novel meta-learning approach achieves top-3 performance on zero-shot architecture search on the FROGS dataset. Our approach finds better models than existing alternatives with as little as 10% of their computational cost. \u2b50 Whether you're writing a blog post, a research paper, a script for a video, a Twitter thread, or giving a public talk, if your purpose is to persuade an audience to buy an idea from you, use the VSN-C framework. \ud83d\udcd3 To learn more, check \"Make it Clear\" by Patrick Winston. In this thread you've learned about the VSN-C framework for persuasive communication. If you want to know more about the art of writen and oral communication, I have a bunch more threads like this planned. \ud83d\udd96 Stay tunned! \u2693 {current}/{total}","title":"Vsnc"},{"location":"tweetstorms/wisdomwednesday/why/","text":"Today is #WisdomWednesday \ud83e\udd29! A moment to share ideas to help you improve. Today's topic is inspired by @simonsinek and @neiltyson . Let's talk about how to communicate an idea. The key insight is this: \ud83d\udd11 Start with Why! A path to effective communication! \ud83e\uddf5\ud83d\udc47 You have an amazing idea. Whether it is a solution to a problem you care deeply about or an incredible new insight, it doesn't matter how genius it is if you are the only one who cares. \u2753 Why? \u2b50 Because it's not enough to be right. You have to be effective. Being effective means to produce the desired effect . To have someone or something change some behaviour because of you. Making anything change is hard, but it is especially hard to do it with people. \ud83d\udc49 If your idea does not spark a change, then it is a dead idea. \u2753 What do you do then? \ud83d\udca1 You convince them that the problem you care about is also a problem they have or a problem they should care about. \u2753 How do you do it? \u2699\ufe0f Start by structuring your discourse for maximum impact. Whether it is a 1-minute pitch or a PhD viva, always start with the problem, and then build towards the solution. Answer these questions in this order: \u2b50 Why \ud83d\udca1 What \u2699\ufe0f How \u2b50 Connect with your audience: 1\ufe0f\u20e3 Use a story, preferably with people as protagonists 2\ufe0f\u20e3 Be explicit about the problem 3\ufe0f\u20e3 Layout the consequences of leaving that problem unsolved 4\ufe0f\u20e3 Be kind of dramatic (\ud83d\udc4d @josejorgexl ) 5\ufe0f\u20e3 Promise them something \ud83d\udca1 Get your main point across: \ud83d\uddbc\ufe0f Use graphics \u26a1\ufe0f Be concise \u2699\ufe0f You got them, now squeeze: 1\ufe0f\u20e3 Do not patronize your audience 2\ufe0f\u20e3 Use concrete and realistic examples 3\ufe0f\u20e3 Explain things as simple as possible, but not simpler 4\ufe0f\u20e3 Leave unimportant details unexplained 5\ufe0f\u20e3 Do not show off 6\ufe0f\u20e3 Always fulfil what you promised Once you have them convinced, leave with a Call to Action . Something your audience can do right there about that issue. \u26a1\ufe0f Start applying these insights today, in your emails, Twitter threads, or presentations, and give your communication skills a level-up! As usual, if you like this topic, reply in this thread or @ me at any time. Feel free to \u2764\ufe0f like and \ud83d\udd01 retweet if you think someone else could benefit from knowing this stuff. \ud83e\uddf5 Read this thread online at https://apiad.net/tweetstorms/wisdomwednesday-why Stay curious \ud83d\udd96 \ud83c\udfa5 https://youtu.be/qp0HIF3SfI4 \ud83c\udfa5 https://youtu.be/Tv0kQbOIrjY \ud83d\udcda https://simonsinek.com/product/start-with-why/ \ud83d\udcc4 https://simonsinek.com/commit/the-science-of-why/ \ud83d\udde8\ufe0f You can see this tweetstorm as originally posted in this Twitter thread .","title":"Start with Why"}]}